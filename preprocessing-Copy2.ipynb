{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__1Nxb4gRIeN"
   },
   "source": [
    "Done so far :\n",
    "\n",
    "\n",
    "*   Lemmatization\n",
    "*   Stop Words Removal\n",
    "\n",
    "Verify :\n",
    "\n",
    "* Normalization - removing accents, etc.\n",
    "* Dates replaced with strings\n",
    "* Case-folding\n",
    "* Removed HTML entity codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "### pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlcodes(document):\n",
    "    \n",
    "    replacement = {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                  }\n",
    "    \n",
    "    for str in replacement:\n",
    "        document = document.replace(str, replacement[str])\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    \n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    \n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    \n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "                    else:\n",
    "                        replace_with=' '.join(word for word in split)\n",
    "                        cleaned_doc = cleaned_doc.replace(word, replace_with)\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def replace_dates(documentString, docID):\n",
    "    \n",
    "    # regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\n",
    "    regEx = '(([0-9]+(/)[0-9]+(/)[0-9]+)|([0-9]+(/)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    tmp = []\n",
    "    replace_with = []\n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp.append(date)\n",
    "        # date = date.replace('.', '/')\n",
    "        # date = date.replace('-', '/')\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            check_year = date[-3]\n",
    "            \n",
    "            if check_year == '/':\n",
    "                YY = date[-2:]\n",
    "                \n",
    "                if int(YY) <= 19:\n",
    "                    proper_date = date[:-2] + '20' + YY\n",
    "                    date = date.replace(date,proper_date)\n",
    "                else:\n",
    "                    proper_date = date[:-2] + '19' + YY\n",
    "                    date = date.replace(YY,('19'+YY))\n",
    "                    \n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        count_dates.append([docID, date])\n",
    "        newDate = newDate.replace(' ', '')\n",
    "        replace_with.append(newDate)\n",
    "        \n",
    "    for i in range(len(tmp)):\n",
    "        documentString = documentString.replace(tmp[i], replace_with[i])\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filet = \"/home/nihaljain/3-1/CS F469/Assignment-1/mod_titles\"\n",
    "filec = \"/home/nihaljain/3-1/CS F469/Assignment-1/mod_contents\"\n",
    "filed = \"/home/nihaljain/3-1/CS F469/Assignment-1/mod_data\"\n",
    "\n",
    "titles = np.load(filet + \".npy\", allow_pickle = True)\n",
    "contents = np.load(filec + \".npy\", allow_pickle = True)\n",
    "data = np.load(filed + \".npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0547c1b07114cd4aaaaa258a750a56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89163), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "N = len(titles)\n",
    "\n",
    "for i in tqdm_notebook(range(N)):\n",
    "    for j in range(len(contents[i])):\n",
    "        contents[i][j] = unidecode.unidecode(contents[i][j])\n",
    "    for j in range(len(titles[i])):\n",
    "        titles[i][j] = unidecode.unidecode(titles[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('modified_contents_ascii.pickle', 'wb') as handle:\n",
    "    pickle.dump(contents, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('modified_titles_ascii.pickle', 'wb') as handle:\n",
    "    pickle.dump(titles, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# To read the data again\n",
    "\n",
    "# with open('modified_contents_ascii.pickle', 'rb') as handle:\n",
    "#     contents = pickle.load(handle)\n",
    "# with open('modified_titles_ascii.pickle', 'rb') as handle:\n",
    "#     titles = pickle.load(handle)\n",
    "    \n",
    "# print(unserialized_title == titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [],
   "source": [
    "import trie\n",
    "import pickle\n",
    "\n",
    "N = 89163\n",
    "\n",
    "getReference = {}\n",
    "\n",
    "# contents = []\n",
    "# titles = []\n",
    "# data = []\n",
    "\n",
    "# with open('modified_contents_ascii.pickle', 'rb') as handle:\n",
    "#     contents = pickle.load(handle)\n",
    "# with open('modified_titles_ascii.pickle', 'rb') as handle:\n",
    "#     titles = pickle.load(handle)\n",
    "# data = np.load(\"mod_data.npy\", allow_pickle = True)\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = int(data[i][0])\n",
    "    get_index[int(data[i][0])] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102672\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(data)//2, len(data)//2 + 1000): print(get_docID[i])\n",
    "print(get_index[127779])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentRoot = {}\n",
    "collection = trie.CollectionNode()\n",
    "\n",
    "# initializing the root for N documents\n",
    "for i in range(len(data)//2, len(data)):\n",
    "    newDocument = trie.Node()\n",
    "    documentRoot[get_docID[i]] = newDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a45279d5f44677aee007698e3b6d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.947448968887329\n"
     ]
    }
   ],
   "source": [
    "# creating the documents\n",
    "import pickle\n",
    "\n",
    "max_tf = {}\n",
    "# contents = []\n",
    "# titles = []\n",
    "# data = []\n",
    "\n",
    "# with open('modified_contents_ascii.pickle', 'rb') as handle:\n",
    "#     contents = pickle.load(handle)\n",
    "# with open('modified_titles_ascii.pickle', 'rb') as handle:\n",
    "#     titles = pickle.load(handle)\n",
    "# data = np.load(\"mod_data.npy\", allow_pickle = True)\n",
    "\n",
    "# N = len(contents)\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "start = time.time()\n",
    "j = 0\n",
    "for i in tqdm_notebook(range(len(data)//2, len(data)//2 + 1000)):\n",
    "    if data[i][4] == None or data[i][1] == None or data[i][0] == None:\n",
    "        continue\n",
    "    for w in contents[j]:\n",
    "        collection.add_document(w, 0, get_docID[i])\n",
    "        documentRoot[get_docID[i]].add(w, 0)\n",
    "        if get_docID[i] in max_tf:\n",
    "            max_tf[get_docID[i]] = max(documentRoot[get_docID[i]].count_words(w, 0), max_tf[get_docID[i]])\n",
    "        else:\n",
    "            max_tf[get_docID[i]] = documentRoot[get_docID[i]].count_words(w, 0)\n",
    "    for w in titles[j]:\n",
    "        collection.add_title(w, 0, get_docID[i])\n",
    "    j += 1\n",
    "        \n",
    "print(time.time() - start)  #39.19705152511597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('collection.pickle', 'wb') as handle:\n",
    "    pickle.dump(collection, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('documentRoot.pickle', 'wb') as handle:\n",
    "    pickle.dump(documentRoot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# reading from pickle files\n",
    "\n",
    "# with open('collection.pickle', 'rb') as handle:\n",
    "#     collection = pickle.load(handle)\n",
    "# with open('documentRoot.pickle', 'rb') as handle:\n",
    "#     documentRoot = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import queue\n",
    "\n",
    "# documentLength = {}\n",
    "# N = len(documentRoot)\n",
    "\n",
    "# for i in tqdm(range(len(documentRoot))):\n",
    "    \n",
    "#     docID = get_docID[i]\n",
    "#     length = 0\n",
    "#     document = documentRoot[i]\n",
    "#     q = queue.Queue()\n",
    "#     q.put([document, ''])\n",
    "\n",
    "#     while q.qsize() > 0:\n",
    "\n",
    "#         current = q.get()\n",
    "#         reference = current[0]\n",
    "#         word = current[1]\n",
    "\n",
    "#         if reference.words > 0:\n",
    "#             df = len(collection.get_doc_list(word, 0))\n",
    "#             idf = math.log10(N/df)\n",
    "#             # print(word, reference.words, df)\n",
    "#             length += (reference.words * idf) ** 2\n",
    "\n",
    "#         for i in range(256):\n",
    "#             if reference.children[i] is not None:\n",
    "#                 new_word = word + chr(i)\n",
    "#                 q.put([reference.children[i], new_word])\n",
    "\n",
    "#     # print(length**0.5)\n",
    "#     documentLength[docID] = length**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9w6bB9rJfg",
    "outputId": "2e5b7bc8-2e68-4de8-f81d-a142a989c902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11sep']\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "query = '9/11'\n",
    "final_query = replace_dates(query, -1)\n",
    "final_query = lemma_stop(final_query)\n",
    "\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "    # case-folding\n",
    "    final_query[i] = final_query[i].lower()\n",
    "print(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4Kqgk9hrJfj",
    "outputId": "04bfaa8a-8053-4ab7-e55a-cee2d47edb6f"
   },
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfT-kdUqrJfp",
    "outputId": "3c95efd0-76c9-4dfe-e425-e79c089931ee",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  11sep\n",
      "\n",
      "df =  29\n",
      "idf =  3.5464916070505508\n",
      "{127566: 3.5464916070505508, 131122: 3.5464916070505508, 129561: 3.5464916070505508, 128190: 3.5464916070505508, 127998: 3.5464916070505508}\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  131122\n",
      "Keywords:\n",
      "\n",
      "Daughter of only female cop killed on 9/11 stares down al Qaeda terrorist\n",
      "\n",
      "title score =  3.5464916070505508\n",
      "11sep -2.3643277380337 2\n",
      "\n",
      "\n",
      "The daughter of fallen NYPD officer Moira Smith, who was the only female cop to die on 9/11, spent Mother’s Day in Guantanamo Bay — staring down one of the terrorists the US says helped kill her mom and more than 3,000 others. “I am glad Patricia was able to accompany me here to see for herself the monsters responsible for the murder of her mother,” the  ... \n",
      "\n",
      "tf-idf score= 38.122643465469395\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  127566\n",
      "Keywords:\n",
      "\n",
      "Families of 9/11 victims file suit against Saudi Arabia\n",
      "\n",
      "title score =  3.5464916070505508\n",
      "11sep -2.3643277380337 2\n",
      "\n",
      "\n",
      "Families of 9/11 victims filed suit in Manhattan against Saudi Arabia Monday, claiming the Arab country knowingly facilitated the devastating terror attacks. The consolidated action was filed in federal court on behalf of 2,500 spouses, children, parents and siblings of those who died when 19 al Qaeda insurgents hijacked four airplanes and  ... \n",
      "\n",
      "tf-idf score= 38.122643465469395\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  127998\n",
      "Keywords:\n",
      "\n",
      "Students told to write 9/11 essay from terrorists’ viewpoint\n",
      "\n",
      "title score =  3.5464916070505508\n",
      "11sep -2.12789496423033 1\n",
      "\n",
      "\n",
      "An Iowa college professor assigned his students to write a “historical account’’ of 9/11 — from the terrorists’ perspective. “Write a paper that gives a historical account of 911 from the perspective of the terrorist network,” lecturer James Strohman wrote in an assignment for his international studies students at Iowa State University and . “In other words, how might al Qaeda or a  ... \n",
      "\n",
      "tf-idf score= 34.310379118922455\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  128190\n",
      "Keywords:\n",
      "\n",
      "FBI releases trove of 9/11 Pentagon photos\n",
      "\n",
      "title score =  3.5464916070505508\n",
      "11sep -1.950570383877803 1\n",
      "\n",
      "\n",
      "FBI FBI FBI FBI FBI FBI FBI FBI FBI View Slideshow The FBI on Thursday of the devastation from the 9/11 terror attack on the Pentagon. The 27 images show the gaping hole in the facade where a jet hijacked by Islamic jihadists slammed into the building, killing 184 innocents, including 125 Pentagon employees and 59 Other images show first responders battling stubborn blazes at the complex, debris from American  ... \n",
      "\n",
      "tf-idf score= 31.451180859012258\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  129561\n",
      "Keywords:\n",
      "\n",
      "9/11 survivor: My neighbor has made life a living hell\n",
      "\n",
      "title score =  3.5464916070505508\n",
      "11sep -1.9344499674821185 1\n",
      "\n",
      "\n",
      "A former Cantor Fitzgerald partner who nearly died on 9/11 has had another trauma to deal with — a downstairs neighbor from hell, according to a Manhattan lawsuit. New York Times-bestselling author Lauren Manning and her husband are suing the landlord of their former Park Avenue pad after the building sided with the rent-stabilized tenant in a noise dispute  ... \n",
      "\n",
      "tf-idf score= 31.191253744474963\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  128643\n",
      "Keywords:\n",
      "\n",
      "Student in pancake-eating contest died of asphyxia\n",
      "\n",
      "title score =  0\n",
      "11sep -2.3643277380337 1\n",
      "\n",
      "\n",
      "... chief medical examiner’s office ruled. Caitlin Nelson, the 20-year-old daughter of a Port Authority cop who lost his life on 9/11, was participating in the charity contest at her school, Sacred Heart University in Fairfield, Conn., on March 30 when she choked after eating several pancakes and collapsed. She was rushed to a local hospital in critical condition, and was transferred to Columbia-Presbyterian Hospital in Manhattan, where she died Sunday.  ... \n",
      "\n",
      "tf-idf score= 8.38506847925333\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  127666\n",
      "Keywords:\n",
      "\n",
      "De Blasio honors anchor who dropped infamous live F-bomb\n",
      "\n",
      "title score =  0\n",
      "11sep -2.3643277380337 1\n",
      "\n",
      "\n",
      "... his coverage of John F. Kennedy Jr.’s death in a 1999 plane crash on Cape Cod. He also covered the 9/11 attacks, interviewed Fidel Castro in Cuba, reported on presidents at home and abroad in his lengthy career. But he is perhaps best known for an embarrassing gaffe in which he dropped an F-bomb on live TV. In September 2009, the affable anchorman was trying to say “plucking that chicken”  ... \n",
      "\n",
      "tf-idf score= 8.38506847925333\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  128454\n",
      "Keywords:\n",
      "\n",
      "Family to donate organs of student who choked to death at pancake-eating contest\n",
      "\n",
      "title score =  0\n",
      "11sep -2.3643277380337 4\n",
      "\n",
      "\n",
      "A New Jersey co-ed who lost her hero-cop dad on 9/11 — and then — will have her organs donated as she wished, a friend of the grieving family said Monday. “Like her father, right up until the end she was giving of herself, and proof of that is her organs are all being donated,” said Robert Egbert, a spokesman  ... \n",
      "\n",
      "tf-idf score= 8.38506847925333\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  131333\n",
      "Keywords:\n",
      "\n",
      "Sorry, Mr. Mayor: Oscar Lopez Rivera remains an unrepentant terrorist\n",
      "\n",
      "title score =  0\n",
      "11sep -2.2165572544065943 1\n",
      "\n",
      "\n",
      "... mayor of a town targeted by the FALN then and by other terrorists now, a city that lost thousands on 9/11. He skipped the St. Patrick’s Day Parade when he thought it sent the wrong message. He ought to do the same now.  ... \n",
      "\n",
      "tf-idf score= 7.861001699299999\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  131595\n",
      "Keywords:\n",
      "\n",
      "The people of Iran are still America’s natural allies\n",
      "\n",
      "title score =  0\n",
      "11sep -2.2165572544065943 2\n",
      "\n",
      "\n",
      "... candidate, President Hassan Rouhani, won by a landslide. In his speech Sunday in Riyadh, the capital city that gave us 9/11, our president wisely stated that our quarrel isn’t with the Iranian people. And it shouldn’t be. The majority of Iranians — especially the young — want to see the end of the dictatorship of the clerics and the Revolutionary Guards as badly as we do. Yet violent confrontation with  ... \n",
      "\n",
      "tf-idf score= 7.861001699299999\n",
      "\n",
      "\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "\n",
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "title_score = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "factor = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\n",
    "    \n",
    "    for docID in docs_having_query_term_in_title:\n",
    "        if docID in title_score:\n",
    "            title_score[docID] += idf\n",
    "        else:\n",
    "            title_score[docID] = idf\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        \n",
    "        tf_doc = documentRoot[docID].count_words(query_term, 0)\n",
    "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\n",
    "        tfidf_doc = (tf_doc)\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query * tfidf_doc)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] = idf\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query * tfidf_doc)\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] += idf\n",
    "            \n",
    "print(title_score)\n",
    "\n",
    "for docID in scores:\n",
    "    \n",
    "    #if documentLength[docID] != 0:\n",
    "    scores[docID] *= factor[docID]\n",
    "    if docID in title_score:\n",
    "        scores[docID] *= 1 + title_score[docID]\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    \n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    print()\n",
    "    print(data[get_index[sorted_scores[i][0]]][1])\n",
    "    print()\n",
    "    if sorted_scores[i][0] not in title_score:\n",
    "        print('title score = ',0)\n",
    "    else:\n",
    "        print('title score = ',title_score[sorted_scores[i][0]])\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\n",
    "        print(documentRoot[docID].count_words(wordsInDoc[docID][j][1], 0))\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    \n",
    "    for word in data[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word, -1)\n",
    "        check_with = check_with.lower()\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print('\\n')\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    print('\\n')\n",
    "    print('============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, '50/50'], [42, '1/2'], [47, '4/4'], [47, '4/5'], [47, '3/4'], [47, '4/5'], [67, '24/7'], [70, '50/50'], [104, '9/11'], [151, '9/11'], [6432, '9/11'], [6440, '9/11'], [6492, '9/11'], [6620, '12/8'], [6701, '24/7'], [6765, '24/7'], [6802, '9/11'], [6811, '2008/8'], [6812, '12/22'], [6919, '1/5000'], [6919, '1/11'], [6919, '1/150'], [6919, '1/750'], [6919, '1/5000'], [6977, '9/11'], [6979, '1/2'], [6981, '6/19'], [7078, '9/11'], [7127, '9/11'], [7313, '24/7'], [-1, '2/5']]\n"
     ]
    }
   ],
   "source": [
    "print((count_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{128067, 127788, 127440, 127411, 128116, 127157, 127611, 127260, 128190, 127359}\n"
     ]
    }
   ],
   "source": [
    "print(collection.get_doc_list('jet',0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBI FBI FBI FBI FBI FBI FBI FBI FBI View Slideshow The FBI on Thursday   of the devastation from the 9/11 terror attack on the Pentagon. The 27 images show the gaping hole in the facade where a jet hijacked by Islamic jihadists slammed into the building, killing 184 innocents, including 125 Pentagon employees and 59 Other images show first responders battling stubborn blazes at the complex, debris from American Airlines Flight 77’s fuselage and haunting images from the scorched interior of the Arlington, Va. landmark. The airliner’s wings clipped off light poles before slamming into the western side of the building at the first-floor level. The jet ripped a hole into the structure that extended more than 100 yards into the building. It took firefighters days to fully extinguish the blaze. Luckily, the plane struck a portion of the building that was under renovation and relatively empty. If it had struck another side or happened after the renovations were completed, the death toll would have been significantly higher, experts said at the time.\n"
     ]
    }
   ],
   "source": [
    "print(data[get_index[128190]][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[''], ['']]\n"
     ]
    }
   ],
   "source": [
    "a = [['aaaa'],['bbbb']]\n",
    "for document in a:\n",
    "    document[0] = ''\n",
    "    document = []\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
