{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__1Nxb4gRIeN"
   },
   "source": [
    "Done so far :\n",
    "\n",
    "\n",
    "*   Lemmatization\n",
    "*   Stop Words Removal\n",
    "\n",
    "Verify :\n",
    "\n",
    "* Normalization - removing accents, etc.\n",
    "* Dates replaced with strings\n",
    "* Case-folding\n",
    "* Removed HTML entity codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "### pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlcodes(document):\n",
    "    \n",
    "    replacement = {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                  }\n",
    "    \n",
    "    for str in replacement:\n",
    "        document = document.replace(str, replacement[str])\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PvIwQ6ySVHv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data.npy\n",
    "# data.npy is a 2D array containing the dataset information as\n",
    "# data[i][0] : docID of ith document\n",
    "# data[i][1] : title of ith document\n",
    "# data[i][4] : content of ith document\n",
    "\n",
    "data = np.load('data.npy',allow_pickle = True)\n",
    "# sentence = data[0][4]\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6aQ7ZcPCH4d",
    "outputId": "8fc93dc4-51e3-48ff-fa64-d5bbef5847a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a map {index_in_data_npy, docID}\n",
    "\n",
    "# ex. if ith element in data has docID j,\n",
    "# get_docID[i] will return j\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = int(data[i][0])\n",
    "    get_index[int(data[i][0])] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    \n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    \n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    \n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def replace_dates(documentString):\n",
    "    \n",
    "    regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    tmp = []\n",
    "    replace_with = []\n",
    "    \n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp.append(date)\n",
    "        date = date.replace('.', '/')\n",
    "        date = date.replace('-', '/')\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            check_year = date[-3]\n",
    "            \n",
    "            if check_year == '/':\n",
    "                YY = date[-2:]\n",
    "                \n",
    "                if int(YY) <= 19:\n",
    "                    proper_date = date[:-2] + '20' + YY\n",
    "                    date = date.replace(date,proper_date)\n",
    "                else:\n",
    "                    proper_date = date[:-2] + '19' + YY\n",
    "                    date = date.replace(YY,('19'+YY))\n",
    "                    \n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        newDate = newDate.replace(' ', '')\n",
    "        replace_with.append(newDate)\n",
    "        \n",
    "    for i in range(len(tmp)):\n",
    "        documentString = documentString.replace(tmp[i], replace_with[i])\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(replace_dates('12/12/12 to 12/12/1989'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary smaller dataset\n",
    "\n",
    "subset = []\n",
    "counter = 0\n",
    "for document in data:\n",
    "    subset.append(document)\n",
    "    counter += 1\n",
    "    if counter == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:47<00:00,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.09054398536682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "for document in tqdm(subset):\n",
    "    # actually modifying the document\n",
    "    document[4] = remove_htmlcodes(document[4])\n",
    "    document[1] = remove_htmlcodes(document[1])\n",
    "    \n",
    "    # not actually modifying the document\n",
    "    modifiedContent = replace_dates(document[4])\n",
    "    modifiedContent = lemma_stop(clean_document(modifiedContent))\n",
    "    modifiedTitle = replace_dates(document[1])\n",
    "    modifiedTitle = lemma_stop(clean_document(modifiedTitle))\n",
    "    \n",
    "    # case-folding\n",
    "    for i in range(len(modifiedContent)):\n",
    "        modifiedContent[i] = modifiedContent[i].lower()\n",
    "    for i in range(len(modifiedTitle)):\n",
    "        modifiedTitle[i] = modifiedTitle[i].lower()\n",
    "    \n",
    "    titles.append(modifiedTitle)\n",
    "    contents.append(modifiedContent)\n",
    "    \n",
    "print(time.time() - start)  # 110.26236414909363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "contents_temp = contents\n",
    "\n",
    "titles_temp = titles\n",
    "for i in range(1000):\n",
    "    for j in range(len(contents[i])):\n",
    "        contents[i][j] = unidecode.unidecode(contents[i][j])\n",
    "    for j in range(len(titles[i])):\n",
    "        titles[i][j] = unidecode.unidecode(titles[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('modified_contents.pickle', 'wb') as handle:\n",
    "    pickle.dump(contents, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('modified_titles.pickle', 'wb') as handle:\n",
    "    pickle.dump(titles, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\n",
    "# To read the data again\n",
    "\n",
    "# with open('modified_contents.pickle', 'rb') as handle:\n",
    "#     contents = pickle.load(handle)\n",
    "# with open('modified_titles.pickle', 'rb') as handle:\n",
    "#     titles = pickle.load(handle)\n",
    "    \n",
    "# print(unserialized_title == titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [],
   "source": [
    "import trie\n",
    "\n",
    "# Create map from docID of the document to an object of class Node \n",
    "# (i.e, the corresponding document trie structure)\n",
    "# ex. if the docID of the document is 1, \n",
    "# getReference[1] gives the object which is the trie structure of docID 1\n",
    "\n",
    "getReference = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentRoot = []\n",
    "collection = trie.CollectionNode()\n",
    "\n",
    "# initializing the root for 1000 documents\n",
    "for i in range(1000):\n",
    "    newDocument = trie.Node()\n",
    "    documentRoot.append(newDocument)\n",
    "    getReference[get_docID[i]] = newDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 91.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.900481939315796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the documents\n",
    "\n",
    "max_tf = {}\n",
    "N = 1000\n",
    "\n",
    "with open('modified_contents.pickle', 'rb') as handle:\n",
    "    contents = pickle.load(handle)\n",
    "with open('modified_titles.pickle', 'rb') as handle:\n",
    "    titles = pickle.load(handle)\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(N)):\n",
    "    for w in contents[i]:\n",
    "        collection.add_document(w, 0, get_docID[i])\n",
    "        documentRoot[i].add(w, 0)\n",
    "        if get_docID[i] in max_tf:\n",
    "            max_tf[get_docID[i]] = max(documentRoot[i].count_words(w, 0), max_tf[get_docID[i]])\n",
    "        else:\n",
    "            max_tf[get_docID[i]] = documentRoot[i].count_words(w, 0)\n",
    "    for w in titles[i]:\n",
    "        collection.add_title(w, 0, get_docID[i])\n",
    "        \n",
    "print(time.time() - start)  #39.19705152511597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('collection.pickle', 'wb') as handle:\n",
    "    pickle.dump(collection, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('documentRoot.pickle', 'wb') as handle:\n",
    "    pickle.dump(documentRoot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# reading from pickle files\n",
    "\n",
    "with open('collection.pickle', 'rb') as handle:\n",
    "    collection = pickle.load(handle)\n",
    "with open('documentRoot.pickle', 'rb') as handle:\n",
    "    documentRoot = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import queue\n",
    "\n",
    "# documentLength = {}\n",
    "# N = len(documentRoot)\n",
    "\n",
    "# for i in tqdm(range(len(documentRoot))):\n",
    "    \n",
    "#     docID = get_docID[i]\n",
    "#     length = 0\n",
    "#     document = documentRoot[i]\n",
    "#     q = queue.Queue()\n",
    "#     q.put([document, ''])\n",
    "\n",
    "#     while q.qsize() > 0:\n",
    "\n",
    "#         current = q.get()\n",
    "#         reference = current[0]\n",
    "#         word = current[1]\n",
    "\n",
    "#         if reference.words > 0:\n",
    "#             df = len(collection.get_doc_list(word, 0))\n",
    "#             idf = math.log10(N/df)\n",
    "#             # print(word, reference.words, df)\n",
    "#             length += (reference.words * idf) ** 2\n",
    "\n",
    "#         for i in range(256):\n",
    "#             if reference.children[i] is not None:\n",
    "#                 new_word = word + chr(i)\n",
    "#                 q.put([reference.children[i], new_word])\n",
    "\n",
    "#     # print(length**0.5)\n",
    "#     documentLength[docID] = length**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9w6bB9rJfg",
    "outputId": "2e5b7bc8-2e68-4de8-f81d-a142a989c902"
   },
   "outputs": [],
   "source": [
    "query = 'Obama'\n",
    "final_query = replace_dates(query)\n",
    "final_query = lemma_stop(final_query)\n",
    "\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "    # case-folding\n",
    "    final_query[i] = final_query[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4Kqgk9hrJfj",
    "outputId": "04bfaa8a-8053-4ab7-e55a-cee2d47edb6f"
   },
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfT-kdUqrJfp",
    "outputId": "3c95efd0-76c9-4dfe-e425-e79c089931ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  obama\n",
      "\n",
      "df =  37\n",
      "idf =  1.431798275933005\n",
      "{6585: 1.431798275933005, 7149: 1.431798275933005, 6846: 1.431798275933005}\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6846\n",
      "Keywords:\n",
      "\n",
      "What can Obama really do for net neutrality?\n",
      "\n",
      "title score =  1.431798275933005\n",
      "obama -1.2633514199408868 13\n",
      "\n",
      "\n",
      "... morning, a group blocked off Chairman Tom Wheeler's driveway to decry the latest series of rules. And now, President Barack Obama has urged the FCC to take a stand and treat broadband more like a utility than a web portal.Obama's loose four-point proposal is a boon to most net neutrality supporters: he's asking for broadband to be classified as a Title II common carrier, putting it in the same category  ... \n",
      "\n",
      "tf-idf score= 4.398793292763927\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7149\n",
      "Keywords:\n",
      "\n",
      "Moment of truth: will Obama take on the NSA?\n",
      "\n",
      "title score =  1.431798275933005\n",
      "obama -0.8053865302123153 3\n",
      "\n",
      "\n",
      "It's time for a change at the NSA. But will President Obama deliver it? After former National Security Agency contractor Edward Snowden leaked documents showing the agency was engaged in widespread and often unchecked surveillance of phone and internet activity, calls to reform America's top spy organization have come from all corners. This week, we find out whether those calls will  ... \n",
      "\n",
      "tf-idf score= 2.8042307241370033\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6585\n",
      "Keywords:\n",
      "\n",
      "Detroit and Silicon Valley are both in love with Obama’s self-driving car plan\n",
      "\n",
      "title score =  1.431798275933005\n",
      "obama -0.7954434866294473 1\n",
      "\n",
      "\n",
      "Both legacy automakers and disruptive technology companies were quick to praise the Obama administration Thursday for its proposal to encourage the development of self-driving cars, especially for the promise to present a model policy for states to avoid a regulatory patchwork.\"[This is] definitely a positive announcement,\" John Krafcik, the head of Google's self-driving car program, told reporters. \"We're also very excited that  ... \n",
      "\n",
      "tf-idf score= 2.7696105917402507\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  51\n",
      "Keywords:\n",
      "\n",
      "@MichelleObama\n",
      "\n",
      "title score =  0\n",
      "obama -1.3794154121793585 38\n",
      "\n",
      "\n",
      "... .embed-container iframe, .embed-container object, .embed-container embed { position: absolute top: 0 left: 0 width: 100% height: 100% }And then Michelle Obama joined the party. That October, Mrs. Obama held a Vine Q&A for her Let’s Move! initiative, calling on kids and their families to send in video questions about healthy eating, cooking, and gardening. Over the course of five days, she answered a series of family-friendly queries ranging from \"What’s  ... \n",
      "\n",
      "tf-idf score= 1.975044608953821\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6812\n",
      "Keywords:\n",
      "\n",
      "The US probably isn't behind North Korea's internet problems\n",
      "\n",
      "title score =  0\n",
      "obama -1.0413078370421855 5\n",
      "\n",
      "\n",
      "This morning brought some alarming news. Just two days after President Obama promised a proportionate response to the North Korean attack on Sony, the country mysteriously disappeared from the internet and stayed offline for the next 10 hours. Given the timing, the question was inevitable: was this the retaliation Obama had promised? But while it's tempting to connect the two, early  ... \n",
      "\n",
      "tf-idf score= 1.4909427657925278\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6998\n",
      "Keywords:\n",
      "\n",
      "One year of NSA leaks: where are we now?\n",
      "\n",
      "title score =  0\n",
      "obama -0.9715774015259677 5\n",
      "\n",
      "\n",
      "... Patriot Act, it was kept secret from the public and even parts of Congress.After the program was revealed, President Barack Obama, some members of Congress, and intelligence community leaders spoke out in defense of it. Then-NSA head Keith Alexander argued that \"fewer than 300\" unique numbers had been queried in 2012, saying that the program was vital in foiling terrorist plots. These plots would fail to materialize, and an oversight  ... \n",
      "\n",
      "tf-idf score= 1.3911028484403496\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7141\n",
      "Keywords:\n",
      "\n",
      "Why Silicon Valley's NSA deal helps them, but not you\n",
      "\n",
      "title score =  0\n",
      "obama -0.9545321839553367 4\n",
      "\n",
      "\n",
      "... we'll continue to encourage Congress to take additional steps to address all of the reforms we believe are needed.As one Obama aide told Politico more simply, the deal was \"understood to resolve the question of transparency around national security.\" As far as the courts were concerned, the tech companies had won.It's the kind of compromise President Obama lovesSo what did they win, exactly? On the transparency side, companies get to  ... \n",
      "\n",
      "tf-idf score= 1.366697535309817\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7297\n",
      "Keywords:\n",
      "\n",
      "US Treasury won't 'mint the coin' to avoid debt ceiling debate\n",
      "\n",
      "title score =  0\n",
      "obama -0.9111443574119122 3\n",
      "\n",
      "\n",
      "... negotiations and will need to be addressed at some point this year, with Congressional Republicans as far apart from the Obama administration and Democratic leadership as ever. Hence, #MintTheCoin.We may never know the real limits of the law It was never entirely clear whether using a platinum coin to expand the debt limit would survive a challenge in federal court, although a co-author of the 1997 bill was quite sure  ... \n",
      "\n",
      "tf-idf score= 1.3045749200684618\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7208\n",
      "Keywords:\n",
      "\n",
      "Thanks a lot, Healthcare.gov        \n",
      "\n",
      "title score =  0\n",
      "obama -0.8393300238227961 5\n",
      "\n",
      "\n",
      "... Healthcare.gov.It's the first time a major law has depended so heavily on the internet. Since the ACA passed, President Barack Obama has reportedly ended every health care meeting the same way: \"If the website doesn't work, nothing else matters.\" \"The system is unavailable\"Despite his prescient warning, the president didn't seem to realize that the website not working was an actual possibility.Healthcare.gov was frighteningly dysfunctional on day one. Users experienced multi-hour  ... \n",
      "\n",
      "tf-idf score= 1.2017512810482875\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7066\n",
      "Keywords:\n",
      "\n",
      "Icy US-Russia relations revive Cold War fears\n",
      "\n",
      "title score =  0\n",
      "obama -0.8260374668844259 4\n",
      "\n",
      "\n",
      "... send troops into Russia, and have so far chosen to pressure the Kremlin through economic and diplomatic channels. President Barack Obama and his Russian counterpart, Vladimir Putin, have already exchanged targeted economic sanctions, and Russia was excluded from a meeting of the elite G8 group of countries last month. This week, NASA suspended most of its operations with Russia's space agency, citing Russia's \"ongoing violation of Ukraine's sovereignty and territorial  ... \n",
      "\n",
      "tf-idf score= 1.1827190209411877\n",
      "\n",
      "\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "\n",
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "title_score = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "factor = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\n",
    "    \n",
    "    for docID in docs_having_query_term_in_title:\n",
    "        if docID in title_score:\n",
    "            title_score[docID] += idf\n",
    "        else:\n",
    "            title_score[docID] = idf\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        \n",
    "        tf_doc = getReference[docID].count_words(query_term, 0)\n",
    "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\n",
    "        tfidf_doc = (tf_doc)\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query * tfidf_doc)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] = idf\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query * tfidf_doc)\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] += idf\n",
    "            \n",
    "print(title_score)\n",
    "\n",
    "for docID in scores:\n",
    "    \n",
    "    #if documentLength[docID] != 0:\n",
    "    scores[docID] *= factor[docID]\n",
    "    if docID in title_score:\n",
    "        scores[docID] *= 1 + title_score[docID]\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    \n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    print()\n",
    "    print(subset[get_index[sorted_scores[i][0]]][1])\n",
    "    print()\n",
    "    if sorted_scores[i][0] not in title_score:\n",
    "        print('title score = ',0)\n",
    "    else:\n",
    "        print('title score = ',title_score[sorted_scores[i][0]])\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\n",
    "        print(getReference[docID].count_words(wordsInDoc[docID][j][1], 0))\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    \n",
    "    for word in subset[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word)\n",
    "        check_with = check_with.lower()\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print('\\n')\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    print('\\n')\n",
    "    print('============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
