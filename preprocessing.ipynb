{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__1Nxb4gRIeN"
   },
   "source": [
    "Done so far :\n",
    "\n",
    "\n",
    "*   Lemmatization\n",
    "*   Stop Words Removal\n",
    "\n",
    "Verify :\n",
    "\n",
    "* Normalization - removing accents, etc.\n",
    "* Dates replaced with strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "### pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlcodes(document):\n",
    "    \n",
    "    replacement = {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                  }\n",
    "    for str in replacement:\n",
    "        document = document.replace(str, replacement[str])\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # sentence=arr[0][4]\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PvIwQ6ySVHv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data.npy\n",
    "# data.npy is a 2D array containing the dataset information as\n",
    "# data[i][0] : docID of ith document\n",
    "# data[i][1] : title of ith document\n",
    "# data[i][4] : content of ith document\n",
    "\n",
    "data = np.load('data.npy',allow_pickle = True)\n",
    "# sentence = data[0][4]\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6aQ7ZcPCH4d",
    "outputId": "8fc93dc4-51e3-48ff-fa64-d5bbef5847a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204135\n"
     ]
    }
   ],
   "source": [
    "# creating a map {index_in_data_npy, docID}\n",
    "\n",
    "# ex. if ith element in data has docID j,\n",
    "# get_docID[i] will return j\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = int(data[i][0])\n",
    "    get_index[int(data[i][0])] = i\n",
    "    # print(get_docID[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def replace_dates(documentString):\n",
    "    \n",
    "    regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    \n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp = date\n",
    "        date = date.replace('.', '/')\n",
    "        date = date.replace('-', '/')\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        newDate = newDate.replace(' ', '')\n",
    "        documentString = documentString.replace(tmp, newDate)\n",
    "        # print(newDate)\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1891\n",
      "['know', 'States', 'impose', 'reasonable', 'limitation', 'less', 'gun', 'crime', 'less', 'homicide', 'Sen', 'Chris', 'Murphy', 'Conn', 'speak', 'Senate', 'floor', 'June', '15', 'Readers', 'ask', 'fact', 'check', 'gun', 'rhetoric', 'use', 'Democrats', 'wake', 'mass', 'shoot', 'Orlando', 'one', 'case', 'already', 'delve', 'material', 'claim', 'new', 'let', 'take', 'look', 'start', 'Murphy', 'statement', 'Facts', 'Murphy', 'staff', 'say', 'refer', 'chart', 'appear', 'National', 'Journal', '2015', 'turn', 'carefully', 'checked', 'chart', 'President', 'Obama', 'make', 'similar', 'carefully', 'phrase', 'claim', 'gun', 'death', 'Note', 'Murphy', 'refer', 'homicide', 'gun', 'crime', 'President', 'Obama', 'earn', 'Two', 'Pinocchios', 'Readers', 'check', 'full', 'fact', 'check', 'summary', 'note', 'gun', 'death', '60', 'percent', '2013', 'actually', 'suicide', 'data', 'use', 'National', 'Journal', 'chart', 'calculates', 'number', 'gun', 'related', 'death', 'per', '100', '000', 'people', 'include', 'gun', 'death', 'include', 'homicide', 'suicide', 'accidental', 'gun', 'death', 'legal', 'intervention', 'involve', 'firearm', 'remove', 'suicide', 'total', 'reran', 'number', 'case', 'make', 'huge', 'difference', 'Half', '10', 'state', 'low', 'gun', 'death', 'rate', 'turn', 'state', 'less', 'restrictive', 'gun', 'law', 'Moreover', 'counting', 'gun', 'law', 'certainly', 'open', 'interpretation', 'also', 'affect', 'outcome', 'enough', 'count', 'law', 'figure', 'reason', 'gun', 'death', 'low', 'one', 'state', 'another', 'One', 'would', 'need', 'specifically', 'determine', 'whether', 'certain', 'law', 'effect', 'time', 'gun', 'death', 'rate', 'state', 'instance', 'Murphy', 'claim', 'worthy', 'Three', 'Pinocchios', 'specifically', 'refer', 'homicide', 'rather', 'gun', 'death', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21pk0d8', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'id', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'c3a12994', '0d15', '4748', '869d', '9dca491526fa', 'userType', 'ANONYMOUS', 'gameId', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'murphy', '1', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466118080690', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'true', 'photoUploaded', 'false', 'shareUrl', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21pk0d8', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '69a73c61', '9b60', '4a7d', '88b8', '73090f0e953c', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466118080693', 'lastUpdated', '1466118122699', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466118122697', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'AR', '15', 'style', 'weapon', 'legal', 'United', 'States', '2004', 'ban', '10', 'year', 'coincidental', 'massive', 'increase', 'mass', 'shooting', 'country', '2004', 'Murphy', 'June', '15', 'another', 'problematic', 'claim', 'Murphy', 'staff', 'could', 'point', 'specific', 'data', 'back', 'significant', 'contrary', 'data', 'show', '10', 'year', 'assault', 'weapon', 'ban', 'little', 'effect', '2004', 'study', 'Justice', 'Department', 'found', 'ban', 'impact', 'gun', 'violence', 'mixed', 'best', 'ban', 'renew', 'likely', 'effect', 'gun', 'violence', 'likely', 'small', 'best', 'perhaps', 'small', 'reliable', 'measurement', 'report', 'say', 'assault', 'weapon', 'rarely', 'use', 'gun', 'crime', 'James', 'Alan', 'Fox', 'Northeastern', 'University', 'professor', 'collect', 'data', 'back', '1982', 'show', 'assault', 'weapon', 'account', '24', '6', 'percent', 'public', 'mass', 'shooting', 'Assault', 'weapon', 'commonplace', 'mass', 'shooting', 'gun', 'control', 'advocate', 'believe', 'Fox', 'write', '2012', 'article', 'journal', 'Homicide', 'Studies', 'Instead', 'semiautomatic', 'handgun', '47', '9', 'percent', 'far', 'prevalent', 'random', 'massacre', 'firearm', 'would', 'typically', 'classify', 'assault', 'weapon', 'assault', 'weapon', 'ban', 'make', 'difference', 'mass', 'shooting', 'significantly', 'accord', 'Fox', 'data', '1976', '1994', '18', 'mass', 'shooting', 'per', 'year', 'ban', '1995', '2004', '19', 'incident', 'per', 'year', 'ban', '2011', 'average', 'go', 'nearly', '21', '2016', 'study', 'publish', 'Applied', 'Economics', 'Benjamin', 'Blau', 'Utah', 'State', 'University', 'colleague', 'also', 'look', 'whether', 'state', 'federal', 'law', 'assault', 'rifle', 'affected', 'whether', 'weapon', 'use', 'public', 'shooting', '1982', '2014', 'study', 'seem', 'indicate', 'Federal', 'assault', 'rifle', 'ban', 'individual', 'State', 'assault', 'rifle', 'ban', 'affect', 'likelihood', 'assault', 'rifle', 'use', 'mass', 'shoot', 'Blau', 'say', 'Said', 'differently', 'type', 'ban', 'appear', 'deter', 'use', 'assault', 'rifle', 'mass', 'shoot', 'data', 'use', 'determine', 'whether', 'assault', 'rifle', 'ban', 'negatively', 'influence', 'likelihood', 'occurrence', 'mass', 'shoot', 'conclusive', 'colleague', 'Christopher', 'Ingraham', 'point', 'assault', 'style', 'rifle', 'use', 'seven', 'eight', 'high', 'profile', 'public', 'mass', 'shooting', 'since', 'July', 'last', 'year', 'certainly', 'raise', 'profile', 'weapon', 'data', 'far', 'show', 'link', 'use', 'weapon', 'lift', 'ban', 'Murphy', 'asserts', 'bottom', 'line', 'statistic', 'cite', 'story', 'told', 'Wednesday', 'show', 'undeniable', 'gun', 'kill', 'thousand', 'Americans', 'every', 'year', 'say', 'Murphy', 'spokesman', 'Chris', 'Harris', 'step', 'back', 'look', 'totality', 'data', 'easy', 'access', 'firearm', 'way', 'regulation', 'lead', 'increase', 'gun', 'death', 'homicide', 'suicide', 'true', 'U', 'compare', 'state', 'state', 'compare', 'U', 'nation', 'Pinocchio', 'Test', 'Murphy', 'say', 'coincidental', 'mass', 'shooting', 'increase', 'since', 'ban', 'lift', 'data', 'show', 'ban', 'particularly', 'effective', 'first', 'place', 'mass', 'shooting', 'increase', 'significantly', 'since', 'data', 'set', 'relatively', 'small', 'maybe', 'something', 'change', 'past', 'year', 'claim', 'worthy', 'Three', 'Pinocchios', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21piYOd', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'id', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'a0847542', 'f10f', '4409', 'a330', '85f46a3b3c8b', 'userType', 'ANONYMOUS', 'gameId', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'trump', '2', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466118160561', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'true', 'photoUploaded', 'false', 'shareUrl', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21piYOd', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '43f3b445', 'ee45', '45b7', 'bfba', 'cf6132ccda74', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466118160563', 'lastUpdated', '1466126246609', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466126246607', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'America', 'absolutely', 'awash', 'easily', 'obtainable', 'firearm', 'go', 'gun', 'show', 'local', 'convention', 'center', 'come', 'away', 'fully', 'automatic', 'assault', 'rifle', 'without', 'background', 'check', 'likely', 'without', 'show', 'identification', 'card', 'wait', 'Sen', 'Minority', 'Leader', 'Harry', 'Reid', 'Nev', 'quote', 'al', 'Qaeda', 'spokesman', 'statement', 'Senate', 'floor', 'June', '15', '2016', 'indeed', 'al', 'Qaeda', 'spokesman', 'Adam', 'Gadahn', 'use', 'exact', 'language', '2011', 'American', 'born', 'Gadahn', 'later', 'kill', 'drone', 'strike', '2015', 'clip', 'even', 'terrorist', 'get', 'fact', 'wrong', 'Senate', 'leader', 'uncritically', 'quote', 'Reid', 'put', 'terrorist', 'talk', 'gun', 'show', 'loophole', 'specifically', 'point', 'flaw', 'nation', 'gun', 'law', 'allows', 'convict', 'terrorist', 'slip', 'big', 'wide', 'hole', 'slip', 'Actually', 'buy', 'fully', 'automatic', 'assault', 'rifle', 'gun', 'show', 'al', 'Qaeda', 'spokesman', 'extension', 'Reid', 'mix', 'semiautomatic', 'weapon', 'automatic', 'weapon', 'Semiautomatic', 'define', '1968', 'Gun', 'Control', 'Act', 'mean', 'one', 'pull', 'trigger', 'equates', 'one', 'bullet', 'leave', 'barrel', 'Fully', 'automatic', 'rifle', 'available', 'United', 'States', 'require', 'six', 'month', 'paperwork', 'Bureau', 'Alcohol', 'Tobacco', 'Firearms', 'Explosives', 'also', 'significantly', 'expensive', 'single', 'fire', 'counterpart', 'ban', 'many', 'state', 'Moreover', '1986', 'law', 'ban', 'new', 'one', 'automatic', 'weapon', 'purchase', 'would', 'old', 'one', 'Still', 'one', 'could', 'purchase', 'semiautomatic', 'weapon', 'retrofit', 'something', 'call', 'auto', 'sear', 'mimic', 'automatic', 'weapon', 'though', 'weapon', 'still', 'fit', 'definition', 'semiautomatic', 'modification', 'require', 'permission', 'ATF', 'video', 'describe', 'one', 'product', 'call', 'gun', 'show', 'loophole', 'refers', 'private', 'sale', 'within', 'state', 'line', 'license', 'gun', 'dealer', 'gun', 'show', 'must', 'run', 'background', 'check', 'Anyone', 'gun', 'show', 'sell', 'someone', 'state', 'must', 'run', 'background', 'check', 'number', 'state', 'include', 'California', 'New', 'York', 'require', 'background', 'check', 'gun', 'transaction', 'state', 'require', 'handgun', 'purchase', 'gun', 'show', 'require', 'background', 'check', 'matter', 'policy', 'matter', 'state', 'law', 'say', 'Gadahn', 'extensive', 'Reid', 'speak', 'sloppily', 'Reid', 'spokesman', 'decline', 'provide', 'record', 'comment', 'Pinocchio', 'Test', 'may', 'make', 'sense', 'award', 'Pinocchios', 'dead', 'al', 'Qaeda', 'operative', 'case', 'Reid', 'clearly', 'state', 'Gadahn', 'correct', 'even', 'believe', 'quote', 'make', 'noteworthy', 'point', 'Reid', 'quote', 'fully', 'automatic', 'line', 'twice', 'without', 'inform', 'listener', 'actually', 'possible', 'Moreover', 'Reid', 'make', 'comment', 'prepared', 'statement', 'publicly', 'release', 'news', 'release', 'could', 'still', 'update', 'correction', 'Two', 'Pinocchios', 'rating', 'scale', 'Send', 'u', 'fact', 'check', 'fill', 'form', 'Check', '2016', 'candidate', 'fact', 'check', 'page', 'Sign', 'Fact', 'Checker', 'weekly', 'newsletter', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'id', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'd6b018de', '31f6', '46f9', '8dd6', 'fd20a63ac97a', 'userType', 'ANONYMOUS', 'gameId', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'reid', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466117973960', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'false', 'photoUploaded', 'false', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '1a6d57ae', 'd8a8', '43e7', '8b1e', 'f60ce8ccafa1', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466117973962', 'lastUpdated', '1466117973962', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466117973960', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter']\n"
     ]
    }
   ],
   "source": [
    "# Before cleaning \n",
    "\n",
    "print(len(lemma_stop(data[get_index[212853]][4])))\n",
    "print(lemma_stop(data[get_index[212853]][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1116\n",
      "['know', 'States', 'impose', 'reasonable', 'limitation', 'less', 'gun', 'crime', 'less', 'homicide', 'Sen', 'Chris', 'Murphy', 'Conn', 'speak', 'Senate', 'floor', 'June', '15', 'Readers', 'ask', 'fact', 'check', 'gun', 'rhetoric', 'use', 'Democrats', 'wake', 'mass', 'shoot', 'Orlando', 'one', 'case', 'already', 'delve', 'material', 'claim', 'new', 'let', 'take', 'look', 'start', 'Murphy', 'statement', 'Facts', 'Murphy', 'staff', 'say', 'refer', 'chart', 'appear', 'National', 'Journal', '2015', 'turn', 'carefully', 'checked', 'chart', 'President', 'Obama', 'make', 'similar', 'carefully', 'phrase', 'claim', 'gun', 'death', 'Note', 'Murphy', 'refer', 'homicide', 'gun', 'crime', 'President', 'Obama', 'earn', 'Two', 'Pinocchios', 'Readers', 'check', 'full', 'fact', 'check', 'summary', 'note', 'gun', 'death', '60', 'percent', '2013', 'actually', 'suicide', 'data', 'use', 'National', 'Journal', 'chart', 'calculates', 'number', 'gun', 'related', 'death', 'per', '100', '000', 'people', 'include', 'gun', 'death', 'include', 'homicide', 'suicide', 'accidental', 'gun', 'death', 'legal', 'intervention', 'involve', 'firearm', 'remove', 'suicide', 'total', 'reran', 'number', 'case', 'make', 'huge', 'difference', 'Half', '10', 'state', 'low', 'gun', 'death', 'rate', 'turn', 'state', 'less', 'restrictive', 'gun', 'law', 'Moreover', 'counting', 'gun', 'law', 'certainly', 'open', 'interpretation', 'also', 'affect', 'outcome', 'enough', 'count', 'law', 'figure', 'reason', 'gun', 'death', 'low', 'one', 'state', 'another', 'One', 'would', 'need', 'specifically', 'determine', 'whether', 'certain', 'law', 'effect', 'time', 'gun', 'death', 'rate', 'state', 'instance', 'Murphy', 'claim', 'worthy', 'Three', 'Pinocchios', 'specifically', 'refer', 'homicide', 'rather', 'gun', 'death', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'checker', 'rating', 'murphy', '1', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'AR', '15', 'style', 'weapon', 'legal', 'United', 'States', '2004', 'ban', '10', 'year', 'coincidental', 'massive', 'increase', 'mass', 'shooting', 'country', '2004', 'Murphy', 'June', '15', 'another', 'problematic', 'claim', 'Murphy', 'staff', 'could', 'point', 'specific', 'data', 'back', 'significant', 'contrary', 'data', 'show', '10', 'year', 'assault', 'weapon', 'ban', 'little', 'effect', '2004', 'study', 'Justice', 'Department', 'found', 'ban', 'impact', 'gun', 'violence', 'mixed', 'best', 'ban', 'renew', 'likely', 'effect', 'gun', 'violence', 'likely', 'small', 'best', 'perhaps', 'small', 'reliable', 'measurement', 'report', 'say', 'assault', 'weapon', 'rarely', 'use', 'gun', 'crime', 'James', 'Alan', 'Fox', 'Northeastern', 'University', 'professor', 'collect', 'data', 'back', '1982', 'show', 'assault', 'weapon', 'account', '24', '6', 'percent', 'public', 'mass', 'shooting', 'Assault', 'weapon', 'commonplace', 'mass', 'shooting', 'gun', 'control', 'advocate', 'believe', 'Fox', 'write', '2012', 'article', 'journal', 'Homicide', 'Studies', 'Instead', 'semiautomatic', 'handgun', '47', '9', 'percent', 'far', 'prevalent', 'random', 'massacre', 'firearm', 'would', 'typically', 'classify', 'assault', 'weapon', 'assault', 'weapon', 'ban', 'make', 'difference', 'mass', 'shooting', 'significantly', 'accord', 'Fox', 'data', '1976', '1994', '18', 'mass', 'shooting', 'per', 'year', 'ban', '1995', '2004', '19', 'incident', 'per', 'year', 'ban', '2011', 'average', 'go', 'nearly', '21', '2016', 'study', 'publish', 'Applied', 'Economics', 'Benjamin', 'Blau', 'Utah', 'State', 'University', 'colleague', 'also', 'look', 'whether', 'state', 'federal', 'law', 'assault', 'rifle', 'affected', 'whether', 'weapon', 'use', 'public', 'shooting', '1982', '2014', 'study', 'seem', 'indicate', 'Federal', 'assault', 'rifle', 'ban', 'individual', 'State', 'assault', 'rifle', 'ban', 'affect', 'likelihood', 'assault', 'rifle', 'use', 'mass', 'shoot', 'Blau', 'say', 'Said', 'differently', 'type', 'ban', 'appear', 'deter', 'use', 'assault', 'rifle', 'mass', 'shoot', 'data', 'use', 'determine', 'whether', 'assault', 'rifle', 'ban', 'negatively', 'influence', 'likelihood', 'occurrence', 'mass', 'shoot', 'conclusive', 'colleague', 'Christopher', 'Ingraham', 'point', 'assault', 'style', 'rifle', 'use', 'seven', 'eight', 'high', 'profile', 'public', 'mass', 'shooting', 'since', 'July', 'last', 'year', 'certainly', 'raise', 'profile', 'weapon', 'data', 'far', 'show', 'link', 'use', 'weapon', 'lift', 'ban', 'Murphy', 'asserts', 'bottom', 'line', 'statistic', 'cite', 'story', 'told', 'Wednesday', 'show', 'undeniable', 'gun', 'kill', 'thousand', 'Americans', 'every', 'year', 'say', 'Murphy', 'spokesman', 'Chris', 'Harris', 'step', 'back', 'look', 'totality', 'data', 'easy', 'access', 'firearm', 'way', 'regulation', 'lead', 'increase', 'gun', 'death', 'homicide', 'suicide', 'true', 'U', 'compare', 'state', 'state', 'compare', 'U', 'nation', 'Pinocchio', 'Test', 'Murphy', 'say', 'coincidental', 'mass', 'shooting', 'increase', 'since', 'ban', 'lift', 'data', 'show', 'ban', 'particularly', 'effective', 'first', 'place', 'mass', 'shooting', 'increase', 'significantly', 'since', 'data', 'set', 'relatively', 'small', 'maybe', 'something', 'change', 'past', 'year', 'claim', 'worthy', 'Three', 'Pinocchios', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'checker', 'rating', 'trump', '2', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'America', 'absolutely', 'awash', 'easily', 'obtainable', 'firearm', 'go', 'gun', 'show', 'local', 'convention', 'center', 'come', 'away', 'fully', 'automatic', 'assault', 'rifle', 'without', 'background', 'check', 'likely', 'without', 'show', 'identification', 'card', 'wait', 'Sen', 'Minority', 'Leader', 'Harry', 'Reid', 'Nev', 'quote', 'al', 'Qaeda', 'spokesman', 'statement', 'Senate', 'floor', 'June', '15', '2016', 'indeed', 'al', 'Qaeda', 'spokesman', 'Adam', 'Gadahn', 'use', 'exact', 'language', '2011', 'American', 'born', 'Gadahn', 'later', 'kill', 'drone', 'strike', '2015', 'clip', 'even', 'terrorist', 'get', 'fact', 'wrong', 'Senate', 'leader', 'uncritically', 'quote', 'Reid', 'put', 'terrorist', 'talk', 'gun', 'show', 'loophole', 'specifically', 'point', 'flaw', 'nation', 'gun', 'law', 'allows', 'convict', 'terrorist', 'slip', 'big', 'wide', 'hole', 'slip', 'Actually', 'buy', 'fully', 'automatic', 'assault', 'rifle', 'gun', 'show', 'al', 'Qaeda', 'spokesman', 'extension', 'Reid', 'mix', 'semiautomatic', 'weapon', 'automatic', 'weapon', 'Semiautomatic', 'define', '1968', 'Gun', 'Control', 'Act', 'mean', 'one', 'pull', 'trigger', 'equates', 'one', 'bullet', 'leave', 'barrel', 'Fully', 'automatic', 'rifle', 'available', 'United', 'States', 'require', 'six', 'month', 'paperwork', 'Bureau', 'Alcohol', 'Tobacco', 'Firearms', 'Explosives', 'also', 'significantly', 'expensive', 'single', 'fire', 'counterpart', 'ban', 'many', 'state', 'Moreover', '1986', 'law', 'ban', 'new', 'one', 'automatic', 'weapon', 'purchase', 'would', 'old', 'one', 'Still', 'one', 'could', 'purchase', 'semiautomatic', 'weapon', 'retrofit', 'something', 'call', 'auto', 'sear', 'mimic', 'automatic', 'weapon', 'though', 'weapon', 'still', 'fit', 'definition', 'semiautomatic', 'modification', 'require', 'permission', 'ATF', 'video', 'describe', 'one', 'product', 'call', 'gun', 'show', 'loophole', 'refers', 'private', 'sale', 'within', 'state', 'line', 'license', 'gun', 'dealer', 'gun', 'show', 'must', 'run', 'background', 'check', 'Anyone', 'gun', 'show', 'sell', 'someone', 'state', 'must', 'run', 'background', 'check', 'number', 'state', 'include', 'California', 'New', 'York', 'require', 'background', 'check', 'gun', 'transaction', 'state', 'require', 'handgun', 'purchase', 'gun', 'show', 'require', 'background', 'check', 'matter', 'policy', 'matter', 'state', 'law', 'say', 'Gadahn', 'extensive', 'Reid', 'speak', 'sloppily', 'Reid', 'spokesman', 'decline', 'provide', 'record', 'comment', 'Pinocchio', 'Test', 'may', 'make', 'sense', 'award', 'Pinocchios', 'dead', 'al', 'Qaeda', 'operative', 'case', 'Reid', 'clearly', 'state', 'Gadahn', 'correct', 'even', 'believe', 'quote', 'make', 'noteworthy', 'point', 'Reid', 'quote', 'fully', 'automatic', 'line', 'twice', 'without', 'inform', 'listener', 'actually', 'possible', 'Moreover', 'Reid', 'make', 'comment', 'prepared', 'statement', 'publicly', 'release', 'news', 'release', 'could', 'still', 'update', 'correction', 'Two', 'Pinocchios', 'rating', 'scale', 'Send', 'u', 'fact', 'check', 'fill', 'form', 'Check', '2016', 'candidate', 'fact', 'check', 'page', 'Sign', 'Fact', 'Checker', 'weekly', 'newsletter', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'shareURL', 'encodeURIComponent', 'checker', 'rating', 'reid', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter']\n"
     ]
    }
   ],
   "source": [
    "# After cleaning (removing JSON, HTML, etc)\n",
    "\n",
    "cleaned_doc=clean_document(data[get_index[212853]][4])\n",
    "print(len(lemma_stop(cleaned_doc)))\n",
    "print(lemma_stop(cleaned_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsbw8tNbrJfL"
   },
   "source": [
    "*Run the following cell once after all pre-processing (removing JSON etc), and store final lemmatized contents of all docs:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibq9gAauMI5Q"
   },
   "outputs": [],
   "source": [
    "# Run once after all pre-processing \n",
    "\n",
    "\n",
    "# # Tester code\n",
    "# import time\n",
    "\n",
    "# s = time.time()\n",
    "\n",
    "# for i in range(0, len(data)) :\n",
    "#     f_content = clean_document(data[i][4])\n",
    "#     contents = f_content\n",
    "#     # print(contents)\n",
    "#     final = lemma_stop (contents)\n",
    "# #     print(type(final))\n",
    "#     # print (final)\n",
    "    \n",
    "# print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary smaller dataset\n",
    "\n",
    "subset = []\n",
    "counter = 0\n",
    "for document in data:\n",
    "    subset.append(document)\n",
    "    counter += 1\n",
    "    if counter == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:50<00:00,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.75669765472412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "for document in tqdm(subset):\n",
    "    # actually modifying the document\n",
    "    document[4] = remove_htmlcodes(document[4])\n",
    "    \n",
    "    # not actually modifying the document\n",
    "    modifiedContent = replace_dates(document[4])\n",
    "    modifiedContent = lemma_stop(modifiedContent)\n",
    "    modifiedTitle = lemma_stop((document[1]))\n",
    "    # modifiedContent = lemma_stop(clean_document(document[4]))\n",
    "    # modifiedTitle = lemma_stop(clean_document(document[1]))\n",
    "    titles.append(modifiedTitle)\n",
    "    contents.append(modifiedContent)\n",
    "    \n",
    "print(time.time() - start)  # 110.26236414909363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "contents_temp = contents\n",
    "titles_temp = titles\n",
    "for i in range(1000):\n",
    "    for j in range(len(contents[i])):\n",
    "        contents[i][j] = unidecode.unidecode(contents[i][j])\n",
    "    for j in range(len(titles[i])):\n",
    "        titles[i][j] = unidecode.unidecode(titles[i][j])\n",
    "# for document in contents_temp:\n",
    "#     for word in document:\n",
    "#         word = unidecode.unidecode(word)\n",
    "# for title in titles_temp:\n",
    "#     for word in title:\n",
    "#         word = unidecode.unidecode(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [],
   "source": [
    "import trie\n",
    "\n",
    "# Create map from docID of the document to an object of class Node \n",
    "# (i.e, the corresponding document trie structure)\n",
    "# ex. if the docID of the document is 1, \n",
    "# getReference[1] gives the object which is the trie structure of docID 1\n",
    "\n",
    "getReference = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentRoot = []\n",
    "collection = trie.CollectionNode()\n",
    "\n",
    "# initializing the root for 1000 documents\n",
    "for i in range(1000):\n",
    "    newDocument = trie.Node()\n",
    "    documentRoot.append(newDocument)\n",
    "    getReference[get_docID[i]] = newDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 29.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.7455792427063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the documents\n",
    "\n",
    "max_tf = {}\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(1000)):\n",
    "    for w in contents_temp[i]:\n",
    "        collection.add_document(w, 0, get_docID[i])\n",
    "        documentRoot[i].add(w, 0)\n",
    "        if get_docID[i] in max_tf:\n",
    "            max_tf[get_docID[i]] = max(documentRoot[i].count_words(w, 0), max_tf[get_docID[i]])\n",
    "        else:\n",
    "            max_tf[get_docID[i]] = documentRoot[i].count_words(w, 0)\n",
    "    for w in titles_temp[i]:\n",
    "        collection.add_title(w, 0, get_docID[i])\n",
    "        \n",
    "print(time.time() - start)  #39.19705152511597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import queue\n",
    "\n",
    "documentLength = {}\n",
    "N = len(documentRoot)\n",
    "\n",
    "for i in tqdm(range(len(documentRoot))):\n",
    "    \n",
    "    docID = get_docID[i]\n",
    "    length = 0\n",
    "    document = documentRoot[i]\n",
    "    q = queue.Queue()\n",
    "    q.put([document, ''])\n",
    "\n",
    "    while q.qsize() > 0:\n",
    "\n",
    "        current = q.get()\n",
    "        reference = current[0]\n",
    "        word = current[1]\n",
    "\n",
    "        if reference.words > 0:\n",
    "            df = len(collection.get_doc_list(word, 0))\n",
    "            idf = math.log10(N/df)\n",
    "            # print(word, reference.words, df)\n",
    "            length += (reference.words * idf) ** 2\n",
    "\n",
    "        for i in range(256):\n",
    "            if reference.children[i] is not None:\n",
    "                new_word = word + chr(i)\n",
    "                q.put([reference.children[i], new_word])\n",
    "\n",
    "    # print(length**0.5)\n",
    "    documentLength[docID] = length**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.getsizeof(documentRoot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9w6bB9rJfg",
    "outputId": "2e5b7bc8-2e68-4de8-f81d-a142a989c902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['net', 'neutrality']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "query = 'net neutrality'\n",
    "final_query = replace_dates(query)\n",
    "final_query = lemma_stop(final_query)\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "print(final_query)\n",
    "print(len(final_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4Kqgk9hrJfj",
    "outputId": "04bfaa8a-8053-4ab7-e55a-cee2d47edb6f"
   },
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1\n",
    "        \n",
    "    # Test code just to see distribution of query terms in the documents\n",
    "    \n",
    "    # print(w)\n",
    "    # df = len(collection.get_doc_list(w,0))\n",
    "    # print(collection.get_doc_list(w,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(documentRoot[1]), get_docID[1])\n",
    "# document = documentRoot[1]\n",
    "# N = len(documentRoot)\n",
    "\n",
    "# import queue\n",
    "# import math\n",
    "\n",
    "# length = 0\n",
    "# q = queue.Queue()\n",
    "# q.put([document, ''])\n",
    "\n",
    "# while q.qsize() > 0:\n",
    "    \n",
    "#     current = q.get()\n",
    "#     reference = current[0]\n",
    "#     word = current[1]\n",
    "    \n",
    "#     if reference.words > 0:\n",
    "#         df = len(collection.get_doc_list(word, 0))\n",
    "#         idf = math.log10(N/df)\n",
    "#         # print(word, reference.words, df)\n",
    "#         length += (reference.words * idf) ** 2\n",
    "    \n",
    "#     for i in range(256):\n",
    "#         if reference.children[i] is not None:\n",
    "#             new_word = word + chr(i)\n",
    "#             q.put([reference.children[i], new_word])\n",
    "\n",
    "# print(length**0.5)\n",
    "# print(replace_dates(subset[get_index[104]][4]))\n",
    "# replace_dates('12/12')\n",
    "# lemma_stop('12Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfT-kdUqrJfp",
    "outputId": "3c95efd0-76c9-4dfe-e425-e79c089931ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  net\n",
      "\n",
      "df =  43\n",
      "idf =  1.3665315444204134\n",
      "-------------------------------------\n",
      "Term in query =  neutrality\n",
      "\n",
      "df =  17\n",
      "idf =  1.7695510786217261\n",
      "{6913: 3.1360826230421397, 6565: 3.1360826230421397, 7018: 3.1360826230421397, 6768: 3.1360826230421397, 7216: 1.3665315444204134, 6876: 3.1360826230421397, 6846: 3.1360826230421397, 6773: 1.7695510786217261}\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6846\n",
      "Keywords:\n",
      "\n",
      "What can Obama really do for net neutrality?\n",
      "\n",
      "title score =  3.1360826230421397\n",
      "neutrality -1.7175054586622636 16\n",
      "net -1.286147335925095 15\n",
      "\n",
      "\n",
      "Net neutrality might be the most contentious problem the Federal Communications Commission has faced in recent history. The past year has been marked by a landmark legal case that struck down the 2010 Open Internet rules, and a series of polarizing proposals for new ones. Protestors have driven a record number  ... \n",
      "\n",
      "tf-idf score= 38.96067127710208\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6876\n",
      "Keywords:\n",
      "\n",
      "Without net neutrality, what stops HBO from turning the internet into cable?\n",
      "\n",
      "title score =  3.1360826230421397\n",
      "neutrality -1.415640862897381 9\n",
      "net -1.0932252355363308 9\n",
      "\n",
      "\n",
      "Several years ago, someone came up with a potent worst-case scenario for an internet without net neutrality: it would look like cable TV. With the option to slow down or block individual sites and programs, ISPs would give you the internet equivalent of basic cable, then charge extra for \"News\" and \"Hollywood\" bundles with Digg and YouTube instead of CNN and Showtime. There’s no subtlety, but  ... \n",
      "\n",
      "tf-idf score= 32.542745125363254\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7018\n",
      "Keywords:\n",
      "\n",
      "The world reacts to the FCC's net neutrality vote\n",
      "\n",
      "title score =  3.1360826230421397\n",
      "neutrality -1.200766803350457 10\n",
      "net -0.902886913277773 9\n",
      "\n",
      "\n",
      "... the FCC voted to accept \"Protecting and Promoting the Open Internet,\" a provisional set of rules meant to protect net neutrality. After weeks of discussion inside and outside the FCC, the commission has passed a proposal with a lot of wiggle room and put it up for public comment. People on both sides of the aisle are just as conflicted as you’d expect. There’s been a tremendous amount of debate  ... \n",
      "\n",
      "tf-idf score= 27.28669607955341\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6768\n",
      "Keywords:\n",
      "\n",
      "How Facebook stumbled into India's fight for net neutrality\n",
      "\n",
      "title score =  3.1360826230421397\n",
      "neutrality -1.200766803350457 5\n",
      "net -0.8784845642702657 4\n",
      "\n",
      "\n",
      "The fight for net neutrality has come to India. Last week, two companies withdrew from Facebook's Internet.org project, citing neutrality concerns, and the remaining companies are facing mounting pressure to bail out. But while Internet.org has drawn most of the headlines in the US, it's just one part of a much larger struggle in  ... \n",
      "\n",
      "tf-idf score= 26.970170847414774\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6565\n",
      "Keywords:\n",
      "\n",
      "The net neutrality fight is now about data collection\n",
      "\n",
      "title score =  3.1360826230421397\n",
      "neutrality -1.032238129196007 2\n",
      "net -0.7971434009119078 2\n",
      "\n",
      "\n",
      "... II of the Communications Act. The ruling is still facing down various challenges, but the big issue of the net neutrality debate is effectively settled: from here on out, providers will have to treat all the web traffic on their network equally.But net neutrality isn’t the only consequence of Title II classification, and a new fight is taking shape around a relatively overlooked portion of the ruling. The fight centers  ... \n",
      "\n",
      "tf-idf score= 23.72908498724404\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6913\n",
      "Keywords:\n",
      "\n",
      "The father of net neutrality could become the next lieutenant governor of New York \n",
      "\n",
      "title score =  3.1360826230421397\n",
      "neutrality -1.032238129196007 3\n",
      "net -0.7591841913446741 2\n",
      "\n",
      "\n",
      "... 9th could see another tech milestone: the election of Tim Wu, a Columbia law professor nicknamed the \"father of net neutrality,\" as the Democratic nominee for lieutenant governor of New York state.If Wu wins in the primary against his rival Kathy Hochul, a former Congresswoman, it would be a huge upset, making him very likely to win the general election in November. Wu's running mate is Zephyr Teachout, a fellow  ... \n",
      "\n",
      "tf-idf score= 23.236712403917277\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6773\n",
      "Keywords:\n",
      "\n",
      "Net neutrality is now a video game\n",
      "\n",
      "title score =  1.7695510786217261\n",
      "neutrality -1.5282486588096726 8\n",
      "net -1.1180712636167018 7\n",
      "\n",
      "\n",
      "One of the past year’s greatest political victories is that net neutrality — a slightly wonky name for a set of rules governing which companies can send which traffic at which speed from which carrier — has become a hot enough political topic to draw millions of messages to the FCC, Congress, and the White House. So maybe it shouldn’t be  ... \n",
      "\n",
      "tf-idf score= 22.98472021523654\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6578\n",
      "Keywords:\n",
      "\n",
      "Facebook's Indian Free Basics program is stuck in limbo\n",
      "\n",
      "title score =  0\n",
      "neutrality -1.1970492590676383 6\n",
      "net -0.9646105019438211 7\n",
      "\n",
      "\n",
      "Last year, Facebook's Internet.org found itself at the center of India’s net neutrality debate. Over 1 million people wrote to TRAI, the country's Telecom and internet regulator, to share their thoughts on the subject during the comment period last spring.Nearly nine months later, we are no closer to having official guidelines on net neutrality, and in a surprising move, the regulator last  ... \n",
      "\n",
      "tf-idf score= 6.779143613437462\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  52\n",
      "Keywords:\n",
      "\n",
      "The Dragonslayer\n",
      "\n",
      "title score =  0\n",
      "neutrality -1.187461908022474 13\n",
      "net -0.9170145890189616 13\n",
      "\n",
      "\n",
      "... John Oliver called Tom Wheeler a dingo.The host of Last Week Tonight had set his sights on the then-raging net neutrality debate, acerbically calling out broadband providers like Comcast and Verizon for their throttling antics and intense Congressional lobbying. Midway through the segment, Oliver dryly pointed to President Obama’s appointment of former cable and wireless lobbyist Wheeler as the new head of the Federal Communications Commission — \"the equivalent of  ... \n",
      "\n",
      "tf-idf score= 6.599812172972239\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7121\n",
      "Keywords:\n",
      "\n",
      "Why you should be scared of Comcast and Time Warner Cable merging\n",
      "\n",
      "title score =  0\n",
      "neutrality -1.0174918702074924 3\n",
      "net -0.7857556380417376 3\n",
      "\n",
      "\n",
      "... regulators, it could cement the kind of monolithic monopolies that have plagued cable subscribers all along, raising concerns over net neutrality and competition in the marketplace. Despite the very real potential for a media dystopia, however, there could be a silver lining: the chance that the deal could help break down a wall that's kept innovation out of the living room for years. It would turn the behemoth into a  ... \n",
      "\n",
      "tf-idf score= 5.655133175664448\n",
      "\n",
      "\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "title_score = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "factor = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    # print('List of documents with this term=', docs_having_query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\n",
    "    for docID in docs_having_query_term_in_title:\n",
    "        if docID in title_score:\n",
    "            title_score[docID] += math.log10(N/df)\n",
    "        else:\n",
    "            title_score[docID] = math.log10(N/df)\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        # print(docID)\n",
    "        tf_doc = getReference[docID].count_words(query_term, 0)\n",
    "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\n",
    "        # print('tf for doc',docID,'is',tf_doc)\n",
    "        # tfidf_doc_query = tf_doc * idf\n",
    "        # tfidf_doc = 1 + math.log10(tf_doc)\n",
    "        tfidf_doc = (tf_doc)\n",
    "        # tfidf_doc_query = (tf_doc)\n",
    "        \n",
    "        # print('tfidf for doc',doc,'is',tfidf_doc)\n",
    "        # print()\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query * tfidf_doc)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] = idf\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query * tfidf_doc)\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] += idf\n",
    "print(title_score)\n",
    "for docID in scores:\n",
    "    if documentLength[docID] != 0:\n",
    "#         scores[docID] = scores[docID]/ math.sqrt(documentLength[docID])\n",
    "        scores[docID] *= factor[docID]\n",
    "        if docID in title_score:\n",
    "            scores[docID] *= 1 + title_score[docID]\n",
    "\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    # print(i)\n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    print()\n",
    "    print(subset[get_index[sorted_scores[i][0]]][1])\n",
    "    print()\n",
    "    if sorted_scores[i][0] not in title_score:\n",
    "        print('title score = ',0)\n",
    "    else:\n",
    "        print('title score = ',title_score[sorted_scores[i][0]])\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\n",
    "        print(getReference[docID].count_words(wordsInDoc[docID][j][1], 0))\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    for word in subset[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word)\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print('\\n')\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    print('\\n')\n",
    "    print('============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-2bbdb3b4f14c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheck_with\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'9/11'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheck_with\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "check_with=replace_dates('9/11')\n",
    "check_with=str(lemma_stop(check_with))\n",
    "print(check_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqsqWNnFNOhO"
   },
   "source": [
    "***Original Text :***\n",
    "\n",
    "      We’re back in Dale Cooper’s position, wandering through a freshly revived world, and trying to catch up with the ways it’s moved on in his absence.\n",
    "\n",
    "***Processed Text :***\n",
    "\n",
    "      We back Dale Cooper position wander freshly revive world try catch way move absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5079\n",
      "6181\n"
     ]
    }
   ],
   "source": [
    "print(len(subset[get_index[6913]][4]))\n",
    "print(len(subset[get_index[6773]][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wordninja\n",
    "# print(wordninja.split('DragonSlayers'))\n",
    "# str = '&nbsp;&amp;&quot;&copy;&reg;&trade;&ldquo;&rdquo;&lsquo;&rsquo;&laquo;&raquo;&lsaquo;&rsaquo;&sect;&para;&bull;&middot;&hellip;&brvbar;&ndash;&mdash;'\n",
    "# str = str.replace(';', ' ')\n",
    "# str = str.replace('&', '&amp')\n",
    "# print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing else matters.\"\n"
     ]
    }
   ],
   "source": [
    "test = 'nothing else matters.&amprdquo'\n",
    "# print(test)\n",
    "# print(test.replace('&ampnbsp',' '))\n",
    "\n",
    "replacement =   {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                }\n",
    "for str in replacement:\n",
    "    test = test.replace(str, replacement[str])\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
