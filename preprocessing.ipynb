{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihaljainn/news-search/blob/master/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__1Nxb4gRIeN",
        "colab_type": "text"
      },
      "source": [
        "Done so far :\n",
        "\n",
        "\n",
        "*   Lemmatization\n",
        "*   Stop Words Removal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZmPnMNqZIXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qZ-TMc9SVHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "####### After importing nltk, run the following only once ######\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpQkEVQOSVHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict={\"J\": wordnet.ADJ, \n",
        "              \"N\": wordnet.NOUN,\n",
        "              \"V\": wordnet.VERB,\n",
        "              \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag,wordnet.NOUN)\n",
        "\n",
        "def lemma_stop(str):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # sentence=arr[0][4]\n",
        "    tokenizer=RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
        "    tokenized = tokenizer.tokenize(str)\n",
        "    lemmatized=[lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
        "    stop_words=set(stopwords.words('english'))\n",
        "    filtered_sentence=[w for w in lemmatized if w not in stop_words]\n",
        "    after_lemma_stop=' '.join(w for w in filtered_sentence)\n",
        "    return after_lemma_stop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "0PvIwQ6ySVHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading data.npy\n",
        "# data.npy is a 2D array containing the dataset information as\n",
        "# data[i][0] : docID of ith document\n",
        "# data[i][1] : title of ith document\n",
        "# data[i][4] : content of ith document\n",
        "\n",
        "data = np.load('/content/drive/My Drive/Information Retrieval/Assignment 1/data.npy',allow_pickle=True)\n",
        "# sentence = data[0][4]\n",
        "# print(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6aQ7ZcPCH4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a map {docID, index_in_data_npy}\n",
        "\n",
        "# ex. if ith element in data has docID j,\n",
        "# get_index[j] will return i\n",
        "\n",
        "get_index = {}\n",
        "\n",
        "for i in range(0, len(data)) :\n",
        "    get_index[int(data[i][0])] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1r5Sup4SVH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######\n",
        "\n",
        "# docIDs = np.load(\"/content/drive/My Drive/Information Retrieval/Assignment 1/docIDs.npy\")\n",
        "# print(docIDs.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swyl681NSVH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######\n",
        "\n",
        "# for i in range(0,len(docIDs)):\n",
        "# doc_id=docIDs[0]\n",
        "# f_content=open(\"/home/nihaljain/3-1/CS F469/Assignment-1/data/content/\"+str(doc_id),\"r\")\n",
        "# contents=f_content.readlines()\n",
        "# print(str(contents))\n",
        "# contents=str(contents)\n",
        "# print(type(contents))\n",
        "# final=lemma_stop(contents)\n",
        "# print(final)\n",
        "#     if(i<5):\n",
        "#         print(final)\n",
        "#         print()\n",
        "# print(doc_id)\n",
        "# f_content=open(\"/home/nihaljain/3-1/CS F469/Assignment-1/data_post_lemma_stop_removal/content/\"+str(doc_id),\"w+\")\n",
        "# f_content.write(\"Hello\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibq9gAauMI5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tester code\n",
        "\n",
        "f_content = data[get_index[1]][4]\n",
        "contents = f_content\n",
        "# print(contents)\n",
        "final = lemma_stop (contents)\n",
        "# print (final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqsqWNnFNOhO",
        "colab_type": "text"
      },
      "source": [
        "***Original Text :***\n",
        "\n",
        "      We’re back in Dale Cooper’s position, wandering through a freshly revived world, and trying to catch up with the ways it’s moved on in his absence.\n",
        "\n",
        "***Processed Text :***\n",
        "\n",
        "      We back Dale Cooper position wander freshly revive world try catch way move absence"
      ]
    }
  ]
}