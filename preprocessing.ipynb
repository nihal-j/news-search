{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__1Nxb4gRIeN"
   },
   "source": [
    "Done so far :\n",
    "\n",
    "\n",
    "*   Lemmatization\n",
    "*   Stop Words Removal\n",
    "\n",
    "Verify :\n",
    "\n",
    "* Normalization - removing accents, etc.\n",
    "* Dates replaced with strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "### pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # sentence=arr[0][4]\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PvIwQ6ySVHv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data.npy\n",
    "# data.npy is a 2D array containing the dataset information as\n",
    "# data[i][0] : docID of ith document\n",
    "# data[i][1] : title of ith document\n",
    "# data[i][4] : content of ith document\n",
    "\n",
    "data = np.load('data.npy',allow_pickle = True)\n",
    "# sentence = data[0][4]\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6aQ7ZcPCH4d",
    "outputId": "8fc93dc4-51e3-48ff-fa64-d5bbef5847a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204135\n"
     ]
    }
   ],
   "source": [
    "# creating a map {index_in_data_npy, docID}\n",
    "\n",
    "# ex. if ith element in data has docID j,\n",
    "# get_docID[i] will return j\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = int(data[i][0])\n",
    "    get_index[int(data[i][0])] = i\n",
    "    # print(get_docID[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def replace_dates(documentString):\n",
    "    \n",
    "    regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    \n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp = date\n",
    "        date = date.replace('.', '/')\n",
    "        date = date.replace('-', '/')\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        newDate = newDate.replace(' ', '')\n",
    "        documentString = documentString.replace(tmp, newDate)\n",
    "        # print(newDate)\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1891\n",
      "['know', 'States', 'impose', 'reasonable', 'limitation', 'less', 'gun', 'crime', 'less', 'homicide', 'Sen', 'Chris', 'Murphy', 'Conn', 'speak', 'Senate', 'floor', 'June', '15', 'Readers', 'ask', 'fact', 'check', 'gun', 'rhetoric', 'use', 'Democrats', 'wake', 'mass', 'shoot', 'Orlando', 'one', 'case', 'already', 'delve', 'material', 'claim', 'new', 'let', 'take', 'look', 'start', 'Murphy', 'statement', 'Facts', 'Murphy', 'staff', 'say', 'refer', 'chart', 'appear', 'National', 'Journal', '2015', 'turn', 'carefully', 'checked', 'chart', 'President', 'Obama', 'make', 'similar', 'carefully', 'phrase', 'claim', 'gun', 'death', 'Note', 'Murphy', 'refer', 'homicide', 'gun', 'crime', 'President', 'Obama', 'earn', 'Two', 'Pinocchios', 'Readers', 'check', 'full', 'fact', 'check', 'summary', 'note', 'gun', 'death', '60', 'percent', '2013', 'actually', 'suicide', 'data', 'use', 'National', 'Journal', 'chart', 'calculates', 'number', 'gun', 'related', 'death', 'per', '100', '000', 'people', 'include', 'gun', 'death', 'include', 'homicide', 'suicide', 'accidental', 'gun', 'death', 'legal', 'intervention', 'involve', 'firearm', 'remove', 'suicide', 'total', 'reran', 'number', 'case', 'make', 'huge', 'difference', 'Half', '10', 'state', 'low', 'gun', 'death', 'rate', 'turn', 'state', 'less', 'restrictive', 'gun', 'law', 'Moreover', 'counting', 'gun', 'law', 'certainly', 'open', 'interpretation', 'also', 'affect', 'outcome', 'enough', 'count', 'law', 'figure', 'reason', 'gun', 'death', 'low', 'one', 'state', 'another', 'One', 'would', 'need', 'specifically', 'determine', 'whether', 'certain', 'law', 'effect', 'time', 'gun', 'death', 'rate', 'state', 'instance', 'Murphy', 'claim', 'worthy', 'Three', 'Pinocchios', 'specifically', 'refer', 'homicide', 'rather', 'gun', 'death', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21pk0d8', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'id', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'c3a12994', '0d15', '4748', '869d', '9dca491526fa', 'userType', 'ANONYMOUS', 'gameId', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'murphy', '1', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466118080690', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'true', 'photoUploaded', 'false', 'shareUrl', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21pk0d8', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '69a73c61', '9b60', '4a7d', '88b8', '73090f0e953c', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466118080693', 'lastUpdated', '1466118122699', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466118122697', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'AR', '15', 'style', 'weapon', 'legal', 'United', 'States', '2004', 'ban', '10', 'year', 'coincidental', 'massive', 'increase', 'mass', 'shooting', 'country', '2004', 'Murphy', 'June', '15', 'another', 'problematic', 'claim', 'Murphy', 'staff', 'could', 'point', 'specific', 'data', 'back', 'significant', 'contrary', 'data', 'show', '10', 'year', 'assault', 'weapon', 'ban', 'little', 'effect', '2004', 'study', 'Justice', 'Department', 'found', 'ban', 'impact', 'gun', 'violence', 'mixed', 'best', 'ban', 'renew', 'likely', 'effect', 'gun', 'violence', 'likely', 'small', 'best', 'perhaps', 'small', 'reliable', 'measurement', 'report', 'say', 'assault', 'weapon', 'rarely', 'use', 'gun', 'crime', 'James', 'Alan', 'Fox', 'Northeastern', 'University', 'professor', 'collect', 'data', 'back', '1982', 'show', 'assault', 'weapon', 'account', '24', '6', 'percent', 'public', 'mass', 'shooting', 'Assault', 'weapon', 'commonplace', 'mass', 'shooting', 'gun', 'control', 'advocate', 'believe', 'Fox', 'write', '2012', 'article', 'journal', 'Homicide', 'Studies', 'Instead', 'semiautomatic', 'handgun', '47', '9', 'percent', 'far', 'prevalent', 'random', 'massacre', 'firearm', 'would', 'typically', 'classify', 'assault', 'weapon', 'assault', 'weapon', 'ban', 'make', 'difference', 'mass', 'shooting', 'significantly', 'accord', 'Fox', 'data', '1976', '1994', '18', 'mass', 'shooting', 'per', 'year', 'ban', '1995', '2004', '19', 'incident', 'per', 'year', 'ban', '2011', 'average', 'go', 'nearly', '21', '2016', 'study', 'publish', 'Applied', 'Economics', 'Benjamin', 'Blau', 'Utah', 'State', 'University', 'colleague', 'also', 'look', 'whether', 'state', 'federal', 'law', 'assault', 'rifle', 'affected', 'whether', 'weapon', 'use', 'public', 'shooting', '1982', '2014', 'study', 'seem', 'indicate', 'Federal', 'assault', 'rifle', 'ban', 'individual', 'State', 'assault', 'rifle', 'ban', 'affect', 'likelihood', 'assault', 'rifle', 'use', 'mass', 'shoot', 'Blau', 'say', 'Said', 'differently', 'type', 'ban', 'appear', 'deter', 'use', 'assault', 'rifle', 'mass', 'shoot', 'data', 'use', 'determine', 'whether', 'assault', 'rifle', 'ban', 'negatively', 'influence', 'likelihood', 'occurrence', 'mass', 'shoot', 'conclusive', 'colleague', 'Christopher', 'Ingraham', 'point', 'assault', 'style', 'rifle', 'use', 'seven', 'eight', 'high', 'profile', 'public', 'mass', 'shooting', 'since', 'July', 'last', 'year', 'certainly', 'raise', 'profile', 'weapon', 'data', 'far', 'show', 'link', 'use', 'weapon', 'lift', 'ban', 'Murphy', 'asserts', 'bottom', 'line', 'statistic', 'cite', 'story', 'told', 'Wednesday', 'show', 'undeniable', 'gun', 'kill', 'thousand', 'Americans', 'every', 'year', 'say', 'Murphy', 'spokesman', 'Chris', 'Harris', 'step', 'back', 'look', 'totality', 'data', 'easy', 'access', 'firearm', 'way', 'regulation', 'lead', 'increase', 'gun', 'death', 'homicide', 'suicide', 'true', 'U', 'compare', 'state', 'state', 'compare', 'U', 'nation', 'Pinocchio', 'Test', 'Murphy', 'say', 'coincidental', 'mass', 'shooting', 'increase', 'since', 'ban', 'lift', 'data', 'show', 'ban', 'particularly', 'effective', 'first', 'place', 'mass', 'shooting', 'increase', 'significantly', 'since', 'data', 'set', 'relatively', 'small', 'maybe', 'something', 'change', 'past', 'year', 'claim', 'worthy', 'Three', 'Pinocchios', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21piYOd', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'id', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'a0847542', 'f10f', '4409', 'a330', '85f46a3b3c8b', 'userType', 'ANONYMOUS', 'gameId', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'trump', '2', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466118160561', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'true', 'photoUploaded', 'false', 'shareUrl', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21piYOd', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '43f3b445', 'ee45', '45b7', 'bfba', 'cf6132ccda74', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466118160563', 'lastUpdated', '1466126246609', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466126246607', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'America', 'absolutely', 'awash', 'easily', 'obtainable', 'firearm', 'go', 'gun', 'show', 'local', 'convention', 'center', 'come', 'away', 'fully', 'automatic', 'assault', 'rifle', 'without', 'background', 'check', 'likely', 'without', 'show', 'identification', 'card', 'wait', 'Sen', 'Minority', 'Leader', 'Harry', 'Reid', 'Nev', 'quote', 'al', 'Qaeda', 'spokesman', 'statement', 'Senate', 'floor', 'June', '15', '2016', 'indeed', 'al', 'Qaeda', 'spokesman', 'Adam', 'Gadahn', 'use', 'exact', 'language', '2011', 'American', 'born', 'Gadahn', 'later', 'kill', 'drone', 'strike', '2015', 'clip', 'even', 'terrorist', 'get', 'fact', 'wrong', 'Senate', 'leader', 'uncritically', 'quote', 'Reid', 'put', 'terrorist', 'talk', 'gun', 'show', 'loophole', 'specifically', 'point', 'flaw', 'nation', 'gun', 'law', 'allows', 'convict', 'terrorist', 'slip', 'big', 'wide', 'hole', 'slip', 'Actually', 'buy', 'fully', 'automatic', 'assault', 'rifle', 'gun', 'show', 'al', 'Qaeda', 'spokesman', 'extension', 'Reid', 'mix', 'semiautomatic', 'weapon', 'automatic', 'weapon', 'Semiautomatic', 'define', '1968', 'Gun', 'Control', 'Act', 'mean', 'one', 'pull', 'trigger', 'equates', 'one', 'bullet', 'leave', 'barrel', 'Fully', 'automatic', 'rifle', 'available', 'United', 'States', 'require', 'six', 'month', 'paperwork', 'Bureau', 'Alcohol', 'Tobacco', 'Firearms', 'Explosives', 'also', 'significantly', 'expensive', 'single', 'fire', 'counterpart', 'ban', 'many', 'state', 'Moreover', '1986', 'law', 'ban', 'new', 'one', 'automatic', 'weapon', 'purchase', 'would', 'old', 'one', 'Still', 'one', 'could', 'purchase', 'semiautomatic', 'weapon', 'retrofit', 'something', 'call', 'auto', 'sear', 'mimic', 'automatic', 'weapon', 'though', 'weapon', 'still', 'fit', 'definition', 'semiautomatic', 'modification', 'require', 'permission', 'ATF', 'video', 'describe', 'one', 'product', 'call', 'gun', 'show', 'loophole', 'refers', 'private', 'sale', 'within', 'state', 'line', 'license', 'gun', 'dealer', 'gun', 'show', 'must', 'run', 'background', 'check', 'Anyone', 'gun', 'show', 'sell', 'someone', 'state', 'must', 'run', 'background', 'check', 'number', 'state', 'include', 'California', 'New', 'York', 'require', 'background', 'check', 'gun', 'transaction', 'state', 'require', 'handgun', 'purchase', 'gun', 'show', 'require', 'background', 'check', 'matter', 'policy', 'matter', 'state', 'law', 'say', 'Gadahn', 'extensive', 'Reid', 'speak', 'sloppily', 'Reid', 'spokesman', 'decline', 'provide', 'record', 'comment', 'Pinocchio', 'Test', 'may', 'make', 'sense', 'award', 'Pinocchios', 'dead', 'al', 'Qaeda', 'operative', 'case', 'Reid', 'clearly', 'state', 'Gadahn', 'correct', 'even', 'believe', 'quote', 'make', 'noteworthy', 'point', 'Reid', 'quote', 'fully', 'automatic', 'line', 'twice', 'without', 'inform', 'listener', 'actually', 'possible', 'Moreover', 'Reid', 'make', 'comment', 'prepared', 'statement', 'publicly', 'release', 'news', 'release', 'could', 'still', 'update', 'correction', 'Two', 'Pinocchios', 'rating', 'scale', 'Send', 'u', 'fact', 'check', 'fill', 'form', 'Check', '2016', 'candidate', 'fact', 'check', 'page', 'Sign', 'Fact', 'Checker', 'weekly', 'newsletter', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'id', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'd6b018de', '31f6', '46f9', '8dd6', 'fd20a63ac97a', 'userType', 'ANONYMOUS', 'gameId', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'reid', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466117973960', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'false', 'photoUploaded', 'false', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '1a6d57ae', 'd8a8', '43e7', '8b1e', 'f60ce8ccafa1', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466117973962', 'lastUpdated', '1466117973962', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466117973960', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter']\n"
     ]
    }
   ],
   "source": [
    "# Before cleaning \n",
    "\n",
    "print(len(lemma_stop(data[get_index[212853]][4])))\n",
    "print(lemma_stop(data[get_index[212853]][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1116\n",
      "['know', 'States', 'impose', 'reasonable', 'limitation', 'less', 'gun', 'crime', 'less', 'homicide', 'Sen', 'Chris', 'Murphy', 'Conn', 'speak', 'Senate', 'floor', 'June', '15', 'Readers', 'ask', 'fact', 'check', 'gun', 'rhetoric', 'use', 'Democrats', 'wake', 'mass', 'shoot', 'Orlando', 'one', 'case', 'already', 'delve', 'material', 'claim', 'new', 'let', 'take', 'look', 'start', 'Murphy', 'statement', 'Facts', 'Murphy', 'staff', 'say', 'refer', 'chart', 'appear', 'National', 'Journal', '2015', 'turn', 'carefully', 'checked', 'chart', 'President', 'Obama', 'make', 'similar', 'carefully', 'phrase', 'claim', 'gun', 'death', 'Note', 'Murphy', 'refer', 'homicide', 'gun', 'crime', 'President', 'Obama', 'earn', 'Two', 'Pinocchios', 'Readers', 'check', 'full', 'fact', 'check', 'summary', 'note', 'gun', 'death', '60', 'percent', '2013', 'actually', 'suicide', 'data', 'use', 'National', 'Journal', 'chart', 'calculates', 'number', 'gun', 'related', 'death', 'per', '100', '000', 'people', 'include', 'gun', 'death', 'include', 'homicide', 'suicide', 'accidental', 'gun', 'death', 'legal', 'intervention', 'involve', 'firearm', 'remove', 'suicide', 'total', 'reran', 'number', 'case', 'make', 'huge', 'difference', 'Half', '10', 'state', 'low', 'gun', 'death', 'rate', 'turn', 'state', 'less', 'restrictive', 'gun', 'law', 'Moreover', 'counting', 'gun', 'law', 'certainly', 'open', 'interpretation', 'also', 'affect', 'outcome', 'enough', 'count', 'law', 'figure', 'reason', 'gun', 'death', 'low', 'one', 'state', 'another', 'One', 'would', 'need', 'specifically', 'determine', 'whether', 'certain', 'law', 'effect', 'time', 'gun', 'death', 'rate', 'state', 'instance', 'Murphy', 'claim', 'worthy', 'Three', 'Pinocchios', 'specifically', 'refer', 'homicide', 'rather', 'gun', 'death', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'checker', 'rating', 'murphy', '1', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'AR', '15', 'style', 'weapon', 'legal', 'United', 'States', '2004', 'ban', '10', 'year', 'coincidental', 'massive', 'increase', 'mass', 'shooting', 'country', '2004', 'Murphy', 'June', '15', 'another', 'problematic', 'claim', 'Murphy', 'staff', 'could', 'point', 'specific', 'data', 'back', 'significant', 'contrary', 'data', 'show', '10', 'year', 'assault', 'weapon', 'ban', 'little', 'effect', '2004', 'study', 'Justice', 'Department', 'found', 'ban', 'impact', 'gun', 'violence', 'mixed', 'best', 'ban', 'renew', 'likely', 'effect', 'gun', 'violence', 'likely', 'small', 'best', 'perhaps', 'small', 'reliable', 'measurement', 'report', 'say', 'assault', 'weapon', 'rarely', 'use', 'gun', 'crime', 'James', 'Alan', 'Fox', 'Northeastern', 'University', 'professor', 'collect', 'data', 'back', '1982', 'show', 'assault', 'weapon', 'account', '24', '6', 'percent', 'public', 'mass', 'shooting', 'Assault', 'weapon', 'commonplace', 'mass', 'shooting', 'gun', 'control', 'advocate', 'believe', 'Fox', 'write', '2012', 'article', 'journal', 'Homicide', 'Studies', 'Instead', 'semiautomatic', 'handgun', '47', '9', 'percent', 'far', 'prevalent', 'random', 'massacre', 'firearm', 'would', 'typically', 'classify', 'assault', 'weapon', 'assault', 'weapon', 'ban', 'make', 'difference', 'mass', 'shooting', 'significantly', 'accord', 'Fox', 'data', '1976', '1994', '18', 'mass', 'shooting', 'per', 'year', 'ban', '1995', '2004', '19', 'incident', 'per', 'year', 'ban', '2011', 'average', 'go', 'nearly', '21', '2016', 'study', 'publish', 'Applied', 'Economics', 'Benjamin', 'Blau', 'Utah', 'State', 'University', 'colleague', 'also', 'look', 'whether', 'state', 'federal', 'law', 'assault', 'rifle', 'affected', 'whether', 'weapon', 'use', 'public', 'shooting', '1982', '2014', 'study', 'seem', 'indicate', 'Federal', 'assault', 'rifle', 'ban', 'individual', 'State', 'assault', 'rifle', 'ban', 'affect', 'likelihood', 'assault', 'rifle', 'use', 'mass', 'shoot', 'Blau', 'say', 'Said', 'differently', 'type', 'ban', 'appear', 'deter', 'use', 'assault', 'rifle', 'mass', 'shoot', 'data', 'use', 'determine', 'whether', 'assault', 'rifle', 'ban', 'negatively', 'influence', 'likelihood', 'occurrence', 'mass', 'shoot', 'conclusive', 'colleague', 'Christopher', 'Ingraham', 'point', 'assault', 'style', 'rifle', 'use', 'seven', 'eight', 'high', 'profile', 'public', 'mass', 'shooting', 'since', 'July', 'last', 'year', 'certainly', 'raise', 'profile', 'weapon', 'data', 'far', 'show', 'link', 'use', 'weapon', 'lift', 'ban', 'Murphy', 'asserts', 'bottom', 'line', 'statistic', 'cite', 'story', 'told', 'Wednesday', 'show', 'undeniable', 'gun', 'kill', 'thousand', 'Americans', 'every', 'year', 'say', 'Murphy', 'spokesman', 'Chris', 'Harris', 'step', 'back', 'look', 'totality', 'data', 'easy', 'access', 'firearm', 'way', 'regulation', 'lead', 'increase', 'gun', 'death', 'homicide', 'suicide', 'true', 'U', 'compare', 'state', 'state', 'compare', 'U', 'nation', 'Pinocchio', 'Test', 'Murphy', 'say', 'coincidental', 'mass', 'shooting', 'increase', 'since', 'ban', 'lift', 'data', 'show', 'ban', 'particularly', 'effective', 'first', 'place', 'mass', 'shooting', 'increase', 'significantly', 'since', 'data', 'set', 'relatively', 'small', 'maybe', 'something', 'change', 'past', 'year', 'claim', 'worthy', 'Three', 'Pinocchios', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'checker', 'rating', 'trump', '2', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'America', 'absolutely', 'awash', 'easily', 'obtainable', 'firearm', 'go', 'gun', 'show', 'local', 'convention', 'center', 'come', 'away', 'fully', 'automatic', 'assault', 'rifle', 'without', 'background', 'check', 'likely', 'without', 'show', 'identification', 'card', 'wait', 'Sen', 'Minority', 'Leader', 'Harry', 'Reid', 'Nev', 'quote', 'al', 'Qaeda', 'spokesman', 'statement', 'Senate', 'floor', 'June', '15', '2016', 'indeed', 'al', 'Qaeda', 'spokesman', 'Adam', 'Gadahn', 'use', 'exact', 'language', '2011', 'American', 'born', 'Gadahn', 'later', 'kill', 'drone', 'strike', '2015', 'clip', 'even', 'terrorist', 'get', 'fact', 'wrong', 'Senate', 'leader', 'uncritically', 'quote', 'Reid', 'put', 'terrorist', 'talk', 'gun', 'show', 'loophole', 'specifically', 'point', 'flaw', 'nation', 'gun', 'law', 'allows', 'convict', 'terrorist', 'slip', 'big', 'wide', 'hole', 'slip', 'Actually', 'buy', 'fully', 'automatic', 'assault', 'rifle', 'gun', 'show', 'al', 'Qaeda', 'spokesman', 'extension', 'Reid', 'mix', 'semiautomatic', 'weapon', 'automatic', 'weapon', 'Semiautomatic', 'define', '1968', 'Gun', 'Control', 'Act', 'mean', 'one', 'pull', 'trigger', 'equates', 'one', 'bullet', 'leave', 'barrel', 'Fully', 'automatic', 'rifle', 'available', 'United', 'States', 'require', 'six', 'month', 'paperwork', 'Bureau', 'Alcohol', 'Tobacco', 'Firearms', 'Explosives', 'also', 'significantly', 'expensive', 'single', 'fire', 'counterpart', 'ban', 'many', 'state', 'Moreover', '1986', 'law', 'ban', 'new', 'one', 'automatic', 'weapon', 'purchase', 'would', 'old', 'one', 'Still', 'one', 'could', 'purchase', 'semiautomatic', 'weapon', 'retrofit', 'something', 'call', 'auto', 'sear', 'mimic', 'automatic', 'weapon', 'though', 'weapon', 'still', 'fit', 'definition', 'semiautomatic', 'modification', 'require', 'permission', 'ATF', 'video', 'describe', 'one', 'product', 'call', 'gun', 'show', 'loophole', 'refers', 'private', 'sale', 'within', 'state', 'line', 'license', 'gun', 'dealer', 'gun', 'show', 'must', 'run', 'background', 'check', 'Anyone', 'gun', 'show', 'sell', 'someone', 'state', 'must', 'run', 'background', 'check', 'number', 'state', 'include', 'California', 'New', 'York', 'require', 'background', 'check', 'gun', 'transaction', 'state', 'require', 'handgun', 'purchase', 'gun', 'show', 'require', 'background', 'check', 'matter', 'policy', 'matter', 'state', 'law', 'say', 'Gadahn', 'extensive', 'Reid', 'speak', 'sloppily', 'Reid', 'spokesman', 'decline', 'provide', 'record', 'comment', 'Pinocchio', 'Test', 'may', 'make', 'sense', 'award', 'Pinocchios', 'dead', 'al', 'Qaeda', 'operative', 'case', 'Reid', 'clearly', 'state', 'Gadahn', 'correct', 'even', 'believe', 'quote', 'make', 'noteworthy', 'point', 'Reid', 'quote', 'fully', 'automatic', 'line', 'twice', 'without', 'inform', 'listener', 'actually', 'possible', 'Moreover', 'Reid', 'make', 'comment', 'prepared', 'statement', 'publicly', 'release', 'news', 'release', 'could', 'still', 'update', 'correction', 'Two', 'Pinocchios', 'rating', 'scale', 'Send', 'u', 'fact', 'check', 'fill', 'form', 'Check', '2016', 'candidate', 'fact', 'check', 'page', 'Sign', 'Fact', 'Checker', 'weekly', 'newsletter', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'shareURL', 'encodeURIComponent', 'checker', 'rating', 'reid', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter']\n"
     ]
    }
   ],
   "source": [
    "# After cleaning (removing JSON, HTML, etc)\n",
    "\n",
    "cleaned_doc=clean_document(data[get_index[212853]][4])\n",
    "print(len(lemma_stop(cleaned_doc)))\n",
    "print(lemma_stop(cleaned_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsbw8tNbrJfL"
   },
   "source": [
    "*Run the following cell once after all pre-processing (removing JSON etc), and store final lemmatized contents of all docs:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibq9gAauMI5Q"
   },
   "outputs": [],
   "source": [
    "# Run once after all pre-processing \n",
    "\n",
    "\n",
    "# # Tester code\n",
    "# import time\n",
    "\n",
    "# s = time.time()\n",
    "\n",
    "# for i in range(0, len(data)) :\n",
    "#     f_content = clean_document(data[i][4])\n",
    "#     contents = f_content\n",
    "#     # print(contents)\n",
    "#     final = lemma_stop (contents)\n",
    "# #     print(type(final))\n",
    "#     # print (final)\n",
    "    \n",
    "# print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary smaller dataset\n",
    "\n",
    "subset = []\n",
    "counter = 0\n",
    "for document in data:\n",
    "    subset.append(document)\n",
    "    counter += 1\n",
    "    if counter == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:39<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.14584922790527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "for document in tqdm(subset):\n",
    "    modifiedContent = replace_dates(document[4])\n",
    "    modifiedContent = lemma_stop(modifiedContent)\n",
    "    modifiedTitle = lemma_stop((document[1]))\n",
    "    # modifiedContent = lemma_stop(clean_document(document[4]))\n",
    "    # modifiedTitle = lemma_stop(clean_document(document[1]))\n",
    "    titles.append(modifiedTitle)\n",
    "    contents.append(modifiedContent)\n",
    "    \n",
    "print(time.time() - start)  # 110.26236414909363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "contents_temp = contents\n",
    "titles_temp = titles\n",
    "for i in range(1000):\n",
    "    for j in range(len(contents[i])):\n",
    "        contents[i][j] = unidecode.unidecode(contents[i][j])\n",
    "    for j in range(len(titles[i])):\n",
    "        titles[i][j] = unidecode.unidecode(titles[i][j])\n",
    "# for document in contents_temp:\n",
    "#     for word in document:\n",
    "#         word = unidecode.unidecode(word)\n",
    "# for title in titles_temp:\n",
    "#     for word in title:\n",
    "#         word = unidecode.unidecode(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [],
   "source": [
    "import trie\n",
    "\n",
    "# Create map from docID of the document to an object of class Node \n",
    "# (i.e, the corresponding document trie structure)\n",
    "# ex. if the docID of the document is 1, \n",
    "# getReference[1] gives the object which is the trie structure of docID 1\n",
    "\n",
    "getReference = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentRoot = []\n",
    "collection = trie.CollectionNode()\n",
    "\n",
    "# initializing the root for 1000 documents\n",
    "for i in range(1000):\n",
    "    newDocument = trie.Node()\n",
    "    documentRoot.append(newDocument)\n",
    "    getReference[get_docID[i]] = newDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 29.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.69987678527832\n"
     ]
    }
   ],
   "source": [
    "# creating the documents\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(1000)):\n",
    "    for w in contents_temp[i]:\n",
    "        collection.add_(w, 0, get_docID[i])\n",
    "        documentRoot[i].add(w, 0)\n",
    "\n",
    "print(time.time() - start)  #39.19705152511597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import queue\n",
    "\n",
    "documentLength = {}\n",
    "N = len(documentRoot)\n",
    "\n",
    "for i in tqdm(range(len(documentRoot))):\n",
    "    \n",
    "    docID = get_docID[i]\n",
    "    length = 0\n",
    "    document = documentRoot[i]\n",
    "    q = queue.Queue()\n",
    "    q.put([document, ''])\n",
    "\n",
    "    while q.qsize() > 0:\n",
    "\n",
    "        current = q.get()\n",
    "        reference = current[0]\n",
    "        word = current[1]\n",
    "\n",
    "        if reference.words > 0:\n",
    "            df = len(collection.get_doc_list(word, 0))\n",
    "            idf = math.log10(N/df)\n",
    "            # print(word, reference.words, df)\n",
    "            length += (reference.words * idf) ** 2\n",
    "\n",
    "        for i in range(256):\n",
    "            if reference.children[i] is not None:\n",
    "                new_word = word + chr(i)\n",
    "                q.put([reference.children[i], new_word])\n",
    "\n",
    "    # print(length**0.5)\n",
    "    documentLength[docID] = length**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.getsizeof(documentRoot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9w6bB9rJfg",
    "outputId": "2e5b7bc8-2e68-4de8-f81d-a142a989c902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11Sep']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "query = '9/11'\n",
    "final_query = replace_dates(query)\n",
    "final_query = lemma_stop(final_query)\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "print(final_query)\n",
    "print(len(final_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4Kqgk9hrJfj",
    "outputId": "04bfaa8a-8053-4ab7-e55a-cee2d47edb6f"
   },
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1\n",
    "        \n",
    "    # Test code just to see distribution of query terms in the documents\n",
    "    \n",
    "    # print(w)\n",
    "    # df = len(collection.get_doc_list(w,0))\n",
    "    # print(collection.get_doc_list(w,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(documentRoot[1]), get_docID[1])\n",
    "# document = documentRoot[1]\n",
    "# N = len(documentRoot)\n",
    "\n",
    "# import queue\n",
    "# import math\n",
    "\n",
    "# length = 0\n",
    "# q = queue.Queue()\n",
    "# q.put([document, ''])\n",
    "\n",
    "# while q.qsize() > 0:\n",
    "    \n",
    "#     current = q.get()\n",
    "#     reference = current[0]\n",
    "#     word = current[1]\n",
    "    \n",
    "#     if reference.words > 0:\n",
    "#         df = len(collection.get_doc_list(word, 0))\n",
    "#         idf = math.log10(N/df)\n",
    "#         # print(word, reference.words, df)\n",
    "#         length += (reference.words * idf) ** 2\n",
    "    \n",
    "#     for i in range(256):\n",
    "#         if reference.children[i] is not None:\n",
    "#             new_word = word + chr(i)\n",
    "#             q.put([reference.children[i], new_word])\n",
    "\n",
    "# print(length**0.5)\n",
    "# print(replace_dates(subset[get_index[104]][4]))\n",
    "# replace_dates('12/12')\n",
    "# lemma_stop('12Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfT-kdUqrJfp",
    "outputId": "3c95efd0-76c9-4dfe-e425-e79c089931ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  11Sep\n",
      "\n",
      "df =  8\n",
      "idf =  2.0969100130080562\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  104\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "... Malik had watched the twin towers implode from the top story of school, and wanted to show those behind the 9/11 attacks that “actions have consequences.” After five years in the army — including stretches in Baghdad and Fallujah at the height of the Iraqi Civil War — Malik returned home in 2008, went to St. John’s College on the G.I. Bill, and tried... Continue reading&amphellip  ... \n",
      "tf-idf score= 1.2114498721032723\n",
      "\n",
      "doc ID =  6492\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "... newly available federal money to meet those new standards. The RealID Act was passed in 2005 in response to the 9/11 Commission’s identification requirements — including the requirement that driver’s licenses be stored in digital form. States are still in charge of their own licenses, but if licenses don’t meet the new federal requirements, they’ll stop being valid for use in airports as early as 2018.The act itself doesn’t say  ... \n",
      "tf-idf score= 0.7569711629704844\n",
      "\n",
      "doc ID =  6802\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "... lies in economic downturns that occurred in the early 2000s, as well as in enhanced interest in international security following 9/11. \"When the US federal government runs deficits, biomedical research is de-emphasized,\" Moses says. And although this may seem intuitively correct, \"you would hope to see that trend reversed — investment in science and tech is a very good economic investment.\"\"Asia knows that biomedical research is a vehicle for them  ... \n",
      "tf-idf score= 0.6466434118230429\n",
      "\n",
      "doc ID =  7078\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "... excessive oversight.Another compelling answer may simply be cost. Gary Kelly, CEO of budget American carrier Southwest Airlines, argues that after 9/11 \"we were all low-fare carriers.\" In an opinion piece published this week, Kelly paints a picture of razor-thin profit margins at the start of the last decade that forced airlines to shave every expenditure down to a minimum. This goes some way to explaining why plane operators may be  ... \n",
      "tf-idf score= 0.6193462278582513\n",
      "\n",
      "doc ID =  6432\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "... The Clinton administration started making moves toward a National Missile Defense in the 1990s. But the real push came after 9/11, when George Bush pulled out of the anti-ballistic missile treaty and fast-tracked what became the GMD. This, Grego and her colleagues at the Union of Concerned Scientists argue, is what set the system up for failure by rushing missiles that were essentially still prototypes into silos. Karako counters that  ... \n",
      "tf-idf score= 0.40876802019953623\n",
      "\n",
      "doc ID =  6977\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "... al-Qaeda poker network could extract enough untraceable money from the United States in just a few days to fund several 9/11-sized attacks,\" he says. \"Say no to internet gambling.\"The message is clear: internet gambling will fund terrorism and destroy your family.Players reacted strongly, calling it \"ridiculous hysterics\" and \"distortion at its best.\" \"Get a life, Sheldon!\" one commenter wrote.Sheldon is of course Sheldon Adelson, the 80-year-old billionaire casino owner who  ... \n",
      "tf-idf score= 0.40245227170214826\n",
      "\n",
      "doc ID =  6440\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "... Russia sought to meddle in the presidential election. Morell has called the alleged Russian hacking campaign “the political equivalent of 9/11,” while Bash and other Beacon staffers have warned journalists against writing stories based on the leaks.Still, at least one Beacon associate will likely be in Trump’s cabinet: Trump selected Retired Marine General John Kelly, a Beacon advisor, to lead the Department of Homeland Security — which has a $255,000  ... \n",
      "tf-idf score= 0.3314120453705113\n",
      "\n",
      "doc ID =  151\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "... USDA Livestock &amp Meat Domestic Data\"Here is the truth: The earth is round Saddam Hussein did not attack us on 9/11 Elvis is dead Obama was born in the United States and the climate crisis is real.\" &ampndash Al GoreTo produce 1 pound of beef, the friendly neighborhood farmer will need 13 pounds of grain and an estimated 2,500 gallons of water. If a 1,000-pound cow yields 600 pounds of  ... \n",
      "tf-idf score= 0.30721037343071\n"
     ]
    }
   ],
   "source": [
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    # print('List of documents with this term=', docs_having_query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query_term = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        # print(docID)\n",
    "        tf_doc = getReference[docID].count_words(query_term, 0)\n",
    "        # print('tf for doc',docID,'is',tf_doc)\n",
    "        tfidf_doc_query = tf_doc * idf\n",
    "        \n",
    "        # print('tfidf for doc',doc,'is',tfidf_doc)\n",
    "        # print()\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query_term * tfidf_doc_query)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [tfidf_query_term * tfidf_doc_query, query_term])\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query_term * tfidf_doc_query)\n",
    "            bisect.insort(wordsInDoc[docID], [tfidf_query_term * tfidf_doc_query, query_term])\n",
    "        \n",
    "for doc in scores:\n",
    "    if documentLength[doc] != 0:\n",
    "        scores[doc] = scores[doc]/ math.sqrt(documentLength[doc])\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    # print(i)\n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], end = ' ')\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    for word in subset[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word)\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print()\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    \n",
    "# print('\\n\\n')\n",
    "# print('============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11Sep']\n"
     ]
    }
   ],
   "source": [
    "check_with=replace_dates('9/11')\n",
    "check_with=str(lemma_stop(check_with))\n",
    "print(check_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqsqWNnFNOhO"
   },
   "source": [
    "***Original Text :***\n",
    "\n",
    "      We’re back in Dale Cooper’s position, wandering through a freshly revived world, and trying to catch up with the ways it’s moved on in his absence.\n",
    "\n",
    "***Processed Text :***\n",
    "\n",
    "      We back Dale Cooper position wander freshly revive world try catch way move absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
