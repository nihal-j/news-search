{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__1Nxb4gRIeN"
   },
   "source": [
    "Done so far :\n",
    "\n",
    "\n",
    "*   Lemmatization\n",
    "*   Stop Words Removal\n",
    "\n",
    "Verify :\n",
    "\n",
    "* Normalization - removing accents, etc.\n",
    "* Dates replaced with strings\n",
    "* Case-folding\n",
    "* Removed HTML entity codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "### pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlcodes(document):\n",
    "    \n",
    "    replacement = {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                  }\n",
    "    for str in replacement:\n",
    "        document = document.replace(str, replacement[str])\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PvIwQ6ySVHv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data.npy\n",
    "# data.npy is a 2D array containing the dataset information as\n",
    "# data[i][0] : docID of ith document\n",
    "# data[i][1] : title of ith document\n",
    "# data[i][4] : content of ith document\n",
    "\n",
    "data = np.load('data.npy',allow_pickle = True)\n",
    "# sentence = data[0][4]\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6aQ7ZcPCH4d",
    "outputId": "8fc93dc4-51e3-48ff-fa64-d5bbef5847a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204135\n"
     ]
    }
   ],
   "source": [
    "# creating a map {index_in_data_npy, docID}\n",
    "\n",
    "# ex. if ith element in data has docID j,\n",
    "# get_docID[i] will return j\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = int(data[i][0])\n",
    "    get_index[int(data[i][0])] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def replace_dates(documentString):\n",
    "    \n",
    "    regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    \n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp = date\n",
    "        date = date.replace('.', '/')\n",
    "        date = date.replace('-', '/')\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        newDate = newDate.replace(' ', '')\n",
    "        documentString = documentString.replace(tmp, newDate)\n",
    "        # print(newDate)\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1891\n",
      "['know', 'States', 'impose', 'reasonable', 'limitation', 'less', 'gun', 'crime', 'less', 'homicide', 'Sen', 'Chris', 'Murphy', 'Conn', 'speak', 'Senate', 'floor', 'June', '15', 'Readers', 'ask', 'fact', 'check', 'gun', 'rhetoric', 'use', 'Democrats', 'wake', 'mass', 'shoot', 'Orlando', 'one', 'case', 'already', 'delve', 'material', 'claim', 'new', 'let', 'take', 'look', 'start', 'Murphy', 'statement', 'Facts', 'Murphy', 'staff', 'say', 'refer', 'chart', 'appear', 'National', 'Journal', '2015', 'turn', 'carefully', 'checked', 'chart', 'President', 'Obama', 'make', 'similar', 'carefully', 'phrase', 'claim', 'gun', 'death', 'Note', 'Murphy', 'refer', 'homicide', 'gun', 'crime', 'President', 'Obama', 'earn', 'Two', 'Pinocchios', 'Readers', 'check', 'full', 'fact', 'check', 'summary', 'note', 'gun', 'death', '60', 'percent', '2013', 'actually', 'suicide', 'data', 'use', 'National', 'Journal', 'chart', 'calculates', 'number', 'gun', 'related', 'death', 'per', '100', '000', 'people', 'include', 'gun', 'death', 'include', 'homicide', 'suicide', 'accidental', 'gun', 'death', 'legal', 'intervention', 'involve', 'firearm', 'remove', 'suicide', 'total', 'reran', 'number', 'case', 'make', 'huge', 'difference', 'Half', '10', 'state', 'low', 'gun', 'death', 'rate', 'turn', 'state', 'less', 'restrictive', 'gun', 'law', 'Moreover', 'counting', 'gun', 'law', 'certainly', 'open', 'interpretation', 'also', 'affect', 'outcome', 'enough', 'count', 'law', 'figure', 'reason', 'gun', 'death', 'low', 'one', 'state', 'another', 'One', 'would', 'need', 'specifically', 'determine', 'whether', 'certain', 'law', 'effect', 'time', 'gun', 'death', 'rate', 'state', 'instance', 'Murphy', 'claim', 'worthy', 'Three', 'Pinocchios', 'specifically', 'refer', 'homicide', 'rather', 'gun', 'death', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21pk0d8', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'id', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'c3a12994', '0d15', '4748', '869d', '9dca491526fa', 'userType', 'ANONYMOUS', 'gameId', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'murphy', '1', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466118080690', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'true', 'photoUploaded', 'false', 'shareUrl', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21pk0d8', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '69a73c61', '9b60', '4a7d', '88b8', '73090f0e953c', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466118080693', 'lastUpdated', '1466118122699', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466118122697', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'AR', '15', 'style', 'weapon', 'legal', 'United', 'States', '2004', 'ban', '10', 'year', 'coincidental', 'massive', 'increase', 'mass', 'shooting', 'country', '2004', 'Murphy', 'June', '15', 'another', 'problematic', 'claim', 'Murphy', 'staff', 'could', 'point', 'specific', 'data', 'back', 'significant', 'contrary', 'data', 'show', '10', 'year', 'assault', 'weapon', 'ban', 'little', 'effect', '2004', 'study', 'Justice', 'Department', 'found', 'ban', 'impact', 'gun', 'violence', 'mixed', 'best', 'ban', 'renew', 'likely', 'effect', 'gun', 'violence', 'likely', 'small', 'best', 'perhaps', 'small', 'reliable', 'measurement', 'report', 'say', 'assault', 'weapon', 'rarely', 'use', 'gun', 'crime', 'James', 'Alan', 'Fox', 'Northeastern', 'University', 'professor', 'collect', 'data', 'back', '1982', 'show', 'assault', 'weapon', 'account', '24', '6', 'percent', 'public', 'mass', 'shooting', 'Assault', 'weapon', 'commonplace', 'mass', 'shooting', 'gun', 'control', 'advocate', 'believe', 'Fox', 'write', '2012', 'article', 'journal', 'Homicide', 'Studies', 'Instead', 'semiautomatic', 'handgun', '47', '9', 'percent', 'far', 'prevalent', 'random', 'massacre', 'firearm', 'would', 'typically', 'classify', 'assault', 'weapon', 'assault', 'weapon', 'ban', 'make', 'difference', 'mass', 'shooting', 'significantly', 'accord', 'Fox', 'data', '1976', '1994', '18', 'mass', 'shooting', 'per', 'year', 'ban', '1995', '2004', '19', 'incident', 'per', 'year', 'ban', '2011', 'average', 'go', 'nearly', '21', '2016', 'study', 'publish', 'Applied', 'Economics', 'Benjamin', 'Blau', 'Utah', 'State', 'University', 'colleague', 'also', 'look', 'whether', 'state', 'federal', 'law', 'assault', 'rifle', 'affected', 'whether', 'weapon', 'use', 'public', 'shooting', '1982', '2014', 'study', 'seem', 'indicate', 'Federal', 'assault', 'rifle', 'ban', 'individual', 'State', 'assault', 'rifle', 'ban', 'affect', 'likelihood', 'assault', 'rifle', 'use', 'mass', 'shoot', 'Blau', 'say', 'Said', 'differently', 'type', 'ban', 'appear', 'deter', 'use', 'assault', 'rifle', 'mass', 'shoot', 'data', 'use', 'determine', 'whether', 'assault', 'rifle', 'ban', 'negatively', 'influence', 'likelihood', 'occurrence', 'mass', 'shoot', 'conclusive', 'colleague', 'Christopher', 'Ingraham', 'point', 'assault', 'style', 'rifle', 'use', 'seven', 'eight', 'high', 'profile', 'public', 'mass', 'shooting', 'since', 'July', 'last', 'year', 'certainly', 'raise', 'profile', 'weapon', 'data', 'far', 'show', 'link', 'use', 'weapon', 'lift', 'ban', 'Murphy', 'asserts', 'bottom', 'line', 'statistic', 'cite', 'story', 'told', 'Wednesday', 'show', 'undeniable', 'gun', 'kill', 'thousand', 'Americans', 'every', 'year', 'say', 'Murphy', 'spokesman', 'Chris', 'Harris', 'step', 'back', 'look', 'totality', 'data', 'easy', 'access', 'firearm', 'way', 'regulation', 'lead', 'increase', 'gun', 'death', 'homicide', 'suicide', 'true', 'U', 'compare', 'state', 'state', 'compare', 'U', 'nation', 'Pinocchio', 'Test', 'Murphy', 'say', 'coincidental', 'mass', 'shooting', 'increase', 'since', 'ban', 'lift', 'data', 'show', 'ban', 'particularly', 'effective', 'first', 'place', 'mass', 'shooting', 'increase', 'significantly', 'since', 'data', 'set', 'relatively', 'small', 'maybe', 'something', 'change', 'past', 'year', 'claim', 'worthy', 'Three', 'Pinocchios', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21piYOd', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'id', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'a0847542', 'f10f', '4409', 'a330', '85f46a3b3c8b', 'userType', 'ANONYMOUS', 'gameId', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'trump', '2', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466118160561', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'true', 'photoUploaded', 'false', 'shareUrl', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21piYOd', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '43f3b445', 'ee45', '45b7', 'bfba', 'cf6132ccda74', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466118160563', 'lastUpdated', '1466126246609', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466126246607', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'America', 'absolutely', 'awash', 'easily', 'obtainable', 'firearm', 'go', 'gun', 'show', 'local', 'convention', 'center', 'come', 'away', 'fully', 'automatic', 'assault', 'rifle', 'without', 'background', 'check', 'likely', 'without', 'show', 'identification', 'card', 'wait', 'Sen', 'Minority', 'Leader', 'Harry', 'Reid', 'Nev', 'quote', 'al', 'Qaeda', 'spokesman', 'statement', 'Senate', 'floor', 'June', '15', '2016', 'indeed', 'al', 'Qaeda', 'spokesman', 'Adam', 'Gadahn', 'use', 'exact', 'language', '2011', 'American', 'born', 'Gadahn', 'later', 'kill', 'drone', 'strike', '2015', 'clip', 'even', 'terrorist', 'get', 'fact', 'wrong', 'Senate', 'leader', 'uncritically', 'quote', 'Reid', 'put', 'terrorist', 'talk', 'gun', 'show', 'loophole', 'specifically', 'point', 'flaw', 'nation', 'gun', 'law', 'allows', 'convict', 'terrorist', 'slip', 'big', 'wide', 'hole', 'slip', 'Actually', 'buy', 'fully', 'automatic', 'assault', 'rifle', 'gun', 'show', 'al', 'Qaeda', 'spokesman', 'extension', 'Reid', 'mix', 'semiautomatic', 'weapon', 'automatic', 'weapon', 'Semiautomatic', 'define', '1968', 'Gun', 'Control', 'Act', 'mean', 'one', 'pull', 'trigger', 'equates', 'one', 'bullet', 'leave', 'barrel', 'Fully', 'automatic', 'rifle', 'available', 'United', 'States', 'require', 'six', 'month', 'paperwork', 'Bureau', 'Alcohol', 'Tobacco', 'Firearms', 'Explosives', 'also', 'significantly', 'expensive', 'single', 'fire', 'counterpart', 'ban', 'many', 'state', 'Moreover', '1986', 'law', 'ban', 'new', 'one', 'automatic', 'weapon', 'purchase', 'would', 'old', 'one', 'Still', 'one', 'could', 'purchase', 'semiautomatic', 'weapon', 'retrofit', 'something', 'call', 'auto', 'sear', 'mimic', 'automatic', 'weapon', 'though', 'weapon', 'still', 'fit', 'definition', 'semiautomatic', 'modification', 'require', 'permission', 'ATF', 'video', 'describe', 'one', 'product', 'call', 'gun', 'show', 'loophole', 'refers', 'private', 'sale', 'within', 'state', 'line', 'license', 'gun', 'dealer', 'gun', 'show', 'must', 'run', 'background', 'check', 'Anyone', 'gun', 'show', 'sell', 'someone', 'state', 'must', 'run', 'background', 'check', 'number', 'state', 'include', 'California', 'New', 'York', 'require', 'background', 'check', 'gun', 'transaction', 'state', 'require', 'handgun', 'purchase', 'gun', 'show', 'require', 'background', 'check', 'matter', 'policy', 'matter', 'state', 'law', 'say', 'Gadahn', 'extensive', 'Reid', 'speak', 'sloppily', 'Reid', 'spokesman', 'decline', 'provide', 'record', 'comment', 'Pinocchio', 'Test', 'may', 'make', 'sense', 'award', 'Pinocchios', 'dead', 'al', 'Qaeda', 'operative', 'case', 'Reid', 'clearly', 'state', 'Gadahn', 'correct', 'even', 'believe', 'quote', 'make', 'noteworthy', 'point', 'Reid', 'quote', 'fully', 'automatic', 'line', 'twice', 'without', 'inform', 'listener', 'actually', 'possible', 'Moreover', 'Reid', 'make', 'comment', 'prepared', 'statement', 'publicly', 'release', 'news', 'release', 'could', 'still', 'update', 'correction', 'Two', 'Pinocchios', 'rating', 'scale', 'Send', 'u', 'fact', 'check', 'fill', 'form', 'Check', '2016', 'candidate', 'fact', 'check', 'page', 'Sign', 'Fact', 'Checker', 'weekly', 'newsletter', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'id', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'd6b018de', '31f6', '46f9', '8dd6', 'fd20a63ac97a', 'userType', 'ANONYMOUS', 'gameId', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'reid', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466117973960', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'false', 'photoUploaded', 'false', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '1a6d57ae', 'd8a8', '43e7', '8b1e', 'f60ce8ccafa1', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466117973962', 'lastUpdated', '1466117973962', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466117973960', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter']\n"
     ]
    }
   ],
   "source": [
    "# Before cleaning \n",
    "\n",
    "print(len(lemma_stop(data[get_index[212853]][4])))\n",
    "print(lemma_stop(data[get_index[212853]][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1116\n",
      "['know', 'States', 'impose', 'reasonable', 'limitation', 'less', 'gun', 'crime', 'less', 'homicide', 'Sen', 'Chris', 'Murphy', 'Conn', 'speak', 'Senate', 'floor', 'June', '15', 'Readers', 'ask', 'fact', 'check', 'gun', 'rhetoric', 'use', 'Democrats', 'wake', 'mass', 'shoot', 'Orlando', 'one', 'case', 'already', 'delve', 'material', 'claim', 'new', 'let', 'take', 'look', 'start', 'Murphy', 'statement', 'Facts', 'Murphy', 'staff', 'say', 'refer', 'chart', 'appear', 'National', 'Journal', '2015', 'turn', 'carefully', 'checked', 'chart', 'President', 'Obama', 'make', 'similar', 'carefully', 'phrase', 'claim', 'gun', 'death', 'Note', 'Murphy', 'refer', 'homicide', 'gun', 'crime', 'President', 'Obama', 'earn', 'Two', 'Pinocchios', 'Readers', 'check', 'full', 'fact', 'check', 'summary', 'note', 'gun', 'death', '60', 'percent', '2013', 'actually', 'suicide', 'data', 'use', 'National', 'Journal', 'chart', 'calculates', 'number', 'gun', 'related', 'death', 'per', '100', '000', 'people', 'include', 'gun', 'death', 'include', 'homicide', 'suicide', 'accidental', 'gun', 'death', 'legal', 'intervention', 'involve', 'firearm', 'remove', 'suicide', 'total', 'reran', 'number', 'case', 'make', 'huge', 'difference', 'Half', '10', 'state', 'low', 'gun', 'death', 'rate', 'turn', 'state', 'less', 'restrictive', 'gun', 'law', 'Moreover', 'counting', 'gun', 'law', 'certainly', 'open', 'interpretation', 'also', 'affect', 'outcome', 'enough', 'count', 'law', 'figure', 'reason', 'gun', 'death', 'low', 'one', 'state', 'another', 'One', 'would', 'need', 'specifically', 'determine', 'whether', 'certain', 'law', 'effect', 'time', 'gun', 'death', 'rate', 'state', 'instance', 'Murphy', 'claim', 'worthy', 'Three', 'Pinocchios', 'specifically', 'refer', 'homicide', 'rather', 'gun', 'death', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'checker', 'rating', 'murphy', '1', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'AR', '15', 'style', 'weapon', 'legal', 'United', 'States', '2004', 'ban', '10', 'year', 'coincidental', 'massive', 'increase', 'mass', 'shooting', 'country', '2004', 'Murphy', 'June', '15', 'another', 'problematic', 'claim', 'Murphy', 'staff', 'could', 'point', 'specific', 'data', 'back', 'significant', 'contrary', 'data', 'show', '10', 'year', 'assault', 'weapon', 'ban', 'little', 'effect', '2004', 'study', 'Justice', 'Department', 'found', 'ban', 'impact', 'gun', 'violence', 'mixed', 'best', 'ban', 'renew', 'likely', 'effect', 'gun', 'violence', 'likely', 'small', 'best', 'perhaps', 'small', 'reliable', 'measurement', 'report', 'say', 'assault', 'weapon', 'rarely', 'use', 'gun', 'crime', 'James', 'Alan', 'Fox', 'Northeastern', 'University', 'professor', 'collect', 'data', 'back', '1982', 'show', 'assault', 'weapon', 'account', '24', '6', 'percent', 'public', 'mass', 'shooting', 'Assault', 'weapon', 'commonplace', 'mass', 'shooting', 'gun', 'control', 'advocate', 'believe', 'Fox', 'write', '2012', 'article', 'journal', 'Homicide', 'Studies', 'Instead', 'semiautomatic', 'handgun', '47', '9', 'percent', 'far', 'prevalent', 'random', 'massacre', 'firearm', 'would', 'typically', 'classify', 'assault', 'weapon', 'assault', 'weapon', 'ban', 'make', 'difference', 'mass', 'shooting', 'significantly', 'accord', 'Fox', 'data', '1976', '1994', '18', 'mass', 'shooting', 'per', 'year', 'ban', '1995', '2004', '19', 'incident', 'per', 'year', 'ban', '2011', 'average', 'go', 'nearly', '21', '2016', 'study', 'publish', 'Applied', 'Economics', 'Benjamin', 'Blau', 'Utah', 'State', 'University', 'colleague', 'also', 'look', 'whether', 'state', 'federal', 'law', 'assault', 'rifle', 'affected', 'whether', 'weapon', 'use', 'public', 'shooting', '1982', '2014', 'study', 'seem', 'indicate', 'Federal', 'assault', 'rifle', 'ban', 'individual', 'State', 'assault', 'rifle', 'ban', 'affect', 'likelihood', 'assault', 'rifle', 'use', 'mass', 'shoot', 'Blau', 'say', 'Said', 'differently', 'type', 'ban', 'appear', 'deter', 'use', 'assault', 'rifle', 'mass', 'shoot', 'data', 'use', 'determine', 'whether', 'assault', 'rifle', 'ban', 'negatively', 'influence', 'likelihood', 'occurrence', 'mass', 'shoot', 'conclusive', 'colleague', 'Christopher', 'Ingraham', 'point', 'assault', 'style', 'rifle', 'use', 'seven', 'eight', 'high', 'profile', 'public', 'mass', 'shooting', 'since', 'July', 'last', 'year', 'certainly', 'raise', 'profile', 'weapon', 'data', 'far', 'show', 'link', 'use', 'weapon', 'lift', 'ban', 'Murphy', 'asserts', 'bottom', 'line', 'statistic', 'cite', 'story', 'told', 'Wednesday', 'show', 'undeniable', 'gun', 'kill', 'thousand', 'Americans', 'every', 'year', 'say', 'Murphy', 'spokesman', 'Chris', 'Harris', 'step', 'back', 'look', 'totality', 'data', 'easy', 'access', 'firearm', 'way', 'regulation', 'lead', 'increase', 'gun', 'death', 'homicide', 'suicide', 'true', 'U', 'compare', 'state', 'state', 'compare', 'U', 'nation', 'Pinocchio', 'Test', 'Murphy', 'say', 'coincidental', 'mass', 'shooting', 'increase', 'since', 'ban', 'lift', 'data', 'show', 'ban', 'particularly', 'effective', 'first', 'place', 'mass', 'shooting', 'increase', 'significantly', 'since', 'data', 'set', 'relatively', 'small', 'maybe', 'something', 'change', 'past', 'year', 'claim', 'worthy', 'Three', 'Pinocchios', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'checker', 'rating', 'trump', '2', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'America', 'absolutely', 'awash', 'easily', 'obtainable', 'firearm', 'go', 'gun', 'show', 'local', 'convention', 'center', 'come', 'away', 'fully', 'automatic', 'assault', 'rifle', 'without', 'background', 'check', 'likely', 'without', 'show', 'identification', 'card', 'wait', 'Sen', 'Minority', 'Leader', 'Harry', 'Reid', 'Nev', 'quote', 'al', 'Qaeda', 'spokesman', 'statement', 'Senate', 'floor', 'June', '15', '2016', 'indeed', 'al', 'Qaeda', 'spokesman', 'Adam', 'Gadahn', 'use', 'exact', 'language', '2011', 'American', 'born', 'Gadahn', 'later', 'kill', 'drone', 'strike', '2015', 'clip', 'even', 'terrorist', 'get', 'fact', 'wrong', 'Senate', 'leader', 'uncritically', 'quote', 'Reid', 'put', 'terrorist', 'talk', 'gun', 'show', 'loophole', 'specifically', 'point', 'flaw', 'nation', 'gun', 'law', 'allows', 'convict', 'terrorist', 'slip', 'big', 'wide', 'hole', 'slip', 'Actually', 'buy', 'fully', 'automatic', 'assault', 'rifle', 'gun', 'show', 'al', 'Qaeda', 'spokesman', 'extension', 'Reid', 'mix', 'semiautomatic', 'weapon', 'automatic', 'weapon', 'Semiautomatic', 'define', '1968', 'Gun', 'Control', 'Act', 'mean', 'one', 'pull', 'trigger', 'equates', 'one', 'bullet', 'leave', 'barrel', 'Fully', 'automatic', 'rifle', 'available', 'United', 'States', 'require', 'six', 'month', 'paperwork', 'Bureau', 'Alcohol', 'Tobacco', 'Firearms', 'Explosives', 'also', 'significantly', 'expensive', 'single', 'fire', 'counterpart', 'ban', 'many', 'state', 'Moreover', '1986', 'law', 'ban', 'new', 'one', 'automatic', 'weapon', 'purchase', 'would', 'old', 'one', 'Still', 'one', 'could', 'purchase', 'semiautomatic', 'weapon', 'retrofit', 'something', 'call', 'auto', 'sear', 'mimic', 'automatic', 'weapon', 'though', 'weapon', 'still', 'fit', 'definition', 'semiautomatic', 'modification', 'require', 'permission', 'ATF', 'video', 'describe', 'one', 'product', 'call', 'gun', 'show', 'loophole', 'refers', 'private', 'sale', 'within', 'state', 'line', 'license', 'gun', 'dealer', 'gun', 'show', 'must', 'run', 'background', 'check', 'Anyone', 'gun', 'show', 'sell', 'someone', 'state', 'must', 'run', 'background', 'check', 'number', 'state', 'include', 'California', 'New', 'York', 'require', 'background', 'check', 'gun', 'transaction', 'state', 'require', 'handgun', 'purchase', 'gun', 'show', 'require', 'background', 'check', 'matter', 'policy', 'matter', 'state', 'law', 'say', 'Gadahn', 'extensive', 'Reid', 'speak', 'sloppily', 'Reid', 'spokesman', 'decline', 'provide', 'record', 'comment', 'Pinocchio', 'Test', 'may', 'make', 'sense', 'award', 'Pinocchios', 'dead', 'al', 'Qaeda', 'operative', 'case', 'Reid', 'clearly', 'state', 'Gadahn', 'correct', 'even', 'believe', 'quote', 'make', 'noteworthy', 'point', 'Reid', 'quote', 'fully', 'automatic', 'line', 'twice', 'without', 'inform', 'listener', 'actually', 'possible', 'Moreover', 'Reid', 'make', 'comment', 'prepared', 'statement', 'publicly', 'release', 'news', 'release', 'could', 'still', 'update', 'correction', 'Two', 'Pinocchios', 'rating', 'scale', 'Send', 'u', 'fact', 'check', 'fill', 'form', 'Check', '2016', 'candidate', 'fact', 'check', 'page', 'Sign', 'Fact', 'Checker', 'weekly', 'newsletter', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'shareURL', 'encodeURIComponent', 'checker', 'rating', 'reid', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter']\n"
     ]
    }
   ],
   "source": [
    "# After cleaning (removing JSON, HTML, etc)\n",
    "\n",
    "cleaned_doc=clean_document(data[get_index[212853]][4])\n",
    "print(len(lemma_stop(cleaned_doc)))\n",
    "print(lemma_stop(cleaned_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsbw8tNbrJfL"
   },
   "source": [
    "*Run the following cell once after all pre-processing (removing JSON etc), and store final lemmatized contents of all docs:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibq9gAauMI5Q"
   },
   "outputs": [],
   "source": [
    "# Run once after all pre-processing \n",
    "\n",
    "\n",
    "# # Tester code\n",
    "# import time\n",
    "\n",
    "# s = time.time()\n",
    "\n",
    "# for i in range(0, len(data)) :\n",
    "#     f_content = clean_document(data[i][4])\n",
    "#     contents = f_content\n",
    "#     # print(contents)\n",
    "#     final = lemma_stop (contents)\n",
    "# #     print(type(final))\n",
    "#     # print (final)\n",
    "    \n",
    "# print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary smaller dataset\n",
    "\n",
    "subset = []\n",
    "counter = 0\n",
    "for document in data:\n",
    "    subset.append(document)\n",
    "    counter += 1\n",
    "    if counter == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:46<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.05958127975464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "for document in tqdm(subset):\n",
    "    # actually modifying the document\n",
    "    document[4] = remove_htmlcodes(document[4])\n",
    "    \n",
    "    # not actually modifying the document\n",
    "    modifiedContent = replace_dates(document[4])\n",
    "    modifiedContent = lemma_stop(clean_document(modifiedContent))\n",
    "    modifiedTitle = lemma_stop(clean_document(document[1]))\n",
    "    \n",
    "    # case-folding\n",
    "    for i in range(len(modifiedContent)):\n",
    "        modifiedContent[i] = modifiedContent[i].lower()\n",
    "    \n",
    "    # modifiedTitle = lemma_stop((document[1]))\n",
    "    titles.append(modifiedTitle)\n",
    "    contents.append(modifiedContent)\n",
    "    \n",
    "print(time.time() - start)  # 110.26236414909363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "contents_temp = contents\n",
    "\n",
    "titles_temp = titles\n",
    "for i in range(1000):\n",
    "    for j in range(len(contents[i])):\n",
    "        contents[i][j] = unidecode.unidecode(contents[i][j])\n",
    "    for j in range(len(titles[i])):\n",
    "        titles[i][j] = unidecode.unidecode(titles[i][j])\n",
    "# for document in contents_temp:\n",
    "#     for word in document:\n",
    "#         word = unidecode.unidecode(word)\n",
    "# for title in titles_temp:\n",
    "#     for word in title:\n",
    "#         word = unidecode.unidecode(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [],
   "source": [
    "import trie\n",
    "\n",
    "# Create map from docID of the document to an object of class Node \n",
    "# (i.e, the corresponding document trie structure)\n",
    "# ex. if the docID of the document is 1, \n",
    "# getReference[1] gives the object which is the trie structure of docID 1\n",
    "\n",
    "getReference = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentRoot = []\n",
    "collection = trie.CollectionNode()\n",
    "\n",
    "# initializing the root for 1000 documents\n",
    "for i in range(1000):\n",
    "    newDocument = trie.Node()\n",
    "    documentRoot.append(newDocument)\n",
    "    getReference[get_docID[i]] = newDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:58<00:00, 16.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.748138189315796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the documents\n",
    "\n",
    "max_tf = {}\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(1000)):\n",
    "    for w in contents_temp[i]:\n",
    "        collection.add_document(w, 0, get_docID[i])\n",
    "        documentRoot[i].add(w, 0)\n",
    "        if get_docID[i] in max_tf:\n",
    "            max_tf[get_docID[i]] = max(documentRoot[i].count_words(w, 0), max_tf[get_docID[i]])\n",
    "        else:\n",
    "            max_tf[get_docID[i]] = documentRoot[i].count_words(w, 0)\n",
    "    for w in titles_temp[i]:\n",
    "        collection.add_title(w, 0, get_docID[i])\n",
    "        \n",
    "print(time.time() - start)  #39.19705152511597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:38<00:00, 25.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import queue\n",
    "\n",
    "documentLength = {}\n",
    "N = len(documentRoot)\n",
    "\n",
    "for i in tqdm(range(len(documentRoot))):\n",
    "    \n",
    "    docID = get_docID[i]\n",
    "    length = 0\n",
    "    document = documentRoot[i]\n",
    "    q = queue.Queue()\n",
    "    q.put([document, ''])\n",
    "\n",
    "    while q.qsize() > 0:\n",
    "\n",
    "        current = q.get()\n",
    "        reference = current[0]\n",
    "        word = current[1]\n",
    "\n",
    "        if reference.words > 0:\n",
    "            df = len(collection.get_doc_list(word, 0))\n",
    "            idf = math.log10(N/df)\n",
    "            # print(word, reference.words, df)\n",
    "            length += (reference.words * idf) ** 2\n",
    "\n",
    "        for i in range(256):\n",
    "            if reference.children[i] is not None:\n",
    "                new_word = word + chr(i)\n",
    "                q.put([reference.children[i], new_word])\n",
    "\n",
    "    # print(length**0.5)\n",
    "    documentLength[docID] = length**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.getsizeof(documentRoot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9w6bB9rJfg",
    "outputId": "2e5b7bc8-2e68-4de8-f81d-a142a989c902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last', 'week', 'tonight']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "query = 'Last Week Tonight'\n",
    "final_query = replace_dates(query)\n",
    "final_query = lemma_stop(final_query)\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "    \n",
    "    # case-folding\n",
    "    final_query[i] = final_query[i].lower()\n",
    "print(final_query)\n",
    "print(len(final_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4Kqgk9hrJfj",
    "outputId": "04bfaa8a-8053-4ab7-e55a-cee2d47edb6f"
   },
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1\n",
    "        \n",
    "    # Test code just to see distribution of query terms in the documents\n",
    "    \n",
    "    # print(w)\n",
    "    # df = len(collection.get_doc_list(w,0))\n",
    "    # print(collection.get_doc_list(w,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(documentRoot[1]), get_docID[1])\n",
    "# document = documentRoot[1]\n",
    "# N = len(documentRoot)\n",
    "\n",
    "# import queue\n",
    "# import math\n",
    "\n",
    "# length = 0\n",
    "# q = queue.Queue()\n",
    "# q.put([document, ''])\n",
    "\n",
    "# while q.qsize() > 0:\n",
    "    \n",
    "#     current = q.get()\n",
    "#     reference = current[0]\n",
    "#     word = current[1]\n",
    "    \n",
    "#     if reference.words > 0:\n",
    "#         df = len(collection.get_doc_list(word, 0))\n",
    "#         idf = math.log10(N/df)\n",
    "#         # print(word, reference.words, df)\n",
    "#         length += (reference.words * idf) ** 2\n",
    "    \n",
    "#     for i in range(256):\n",
    "#         if reference.children[i] is not None:\n",
    "#             new_word = word + chr(i)\n",
    "#             q.put([reference.children[i], new_word])\n",
    "\n",
    "# print(length**0.5)\n",
    "# print(replace_dates(subset[get_index[104]][4]))\n",
    "# replace_dates('12/12')\n",
    "# lemma_stop('12Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfT-kdUqrJfp",
    "outputId": "3c95efd0-76c9-4dfe-e425-e79c089931ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  last\n",
      "\n",
      "df =  535\n",
      "idf =  0.2716462179787716\n",
      "-------------------------------------\n",
      "Term in query =  week\n",
      "\n",
      "df =  409\n",
      "idf =  0.38827669199265824\n",
      "-------------------------------------\n",
      "Term in query =  tonight\n",
      "\n",
      "df =  25\n",
      "idf =  1.6020599913279625\n",
      "{7024: 0.2716462179787716, 7284: 0.2716462179787716, 6589: 0.2716462179787716, 6816: 0.38827669199265824, 6435: 0.38827669199265824, 6885: 0.38827669199265824, 6997: 0.38827669199265824, 6999: 0.38827669199265824, 6905: 0.38827669199265824, 7130: 0.38827669199265824, 7291: 0.38827669199265824}\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7130\n",
      "Keywords:\n",
      "\n",
      "How the father of 'Saturday Night Live' conquered the rest of the week\n",
      "\n",
      "title score =  0.38827669199265824\n",
      "tonight -1.232353839483048 14\n",
      "week -0.23893950276471276 6\n",
      "last -0.14104707471974678 1\n",
      "\n",
      "\n",
      "Created in 1982 to follow The Tonight Show Starring Johnny Carson, NBC's Late Night has had just three hosts in as many decades: David Letterman, Conan O'Brien, and Jimmy Fallon. As the story goes (and it could very well just be a myth), Letterman's writers gave O'Brien a large, plastic pickle when he took over Late  ... \n",
      "\n",
      "tf-idf score= 5.063165118123579\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6636\n",
      "Keywords:\n",
      "\n",
      "After the terror, Paris looks inward and ahead\n",
      "\n",
      "title score =  0\n",
      "tonight -1.087112136972546 5\n",
      "week -0.24960644485242314 4\n",
      "last -0.14552475963148478 1\n",
      "\n",
      "\n",
      "... to finally pay her respects at a corner she used to frequent with her friends. \"I'm not going to dance tonight. Maybe next week, but not tonight.\"\"It's like when you lose a friend,\" she said. \"You have to go to the funeral to complete the mourning. To try to have some closure.\"The bar and restaurant are both shuttered now, and Paris is slowly lurching back to life. But closure seems  ... \n",
      "\n",
      "tf-idf score= 3.3528090939393755\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7024\n",
      "Keywords:\n",
      "\n",
      "Airbnb takes on hotels with last-minute booking\n",
      "\n",
      "title score =  0.2716462179787716\n",
      "tonight -1.0680399942186416 3\n",
      "last -0.1961889352068906 4\n",
      "\n",
      "\n",
      "... of data it gathered this past year: most people who open the app are looking for a place to stay tonight. That meant most people were immediately skipping past the stream of random listings, only to be forced to navigate through a slew of menus before they could book a place. Even then, the host might decline the request and leave the user stranded. Compared to the ease of booking  ... \n",
      "\n",
      "tf-idf score= 3.012267416317447\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7212\n",
      "Keywords:\n",
      "\n",
      "Dropping the needle: the life and death of Turntable.fm\n",
      "\n",
      "title score =  0\n",
      "tonight -0.8900333285155347 2\n",
      "week -0.20492380966279186 1\n",
      "last -0.150914565543762 2\n",
      "\n",
      "\n",
      "Tonight at 10PM EST, the virtual DJs at Turntable.fm will be escorted off the stage for the last time. Two and a half years after founders Billy Chasen and Seth Goldstein captivated music fans with their highly interactive approach to internet radio, they are shutting the service down to conserve  ... \n",
      "\n",
      "tf-idf score= 2.8181404910321066\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  52\n",
      "Keywords:\n",
      "\n",
      "The Dragonslayer\n",
      "\n",
      "title score =  0\n",
      "tonight -0.8431894691199803 2\n",
      "week -0.2094650575223551 3\n",
      "last -0.15726886304034146 6\n",
      "\n",
      "\n",
      "Two years ago, John Oliver called Tom Wheeler a dingo.The host of Last Week Tonight had set his sights on the then-raging net neutrality debate, acerbically calling out broadband providers like Comcast and Verizon for their throttling antics and intense Congressional lobbying. Midway through the segment, Oliver dryly pointed to President Obama’s appointment of former cable and wireless lobbyist Wheeler as the new head  ... \n",
      "\n",
      "tf-idf score= 2.7368260193444165\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  7276\n",
      "Keywords:\n",
      "\n",
      "As Galaxy S4 launch draws near, Apple plays a very different game\n",
      "\n",
      "title score =  0\n",
      "tonight -0.8481494071736272 1\n",
      "week -0.21697815140766197 2\n",
      "last -0.14381270363582024 1\n",
      "\n",
      "\n",
      "... - may be considered by Apple too tangential to the Galaxy S4 to use it as a weapon for disrupting tonight's news flow. As for the next iPhone, it could still be many months off, its specs too fluid right now for Apple PR handlers to whisper definitive information to a WSJ reporter in a back alley somewhere.But the fact remains that, even in the absence of rumors worth leaking,  ... \n",
      "\n",
      "tf-idf score= 2.734602201827505\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  51\n",
      "Keywords:\n",
      "\n",
      "@MichelleObama\n",
      "\n",
      "title score =  0\n",
      "tonight -0.8596419465662238 3\n",
      "week -0.1988734276059957 1\n",
      "last -0.14244862650106316 2\n",
      "\n",
      "\n",
      "... Snake’s triple-platinum trap-meets-EDM single, had swallowed pop culture whole. Jimmy Fallon and Robin Wright were dancing to it on The Tonight Show Jonah Hill and Channing Tatum posed while it blared on the soundtrack of 22 Jump Street across YouTube, teens and kittens alike bobbed their heads along. Spin called it “an undeniable force.” @MichelleObamaAn exclusive look at how the First Lady mastered social mediaBy Kwame Opam • Photography by  ... \n",
      "\n",
      "tf-idf score= 2.7165600345990772\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6765\n",
      "Keywords:\n",
      "\n",
      "Eurovision and Nothingness: backstage with the hopefuls of Europe's biggest TV event\n",
      "\n",
      "title score =  0\n",
      "tonight -0.8153341027294094 1\n",
      "week -0.21840563924587025 7\n",
      "last -0.16492806091568274 12\n",
      "\n",
      "\n",
      "... continual ascent for Anti Social Media. Their producer, Lars \"Chief1\" Pedersen, brought them together in January solely to compete in tonight’s show — just for a chance at those three minutes on the world’s stage. If they win enough votes, they’ll stay on through Saturday night’s final, with a shot at bringing the Eurovision crown back to Denmark, a three-time champion. If they don’t, they’ll hop on a plane and  ... \n",
      "\n",
      "tf-idf score= 2.7113660744774672\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  6627\n",
      "Keywords:\n",
      "\n",
      "With Secret Hitler, Cards Against Humanity's co-working space becomes an idea machine\n",
      "\n",
      "title score =  0\n",
      "tonight -0.8431894691199803 1\n",
      "week -0.20435615368034643 1\n",
      "last -0.15012027835668953 2\n",
      "\n",
      "\n",
      "... get it.' For Secret Hitler, I’ve already played it more than 50 times and I cannot wait to play it tonight.\"  ... \n",
      "\n",
      "tf-idf score= 2.709099789886499\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  58\n",
      "Keywords:\n",
      "\n",
      "Mutant Situations\n",
      "\n",
      "title score =  0\n",
      "tonight -0.8330711954905405 1\n",
      "week -0.20966941367603548 2\n",
      "last -0.1521218820681121 3\n",
      "\n",
      "\n",
      "... might not be perfect, but no one can tell the difference.\"This is the last song I’m gonna put you through tonight,\" he assures the crowd, and goes right into \"Rage,\" Riot Boi’s first and most electrifying single.He closes his eyes and crouches at the edge of the stage during the singsong intro, but when the chorus bursts in — \"RAGE! RUH-RUH-RUH-RUH-RAGE! RAGE!\" — he explodes to his feet, his energy  ... \n",
      "\n",
      "tf-idf score= 2.7027585245768595\n",
      "\n",
      "\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "title_score = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "factor = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    # print('List of documents with this term=', docs_having_query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\n",
    "    for docID in docs_having_query_term_in_title:\n",
    "        if docID in title_score:\n",
    "            title_score[docID] += idf\n",
    "        else:\n",
    "            title_score[docID] = idf\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        # print(docID)\n",
    "        tf_doc = getReference[docID].count_words(query_term, 0)\n",
    "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\n",
    "        # print('tf for doc',docID,'is',tf_doc)\n",
    "        # tfidf_doc_query = tf_doc * idf\n",
    "        # tfidf_doc = 1 + math.log10(tf_doc)\n",
    "        tfidf_doc = (tf_doc)\n",
    "        # tfidf_doc_query = (tf_doc)\n",
    "        \n",
    "        # print('tfidf for doc',doc,'is',tfidf_doc)\n",
    "        # print()\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query * tfidf_doc)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] = idf\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query * tfidf_doc)\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] += idf\n",
    "print(title_score)\n",
    "for docID in scores:\n",
    "    if documentLength[docID] != 0:\n",
    "#         scores[docID] = scores[docID]/ math.sqrt(documentLength[docID])\n",
    "        scores[docID] *= factor[docID]\n",
    "        if docID in title_score:\n",
    "            scores[docID] *= 1 + title_score[docID]\n",
    "\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    # print(i)\n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    print()\n",
    "    print(subset[get_index[sorted_scores[i][0]]][1])\n",
    "    print()\n",
    "    if sorted_scores[i][0] not in title_score:\n",
    "        print('title score = ',0)\n",
    "    else:\n",
    "        print('title score = ',title_score[sorted_scores[i][0]])\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\n",
    "        print(getReference[docID].count_words(wordsInDoc[docID][j][1], 0))\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    for word in subset[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word)\n",
    "        check_with = check_with.lower()\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print('\\n')\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    print('\\n')\n",
    "    print('============================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqsqWNnFNOhO"
   },
   "source": [
    "***Original Text :***\n",
    "\n",
    "      We’re back in Dale Cooper’s position, wandering through a freshly revived world, and trying to catch up with the ways it’s moved on in his absence.\n",
    "\n",
    "***Processed Text :***\n",
    "\n",
    "      We back Dale Cooper position wander freshly revive world try catch way move absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first lady need turnip fall 2014 turn lil jon dj snake triple platinum trap meet edm single swallow pop culture whole jimmy fallon robin wright dance tonight show jonah hill channing tatum pose blare soundtrack 22 jump street across youtube teen kitten alike bobbed head along spin call undeniable force michelleobamaan exclusive look first lady master social mediaby kwame opam photography james bareham lt try typekit load async true catch e gtthe first lady need turnip fall 2014 turn lil jon dj snake triple platinum trap meet edm single swallow pop culture whole jimmy fallon robin wright dance tonight show jonah hill channing tatum pose blare soundtrack 22 jump street across youtube teen kitten bobbed head along spin call undeniable force embed container position relative pad bottom 56 25 height 0 overflow hidden max width 100 embed container iframe embed container object embed container embed position absolute top 0 left 0 width 100 height 100 michelle obama join party october mrs obama held vine q let move initiative call kid family send video question healthy eat cooking gardening course five day answer series family friendly query range favorite fall vegetable sweet potato favorite food memory get pizza reward good grade cute especially memorable youtube star president obama impersonator iman alphacat crosson chime average ask first lady best obama baritone many calorie burn every time turn mrs obama team seize opportunity next day member white house office digital strategy joanna rosholm first lady press secretary suggest riff turn rosholm call kitchen turnip white house mrs obama get joke right away know lil jon tell verge kid sing get immediately like okay could cute within minute see crosson vine first lady fdr map room film response turnip deadpanned camera glorious purple white root vegetable hand bass drop mrs obama close eye bounce beat simple self consciously silly vine rack six million view single day headline effusive michelle obama makes best vine ever turnip write jezebel best vine ever cry us weekly decade social initiative mainstay first lady office lady bird johnson environment former librarian laura bush literacy last seven year mrs obama focus four major initiative reach higher teen pursue high learn let move fight childhood obesity let girls learn educate woman girl around world joining forces aid veteran family mrs obama tenure also coincide rise social medium obama presidency twitter go upstart global newswire facebook count 05jan billion user instagram snapchat platform exist decade ago dominate pop culture click iphone mrs obama reach audience mrs johnson mrs bush could dream people get know directly see kind silly sometimes care social medium bypass middle man mrs obama say people get know directly see kind silly sometimes care feel passion filter another source young people particular like knew long ago time sort communication first lady need digital component say caroline adler first lady communication director increasingly real way connect directly audience except digitally outside expert speak highly first lady effort really master medium say michelle barna stern vp social marketing digital ad firm 360i personable accessible authentic relatable also great sense humor social medium double edge sword one tone deaf tweet poorly time vine thoughtless facebook post spell embarrassment unlikely house social medium task force mrs obama built close knit collection strategist initiative director manager work craft digital portrait first lady cool care touch first lady turn turnip make country swoon still advance serious initiative centerit little know fact first lady office congressional authority independent funding say adler get nothing add mrs obama crack room mid january large team verge white house film first ever 360 degree video interview first lady get glimpse social medium hit factory mrs obama swarm stylist staffer countless camera operator amid chaos warm funny playfully rib staff poke fun verge delegation obamas uniquely able authentic bunch new way bunch new channel interview shake hand first lady introduce writer meeting staff hopefully everyone behave say throw eye around room arrange shot several young staffer take selfies first lady atmosphere lively people crowd around monitor take look almost joke alright take name later quip back obvious affection team room break laughter george w bush president could get beer one accuse laura cool bill clinton personable cool end play sax hillary cool detach technocratic raw power wield blackberry behind sunglass private jet obamas different president reportedly text jay z call private meeting kendrick lamar favorite rapper oval office first lady count beyonce personal friend afraid show mom dances tonight show kyle lierman white house associate director public engagement say obamas uniquely able authentic bunch new way bunch new channel obamas charisma front crowd camera make natural ambassador young generation two young daughter sasha malia give insight resonates importantly get two gen zers living roof say mrs obama think cool know watch vine know giggle president obama guest zach galifianakis two ferns promote affordable care act first time sit president appear online series even sasha malia impressed president obama guest two ferns even sasha malia impressed talk dinner table day say mrs obama draw scene maybe talk putin maybe talk nuclear deal minute say yeah taped thing two ferns malia like conversation stop generation look authenticity look feel real natural mrs obama go know meet young people watch nightly news watch sunday morning show reading newspaper phone shift digital communication boon office limited budget mrs obama team reach million young people almost cost sidestep time intensive bureaucracy entrepreneurial find opportunity amplify surprisingly limited resource observes adler lot like startup harvard grad previously work state department hillary clinton adler poise professional always ready expand mrs obama point quickly becomes theme talk staff first lady set goal natural charisma connect audience team hard work develop strategy come idea vet pitch left right maggie morrow assistant deputy chief staff hugh mcdermott scheduler first lady julia kim former assistant deputy chief staff sriramya gainedi special assistant chief staff joseph mahshie trip coordinator kristina broadie deputy associate director social officeif 2014 year turn 2015 year dab cam newton signature touchdown dance move go viral carolina panthers make super bowl run suddenly everyone dab would first lady dab verge ask seem like surefire viral sensation hillary clinton dabbed ellen perhaps ultimate sign broad cultural acceptance safer would first lady dab verge rosholm first lady press secretary laugh idea promise inquire take pitch mrs obama inner social medium quorum communication team office digital strategy every day group meet discus initiative figure best execute across many platform use white house first lady platform include facebook twitter instagram spotify white house launch snapchat visit frequently conversation pull white house social secretary well first lady office presidential correspondence office public engagement mrs obama sits center listen pitch give feedback ultimately make executive decision misconception social medium simple technically anyone get twitter get facebook say 360i barna stern tremendous amount strategy thought investment really go clear mrs obama understand utilize platform social medium best practice order reach goal jason goldman white house chief digital officer recruit stint google twitter medium say team decision happen quickly completely surprised much faster place feel work startup say goldman laid back demeanor becomes animate discuss social medium advantage platform traditional medium measurable get feedback people like work first lady one ask u figure new platform use break new ground conversation mrs obama team become clear care view click engagement much medium organization term staff us land piece medium land people engage participate message send look right moment land might land week later might land two year later say kori schulman deputy director digital strategy goldman actually amaze position first lady many time one ask u figure new platform use break new ground excite uncharted territory add krishanti vignarajah mrs obama policy director also daunt marshall scholar law degree yale vignarajah advises first lady international affair launch let girls learn playlist like actually go get traction interest way engage lead internal debate land mrs obama put bluntly always say well make go viral make gonna go resonate another component whether view actually impact impact counterpoint land conversation talk staff word conversation uttered 50 time count mrs obama put bluntly make go viral mrs obama always try use social medium speak people get part conversation say vignarajah first lady want audience young child teenager parent soldier discus issue advocate take entire decade people understand internet fundamentally platform human conversation say goldman participate ultimately dab reject consult team rosholm told u dab hazy connection marijuana culture rule hillary clinton could dab want first lady would abstain platform first lady truly shine vine white house 436 000 follower six second video platform perfect mix straight talk goofy humor help push initiative first lady team call vine superstar user mrs obama casually refers vivers important viners vivers michael carissa rae alvarado met 2011 extra music video shoot los angeles instant chemistry michael say 2012 pair married around time start film vine perform cover new name us duo rendition hit like lana del rey summertime sadness blackstreet diggity earn five million vine follower spot network morning show eventually label attention march 2014 alvarados become first vine musician sign major label release two album far prepare put third independently first lady truly shine vine last october first lady invite us duo white house help promote better make room initiative encourages teen make room senior year schedule college application process get call say michelle obama want meet stop whatever gonna make happen say michael us duo join five vivers three hour event lele pons king bach jerome jarre amymarie gaertner chris melberger vines us duo sang better make room inspire cover fifth harmony bo king bach slid scene woefully underdressed basketball short moonwalking way justin bieber sorry jerome jarre give impromptu french lesson vivers produce nine piece content generate than91 7 million view date group five viners 41 million follower say eric waldo five year veteran department education spearhead reach higher campaign put video suddenly kid knew talk kid come say saw video king bach incredible put video suddenly kid knew talk partnership limited vine mrs obama team recognizes may one influential people planet still need help reach certain audience example first lady youtube channel let move 15 691 follower partner millennial flypaper like collegehumor funny die film short call snackpocalypse feature star chloe grace moretz tyler posey promote healthy eat cuff interview tyler oakley eight million youtube subscriber talk education content produce partnership address mrs obama initiative head others touch obliquely trick say waldo light touch success model read psa say please make sure go college say go rap jay pharoah sense humor make fun little bit approach make mrs obama seem accessible slowly surely raise awareness share far social medium campaign take mrs obama initiative hashtag reduce childhood obesity vine improve college enrollment think able move needle range issue say first lady first come office people question whether childhood obesity even issue see conversation fact change hashtag reduce childhood obesity number partially bear belief power conversation 2014 centers disease control prevention report obesity fell drastically child ages2 5 though overall remain unchanged across children2 18 3 9percent veteran unemployment rate low eight year number point harder issue 2008 2013 overall college enrollment drop 69 percent 66 percent unesco report equal access primary secondary school reach less half world country leave million girl around world without education starting online conversation might raise awareness issue long term change require conversation turn real action meanwhile country eye lock six would commanders chief social medium strategy hillary clinton arguably strong instagram account contender bernie sanders supporter feel bern tumblr ted cruz troll donald trump snapchat trump create movement partly powerful unparalleled use twitter whichever administration enters white house 2017 building obamas foundation never another first digital presidency may long time another turnip obamas set high social medium bar next first family quite yet platform unique mrs obama say white house never spend twelve month every issue make sure drive end figure want drop mic stuff published march 14 2016videoexecutive producers nilay patel tre shallowhorn director tom connors director photography ian mcalpin art director james bareham sound design andrew marino post production supervisors miriam nielsen noah shulman animation marcus mullins scott waraniak lunar north color max jeffrey vr producers craig gilbert total cinema 360 lucas wilson supersphere production vr operators joergen geerds mark waldendesignersjosh laincz kelsey scherer jason santa maria tyson whitingeditorsnilay patel michael zelenkocopy editor aaron mate editorial assistance chaim gartenberg lt var scene1 document getelementbyid scene1 var parallax1 new parallax scene1 var scene2 document getelementbyid scene2 var parallax2 new parallax scene2 var scene3 document getelementbyid scene3 var parallax3 new parallax scene3 var scene4 document getelementbyid scene4 var parallax4 new parallax scene4 var scene5 document getelementbyid scene5 var parallax5 new parallax scene5 var scene6 document getelementbyid scene6 var parallax6 new parallax scene6 var scene7 document getelementbyid scene7 var parallax7 new parallax scene7 var scene8 document getelementbyid scene8 var parallax8 new parallax scene8 var scene9 document getelementbyid scene9 var parallax9 new parallax scene9 android webos iphone ipad ipod blackberry iemobile opera mini test navigator useragent gt\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(contents[get_index[51]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wordninja\n",
    "# print(wordninja.split('DragonSlayers'))\n",
    "# str = '&nbsp;&amp;&quot;&copy;&reg;&trade;&ldquo;&rdquo;&lsquo;&rsquo;&laquo;&raquo;&lsaquo;&rsaquo;&sect;&para;&bull;&middot;&hellip;&brvbar;&ndash;&mdash;'\n",
    "# str = str.replace(';', ' ')\n",
    "# str = str.replace('&', '&amp')\n",
    "# print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing else matters.\"\n"
     ]
    }
   ],
   "source": [
    "test = 'nothing else matters.&amprdquo'\n",
    "# print(test)\n",
    "# print(test.replace('&ampnbsp',' '))\n",
    "\n",
    "replacement =   {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                }\n",
    "for str in replacement:\n",
    "    test = test.replace(str, replacement[str])\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
