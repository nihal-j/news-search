{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__1Nxb4gRIeN"
   },
   "source": [
    "Done so far :\n",
    "\n",
    "\n",
    "*   Lemmatization\n",
    "*   Stop Words Removal\n",
    "\n",
    "Verify :\n",
    "\n",
    "* Normalization - removing accents, etc.\n",
    "* Dates replaced with strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "### pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # sentence=arr[0][4]\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PvIwQ6ySVHv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data.npy\n",
    "# data.npy is a 2D array containing the dataset information as\n",
    "# data[i][0] : docID of ith document\n",
    "# data[i][1] : title of ith document\n",
    "# data[i][4] : content of ith document\n",
    "\n",
    "data = np.load('data.npy',allow_pickle = True)\n",
    "# sentence = data[0][4]\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6aQ7ZcPCH4d",
    "outputId": "8fc93dc4-51e3-48ff-fa64-d5bbef5847a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204135\n"
     ]
    }
   ],
   "source": [
    "# creating a map {index_in_data_npy, docID}\n",
    "\n",
    "# ex. if ith element in data has docID j,\n",
    "# get_docID[i] will return j\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = int(data[i][0])\n",
    "    get_index[int(data[i][0])] = i\n",
    "    # print(get_docID[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def replace_dates(documentString):\n",
    "    \n",
    "    regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    \n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp = date\n",
    "        date = date.replace('.', '/')\n",
    "        date = date.replace('-', '/')\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        newDate = newDate.replace(' ', '')\n",
    "        documentString = documentString.replace(tmp, newDate)\n",
    "        # print(newDate)\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_docIndex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ddfc4c504704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Before cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_docIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m212853\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_docIndex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m212853\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_docIndex' is not defined"
     ]
    }
   ],
   "source": [
    "# Before cleaning \n",
    "\n",
    "print(len(lemma_stop(data[get_docIndex[212853]][4])))\n",
    "print(lemma_stop(data[get_docIndex[212853]][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning (removing JSON, HTML, etc)\n",
    "\n",
    "cleaned_doc=clean_document(data[get_docIndex[212853]][4])\n",
    "print(len(lemma_stop(cleaned_doc)))\n",
    "print(lemma_stop(cleaned_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsbw8tNbrJfL"
   },
   "source": [
    "*Run the following cell once after all pre-processing (removing JSON etc), and store final lemmatized contents of all docs:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibq9gAauMI5Q"
   },
   "outputs": [],
   "source": [
    "# Run once after all pre-processing \n",
    "\n",
    "\n",
    "# # Tester code\n",
    "# import time\n",
    "\n",
    "# s = time.time()\n",
    "\n",
    "# for i in range(0, len(data)) :\n",
    "#     f_content = clean_document(data[i][4])\n",
    "#     contents = f_content\n",
    "#     # print(contents)\n",
    "#     final = lemma_stop (contents)\n",
    "# #     print(type(final))\n",
    "#     # print (final)\n",
    "    \n",
    "# print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary smaller dataset\n",
    "\n",
    "subset = []\n",
    "counter = 0\n",
    "for document in data:\n",
    "    subset.append(document)\n",
    "    counter += 1\n",
    "    if counter == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:53<00:00,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.50626349449158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "for document in tqdm(subset):\n",
    "    modifiedContent = replace_dates(document[4])\n",
    "    modifiedContent = lemma_stop(modifiedContent)\n",
    "    modifiedTitle = lemma_stop((document[1]))\n",
    "    # modifiedContent = lemma_stop(clean_document(document[4]))\n",
    "    # modifiedTitle = lemma_stop(clean_document(document[1]))\n",
    "    titles.append(modifiedTitle)\n",
    "    contents.append(modifiedContent)\n",
    "    \n",
    "print(time.time() - start)  # 110.26236414909363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "contents_temp = contents\n",
    "titles_temp = titles\n",
    "for i in range(1000):\n",
    "    for j in range(len(contents[i])):\n",
    "        contents[i][j] = unidecode.unidecode(contents[i][j])\n",
    "    for j in range(len(titles[i])):\n",
    "        titles[i][j] = unidecode.unidecode(titles[i][j])\n",
    "# for document in contents_temp:\n",
    "#     for word in document:\n",
    "#         word = unidecode.unidecode(word)\n",
    "# for title in titles_temp:\n",
    "#     for word in title:\n",
    "#         word = unidecode.unidecode(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [],
   "source": [
    "import trie\n",
    "\n",
    "# Create map from docID of the document to an object of class Node \n",
    "# (i.e, the corresponding document trie structure)\n",
    "# ex. if the docID of the document is 1, \n",
    "# getReference[1] gives the object which is the trie structure of docID 1\n",
    "\n",
    "getReference = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentRoot = []\n",
    "collection = trie.CollectionNode()\n",
    "\n",
    "# initializing the root for 1000 documents\n",
    "for i in range(1000):\n",
    "    newDocument = trie.Node()\n",
    "    documentRoot.append(newDocument)\n",
    "    getReference[get_docID[i]] = newDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:34<00:00, 28.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.5952262878418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the documents\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(1000)):\n",
    "    for w in contents_temp[i]:\n",
    "        collection.add_(w, 0, get_docID[i])\n",
    "        documentRoot[i].add(w, 0)\n",
    "\n",
    "print(time.time() - start)  #39.19705152511597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:43<00:00, 23.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import queue\n",
    "\n",
    "documentLength = {}\n",
    "N = len(documentRoot)\n",
    "\n",
    "for i in tqdm(range(len(documentRoot))):\n",
    "    \n",
    "    docID = get_docID[i]\n",
    "    length = 0\n",
    "    document = documentRoot[i]\n",
    "    q = queue.Queue()\n",
    "    q.put([document, ''])\n",
    "\n",
    "    while q.qsize() > 0:\n",
    "\n",
    "        current = q.get()\n",
    "        reference = current[0]\n",
    "        word = current[1]\n",
    "\n",
    "        if reference.words > 0:\n",
    "            df = len(collection.get_doc_list(word, 0))\n",
    "            idf = math.log10(N/df)\n",
    "            # print(word, reference.words, df)\n",
    "            length += (reference.words * idf) ** 2\n",
    "\n",
    "        for i in range(256):\n",
    "            if reference.children[i] is not None:\n",
    "                new_word = word + chr(i)\n",
    "                q.put([reference.children[i], new_word])\n",
    "\n",
    "    # print(length**0.5)\n",
    "    documentLength[docID] = length**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.getsizeof(documentRoot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9w6bB9rJfg",
    "outputId": "2e5b7bc8-2e68-4de8-f81d-a142a989c902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11Sep']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "query = '9/11'\n",
    "final_query = replace_dates(query)\n",
    "final_query = lemma_stop(final_query)\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "print(final_query)\n",
    "print(len(final_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4Kqgk9hrJfj",
    "outputId": "04bfaa8a-8053-4ab7-e55a-cee2d47edb6f"
   },
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1\n",
    "        \n",
    "    # Test code just to see distribution of query terms in the documents\n",
    "    \n",
    "    # print(w)\n",
    "    # df = len(collection.get_doc_list(w,0))\n",
    "    # print(collection.get_doc_list(w,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(documentRoot[1]), get_docID[1])\n",
    "# document = documentRoot[1]\n",
    "# N = len(documentRoot)\n",
    "\n",
    "# import queue\n",
    "# import math\n",
    "\n",
    "# length = 0\n",
    "# q = queue.Queue()\n",
    "# q.put([document, ''])\n",
    "\n",
    "# while q.qsize() > 0:\n",
    "    \n",
    "#     current = q.get()\n",
    "#     reference = current[0]\n",
    "#     word = current[1]\n",
    "    \n",
    "#     if reference.words > 0:\n",
    "#         df = len(collection.get_doc_list(word, 0))\n",
    "#         idf = math.log10(N/df)\n",
    "#         # print(word, reference.words, df)\n",
    "#         length += (reference.words * idf) ** 2\n",
    "    \n",
    "#     for i in range(256):\n",
    "#         if reference.children[i] is not None:\n",
    "#             new_word = word + chr(i)\n",
    "#             q.put([reference.children[i], new_word])\n",
    "\n",
    "# print(length**0.5)\n",
    "# print(replace_dates(subset[get_index[104]][4]))\n",
    "# replace_dates('12/12')\n",
    "# lemma_stop('12Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfT-kdUqrJfp",
    "outputId": "3c95efd0-76c9-4dfe-e425-e79c089931ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  11Sep\n",
      "\n",
      "df =  8\n",
      "idf =  2.0969100130080562\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  104\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "   ",
      "Shortly after his 17th birthday, Ali Malik asked his mother for permission to join the U.S. military. A New York City high school student, Malik had watched the twin towers implode from the top story of school, and wanted to show those behind the 9/11 attacks that “actions have consequences.” After five years in the army — including stretches in Baghdad and Fallujah at the height of the Iraqi Civil War — Malik returned home in 2008, went to St. John’s College on the G.I. Bill, and tried...  C\n",
      "tf-idf score= 1.2114498721032723\n",
      "\n",
      "doc ID =  6492\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "      By the time the law caught up with him, Ronald Carnes had been on the run for 40 years. He’d been moving from state to state after escaping from a North Carolina prison in 1973, finally landing in Waterloo, Iowa, under a pair of assumed names.He probably could have spent the rest of his life that way if it weren’t for a facial recognition program in the Iowa Department of Motor Vehicles. Scanning through the driver’s license database, the program found Carnes’ face in the system under two \n",
      "tf-idf score= 0.7569711629704844\n",
      "\n",
      "doc ID =  6802\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "      The US’s investment in medical research between 2004 and 2012 declined significantly. The same can’t be said for the rest of the world, as global investment in biomedical research actually increased during that same period, according to a study published today in the Journal of the American Medical Association. The US is \"at risk of losing its global scientific leadership and competitiveness.\"\"The United States is at risk of losing its global scientific leadership and competitiveness,\" wri\n",
      "tf-idf score= 0.6466434118230429\n",
      "\n",
      "doc ID =  7078\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "      Air travel has left an indelible mark upon the 20th century. Before the advent of the internet, the worldwide web that brought disparate people and cultures together was composed of flight paths. The face of international conflict was changed by the bombing raids of the Second World War, and some have even argued that the devastation wrought from the skies then is what kept the Cold War cold. But the 20th century also continues to exercise an influence on flying today, with the recent disa\n",
      "tf-idf score= 0.6193462278582513\n",
      "\n",
      "doc ID =  6432\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "      If a nuclear-tipped missile were hurtling toward the United States, would we be able to stop it? Maybe, if we were very lucky. But some experts warn that the United States’ missile defense system isn’t as reliable as people might think. Right now, a constellation of sensors and 36 interceptor missiles make up the ground-based midcourse defense system, or GMD. It’s intended to act as insurance against a small-scale nuclear attack from North Korea, or possibly Iran, according to the Departme\n",
      "tf-idf score= 0.40876802019953623\n",
      "\n",
      "doc ID =  6977\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "      In February, an attack ad titled \"Don’t Let the Games Begin\" started popping up in online gambling forums.The ad opens on two shadows shaking hands in a dark city alley. We see a distraught mother at her laptop, a young child at a computer in a dim bedroom, police cars with sirens. A narrator with a scaly voice comes on. \"An established al-Qaeda poker network could extract enough untraceable money from the United States in just a few days to fund several 9/11-sized attacks,\" he says. \"Say \n",
      "tf-idf score= 0.40245227170214826\n",
      "\n",
      "doc ID =  6440\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "      In March 2015, Philippe Reines, a former aide to Hillary Clinton at the US State Department, reached out to an old colleague about his consulting firm’s client. Reines contacted Capricia Marshall, a consultant who had been the US chief of protocol, a top State Department officer who acts as a liaison with foreign diplomats. Reines wanted Marshall to arrange meetings with foreign embassies for Dataminr — a company that has come under scrutiny from privacy experts for its service analyzing T\n",
      "tf-idf score= 0.3314120453705113\n",
      "\n",
      "doc ID =  151\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "      &ampldquoJesus,&amprdquo Molly said, her own plate empty, &ampldquogimme that. You know what this costs?&amprdquo She took his plate. &ampldquoThey gotta raise a whole animal for years and then they kill it. This isn&amprsquot vat stuff.&amprdquo She forked a mouthful up and chewed.&amprdquo &ampndash William Gibson, Neuromancer (1984)On Monday, August 5th, 2013, at a television studio in London in front of around 100 people, Dr. Mark Post of Maastricht University in the Netherlands unveil\n",
      "tf-idf score= 0.30721037343071\n"
     ]
    }
   ],
   "source": [
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    # print('List of documents with this term=', docs_having_query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query_term = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        # print(docID)\n",
    "        tf_doc = getReference[docID].count_words(query_term, 0)\n",
    "        # print('tf for doc',docID,'is',tf_doc)\n",
    "        tfidf_doc_query = tf_doc * idf\n",
    "        \n",
    "        # print('tfidf for doc',doc,'is',tfidf_doc)\n",
    "        # print()\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query_term * tfidf_doc_query)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [tfidf_query_term * tfidf_doc_query, query_term])\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query_term * tfidf_doc_query)\n",
    "            bisect.insort(wordsInDoc[docID], [tfidf_query_term * tfidf_doc_query, query_term])\n",
    "        \n",
    "for doc in scores:\n",
    "    if documentLength[doc] != 0:\n",
    "        scores[doc] = scores[doc]/ math.sqrt(documentLength[doc])\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    # print(i)\n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], end = ' ')\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    for word in subset[get_index[docID]][4]:\n",
    "        print (word, end = '')\n",
    "        count += 1\n",
    "        if count == 500:\n",
    "            break\n",
    "    print()\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    \n",
    "# print('\\n\\n')\n",
    "# print('============================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqsqWNnFNOhO"
   },
   "source": [
    "***Original Text :***\n",
    "\n",
    "      We’re back in Dale Cooper’s position, wandering through a freshly revived world, and trying to catch up with the ways it’s moved on in his absence.\n",
    "\n",
    "***Processed Text :***\n",
    "\n",
    "      We back Dale Cooper position wander freshly revive world try catch way move absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
