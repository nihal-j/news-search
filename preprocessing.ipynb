{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__1Nxb4gRIeN"
   },
   "source": [
    "Done so far :\n",
    "\n",
    "\n",
    "*   Lemmatization\n",
    "*   Stop Words Removal\n",
    "\n",
    "Verify :\n",
    "\n",
    "* Normalization - removing accents, etc.\n",
    "* Dates replaced with strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "### pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # sentence=arr[0][4]\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PvIwQ6ySVHv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data.npy\n",
    "# data.npy is a 2D array containing the dataset information as\n",
    "# data[i][0] : docID of ith document\n",
    "# data[i][1] : title of ith document\n",
    "# data[i][4] : content of ith document\n",
    "\n",
    "data = np.load('data.npy',allow_pickle = True)\n",
    "# sentence = data[0][4]\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6aQ7ZcPCH4d",
    "outputId": "8fc93dc4-51e3-48ff-fa64-d5bbef5847a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204135\n"
     ]
    }
   ],
   "source": [
    "# creating a map {index_in_data_npy, docID}\n",
    "\n",
    "# ex. if ith element in data has docID j,\n",
    "# get_docID[i] will return j\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = int(data[i][0])\n",
    "    get_index[int(data[i][0])] = i\n",
    "    # print(get_docID[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def replace_dates(documentString):\n",
    "    \n",
    "    regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    \n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp = date\n",
    "        date = date.replace('.', '/')\n",
    "        date = date.replace('-', '/')\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        newDate = newDate.replace(' ', '')\n",
    "        documentString = documentString.replace(tmp, newDate)\n",
    "        # print(newDate)\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1891\n",
      "['know', 'States', 'impose', 'reasonable', 'limitation', 'less', 'gun', 'crime', 'less', 'homicide', 'Sen', 'Chris', 'Murphy', 'Conn', 'speak', 'Senate', 'floor', 'June', '15', 'Readers', 'ask', 'fact', 'check', 'gun', 'rhetoric', 'use', 'Democrats', 'wake', 'mass', 'shoot', 'Orlando', 'one', 'case', 'already', 'delve', 'material', 'claim', 'new', 'let', 'take', 'look', 'start', 'Murphy', 'statement', 'Facts', 'Murphy', 'staff', 'say', 'refer', 'chart', 'appear', 'National', 'Journal', '2015', 'turn', 'carefully', 'checked', 'chart', 'President', 'Obama', 'make', 'similar', 'carefully', 'phrase', 'claim', 'gun', 'death', 'Note', 'Murphy', 'refer', 'homicide', 'gun', 'crime', 'President', 'Obama', 'earn', 'Two', 'Pinocchios', 'Readers', 'check', 'full', 'fact', 'check', 'summary', 'note', 'gun', 'death', '60', 'percent', '2013', 'actually', 'suicide', 'data', 'use', 'National', 'Journal', 'chart', 'calculates', 'number', 'gun', 'related', 'death', 'per', '100', '000', 'people', 'include', 'gun', 'death', 'include', 'homicide', 'suicide', 'accidental', 'gun', 'death', 'legal', 'intervention', 'involve', 'firearm', 'remove', 'suicide', 'total', 'reran', 'number', 'case', 'make', 'huge', 'difference', 'Half', '10', 'state', 'low', 'gun', 'death', 'rate', 'turn', 'state', 'less', 'restrictive', 'gun', 'law', 'Moreover', 'counting', 'gun', 'law', 'certainly', 'open', 'interpretation', 'also', 'affect', 'outcome', 'enough', 'count', 'law', 'figure', 'reason', 'gun', 'death', 'low', 'one', 'state', 'another', 'One', 'would', 'need', 'specifically', 'determine', 'whether', 'certain', 'law', 'effect', 'time', 'gun', 'death', 'rate', 'state', 'instance', 'Murphy', 'claim', 'worthy', 'Three', 'Pinocchios', 'specifically', 'refer', 'homicide', 'rather', 'gun', 'death', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21pk0d8', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'id', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'c3a12994', '0d15', '4748', '869d', '9dca491526fa', 'userType', 'ANONYMOUS', 'gameId', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', 'e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'murphy', '1', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466118080690', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'true', 'photoUploaded', 'false', 'shareUrl', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21pk0d8', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '69a73c61', '9b60', '4a7d', '88b8', '73090f0e953c', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466118080693', 'lastUpdated', '1466118122699', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466118122697', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_e68a3044', 'a988', '45a6', 'b59f', 'c346e82d664c', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'AR', '15', 'style', 'weapon', 'legal', 'United', 'States', '2004', 'ban', '10', 'year', 'coincidental', 'massive', 'increase', 'mass', 'shooting', 'country', '2004', 'Murphy', 'June', '15', 'another', 'problematic', 'claim', 'Murphy', 'staff', 'could', 'point', 'specific', 'data', 'back', 'significant', 'contrary', 'data', 'show', '10', 'year', 'assault', 'weapon', 'ban', 'little', 'effect', '2004', 'study', 'Justice', 'Department', 'found', 'ban', 'impact', 'gun', 'violence', 'mixed', 'best', 'ban', 'renew', 'likely', 'effect', 'gun', 'violence', 'likely', 'small', 'best', 'perhaps', 'small', 'reliable', 'measurement', 'report', 'say', 'assault', 'weapon', 'rarely', 'use', 'gun', 'crime', 'James', 'Alan', 'Fox', 'Northeastern', 'University', 'professor', 'collect', 'data', 'back', '1982', 'show', 'assault', 'weapon', 'account', '24', '6', 'percent', 'public', 'mass', 'shooting', 'Assault', 'weapon', 'commonplace', 'mass', 'shooting', 'gun', 'control', 'advocate', 'believe', 'Fox', 'write', '2012', 'article', 'journal', 'Homicide', 'Studies', 'Instead', 'semiautomatic', 'handgun', '47', '9', 'percent', 'far', 'prevalent', 'random', 'massacre', 'firearm', 'would', 'typically', 'classify', 'assault', 'weapon', 'assault', 'weapon', 'ban', 'make', 'difference', 'mass', 'shooting', 'significantly', 'accord', 'Fox', 'data', '1976', '1994', '18', 'mass', 'shooting', 'per', 'year', 'ban', '1995', '2004', '19', 'incident', 'per', 'year', 'ban', '2011', 'average', 'go', 'nearly', '21', '2016', 'study', 'publish', 'Applied', 'Economics', 'Benjamin', 'Blau', 'Utah', 'State', 'University', 'colleague', 'also', 'look', 'whether', 'state', 'federal', 'law', 'assault', 'rifle', 'affected', 'whether', 'weapon', 'use', 'public', 'shooting', '1982', '2014', 'study', 'seem', 'indicate', 'Federal', 'assault', 'rifle', 'ban', 'individual', 'State', 'assault', 'rifle', 'ban', 'affect', 'likelihood', 'assault', 'rifle', 'use', 'mass', 'shoot', 'Blau', 'say', 'Said', 'differently', 'type', 'ban', 'appear', 'deter', 'use', 'assault', 'rifle', 'mass', 'shoot', 'data', 'use', 'determine', 'whether', 'assault', 'rifle', 'ban', 'negatively', 'influence', 'likelihood', 'occurrence', 'mass', 'shoot', 'conclusive', 'colleague', 'Christopher', 'Ingraham', 'point', 'assault', 'style', 'rifle', 'use', 'seven', 'eight', 'high', 'profile', 'public', 'mass', 'shooting', 'since', 'July', 'last', 'year', 'certainly', 'raise', 'profile', 'weapon', 'data', 'far', 'show', 'link', 'use', 'weapon', 'lift', 'ban', 'Murphy', 'asserts', 'bottom', 'line', 'statistic', 'cite', 'story', 'told', 'Wednesday', 'show', 'undeniable', 'gun', 'kill', 'thousand', 'Americans', 'every', 'year', 'say', 'Murphy', 'spokesman', 'Chris', 'Harris', 'step', 'back', 'look', 'totality', 'data', 'easy', 'access', 'firearm', 'way', 'regulation', 'lead', 'increase', 'gun', 'death', 'homicide', 'suicide', 'true', 'U', 'compare', 'state', 'state', 'compare', 'U', 'nation', 'Pinocchio', 'Test', 'Murphy', 'say', 'coincidental', 'mass', 'shooting', 'increase', 'since', 'ban', 'lift', 'data', 'show', 'ban', 'particularly', 'effective', 'first', 'place', 'mass', 'shooting', 'increase', 'significantly', 'since', 'data', 'set', 'relatively', 'small', 'maybe', 'something', 'change', 'past', 'year', 'claim', 'worthy', 'Three', 'Pinocchios', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21piYOd', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'id', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'a0847542', 'f10f', '4409', 'a330', '85f46a3b3c8b', 'userType', 'ANONYMOUS', 'gameId', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', '2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'trump', '2', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466118160561', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'true', 'photoUploaded', 'false', 'shareUrl', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'wapo', 'st', '21piYOd', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '43f3b445', 'ee45', '45b7', 'bfba', 'cf6132ccda74', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466118160563', 'lastUpdated', '1466126246609', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466126246607', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_2558761d', '9989', '416f', 'a8c6', '9fdd7289fb97', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'America', 'absolutely', 'awash', 'easily', 'obtainable', 'firearm', 'go', 'gun', 'show', 'local', 'convention', 'center', 'come', 'away', 'fully', 'automatic', 'assault', 'rifle', 'without', 'background', 'check', 'likely', 'without', 'show', 'identification', 'card', 'wait', 'Sen', 'Minority', 'Leader', 'Harry', 'Reid', 'Nev', 'quote', 'al', 'Qaeda', 'spokesman', 'statement', 'Senate', 'floor', 'June', '15', '2016', 'indeed', 'al', 'Qaeda', 'spokesman', 'Adam', 'Gadahn', 'use', 'exact', 'language', '2011', 'American', 'born', 'Gadahn', 'later', 'kill', 'drone', 'strike', '2015', 'clip', 'even', 'terrorist', 'get', 'fact', 'wrong', 'Senate', 'leader', 'uncritically', 'quote', 'Reid', 'put', 'terrorist', 'talk', 'gun', 'show', 'loophole', 'specifically', 'point', 'flaw', 'nation', 'gun', 'law', 'allows', 'convict', 'terrorist', 'slip', 'big', 'wide', 'hole', 'slip', 'Actually', 'buy', 'fully', 'automatic', 'assault', 'rifle', 'gun', 'show', 'al', 'Qaeda', 'spokesman', 'extension', 'Reid', 'mix', 'semiautomatic', 'weapon', 'automatic', 'weapon', 'Semiautomatic', 'define', '1968', 'Gun', 'Control', 'Act', 'mean', 'one', 'pull', 'trigger', 'equates', 'one', 'bullet', 'leave', 'barrel', 'Fully', 'automatic', 'rifle', 'available', 'United', 'States', 'require', 'six', 'month', 'paperwork', 'Bureau', 'Alcohol', 'Tobacco', 'Firearms', 'Explosives', 'also', 'significantly', 'expensive', 'single', 'fire', 'counterpart', 'ban', 'many', 'state', 'Moreover', '1986', 'law', 'ban', 'new', 'one', 'automatic', 'weapon', 'purchase', 'would', 'old', 'one', 'Still', 'one', 'could', 'purchase', 'semiautomatic', 'weapon', 'retrofit', 'something', 'call', 'auto', 'sear', 'mimic', 'automatic', 'weapon', 'though', 'weapon', 'still', 'fit', 'definition', 'semiautomatic', 'modification', 'require', 'permission', 'ATF', 'video', 'describe', 'one', 'product', 'call', 'gun', 'show', 'loophole', 'refers', 'private', 'sale', 'within', 'state', 'line', 'license', 'gun', 'dealer', 'gun', 'show', 'must', 'run', 'background', 'check', 'Anyone', 'gun', 'show', 'sell', 'someone', 'state', 'must', 'run', 'background', 'check', 'number', 'state', 'include', 'California', 'New', 'York', 'require', 'background', 'check', 'gun', 'transaction', 'state', 'require', 'handgun', 'purchase', 'gun', 'show', 'require', 'background', 'check', 'matter', 'policy', 'matter', 'state', 'law', 'say', 'Gadahn', 'extensive', 'Reid', 'speak', 'sloppily', 'Reid', 'spokesman', 'decline', 'provide', 'record', 'comment', 'Pinocchio', 'Test', 'may', 'make', 'sense', 'award', 'Pinocchios', 'dead', 'al', 'Qaeda', 'operative', 'case', 'Reid', 'clearly', 'state', 'Gadahn', 'correct', 'even', 'believe', 'quote', 'make', 'noteworthy', 'point', 'Reid', 'quote', 'fully', 'automatic', 'line', 'twice', 'without', 'inform', 'listener', 'actually', 'possible', 'Moreover', 'Reid', 'make', 'comment', 'prepared', 'statement', 'publicly', 'release', 'news', 'release', 'could', 'still', 'update', 'correction', 'Two', 'Pinocchios', 'rating', 'scale', 'Send', 'u', 'fact', 'check', 'fill', 'form', 'Check', '2016', 'candidate', 'fact', 'check', 'page', 'Sign', 'Fact', 'Checker', 'weekly', 'newsletter', 'var', 'urlRegex', 'b', 'http', 'Z0', '9', '_', 'Z0', '9', '_', 'ig', 'endPointString', 'Map', 'pattern', 'x3e', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'poll', 'washingtonpost', 'com', 'userpolls', 'game', 'webapi', 'poll', 'id', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'Config', 'WPGames', 'WPGames', 'WPGames', 'common', 'WPGames', 'common', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'WPGames', 'common', 'api', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'endPointString', 'match', 'urlRegex', '0', 'replace', 'poll', 'var', 'shareURL', 'encodeURIComponent', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'window', 'interactiveOmniture', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'id', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'type', 'interactive', 'subtype', 'poll', 'embed', 'embed', 'true', 'userId', 'd6b018de', '31f6', '46f9', '8dd6', 'fd20a63ac97a', 'userType', 'ANONYMOUS', 'gameId', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'correctAnswers', '0', 'wrongAnswers', '0', 'answeredQuestions', '0', 'game', 'gameId', 'b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'sectionId', 'politics', 'blogName', 'fact', 'checker', 'title', 'fact', 'checker', 'rating', 'reid', 'byline', 'glenn', 'kessler', 'bylineBioPage', 'http', 'web', 'archive', 'org', 'web', '20160617095400', 'http', 'www', 'washingtonpost', 'com', 'people', 'glenn', 'kessler', 'articleReferences', 'createdTimestamp', '1466117973960', 'archive', 'false', 'allowMoreThanOnce', 'false', 'createdBy', 'KESSLERGA', 'lastUpdatedBy', 'KESSLERGA', 'live', 'false', 'photoUploaded', 'false', 'commercialNode', 'politics', 'embedToggles', 'headline', 'false', 'byline', 'false', 'captchaProtected', 'false', 'question', 'questionId', '1a6d57ae', 'd8a8', '43e7', '8b1e', 'f60ce8ccafa1', 'questionText', 'p', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', 'p', 'excludeFromTrivia', 'false', 'articleReference', 'option', 'optionId', '9316b3ff', 'f09c', '4846', 'ad19', 'd61d85ee4ace', 'optionText', 'hasComment', 'false', 'optionId', 'd6f6a56a', 'e3cb', '47c5', '8e22', 'cf0e6b5acde8', 'optionText', 'hasComment', 'false', 'optionId', '6b850572', '808b', '4b93', 'aedf', '98ffd0a039e7', 'optionText', 'hasComment', 'false', 'optionId', '1a53846e', '4b40', '4340', 'b586', '701aefcd9b94', 'optionText', 'hasComment', 'false', 'optionId', '2de7a71d', '0242', '4b76', '9662', '87fe2a78bdee', 'optionText', 'hasComment', 'false', 'createdDate', '1466117973962', 'lastUpdated', '1466117973962', 'multipleSelectionAmount', '0', 'type', 'RATING', 'rating', 'iconType', 'PINOCCHIO', 'customIconName', 'PINOCCHIO', 'exclusive', 'false', 'allowDuplicate', 'false', 'lastUpdatedTimestamp', '1466117973960', '0', 'var', 'poll', 'JSON', 'parse', 'document', 'getElementById', 'pollData_b80ce38c', 'fbcf', '4bfc', '985e', 'fc63b800449f', 'innerHTML', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter']\n"
     ]
    }
   ],
   "source": [
    "# Before cleaning \n",
    "\n",
    "print(len(lemma_stop(data[get_index[212853]][4])))\n",
    "print(lemma_stop(data[get_index[212853]][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1116\n",
      "['know', 'States', 'impose', 'reasonable', 'limitation', 'less', 'gun', 'crime', 'less', 'homicide', 'Sen', 'Chris', 'Murphy', 'Conn', 'speak', 'Senate', 'floor', 'June', '15', 'Readers', 'ask', 'fact', 'check', 'gun', 'rhetoric', 'use', 'Democrats', 'wake', 'mass', 'shoot', 'Orlando', 'one', 'case', 'already', 'delve', 'material', 'claim', 'new', 'let', 'take', 'look', 'start', 'Murphy', 'statement', 'Facts', 'Murphy', 'staff', 'say', 'refer', 'chart', 'appear', 'National', 'Journal', '2015', 'turn', 'carefully', 'checked', 'chart', 'President', 'Obama', 'make', 'similar', 'carefully', 'phrase', 'claim', 'gun', 'death', 'Note', 'Murphy', 'refer', 'homicide', 'gun', 'crime', 'President', 'Obama', 'earn', 'Two', 'Pinocchios', 'Readers', 'check', 'full', 'fact', 'check', 'summary', 'note', 'gun', 'death', '60', 'percent', '2013', 'actually', 'suicide', 'data', 'use', 'National', 'Journal', 'chart', 'calculates', 'number', 'gun', 'related', 'death', 'per', '100', '000', 'people', 'include', 'gun', 'death', 'include', 'homicide', 'suicide', 'accidental', 'gun', 'death', 'legal', 'intervention', 'involve', 'firearm', 'remove', 'suicide', 'total', 'reran', 'number', 'case', 'make', 'huge', 'difference', 'Half', '10', 'state', 'low', 'gun', 'death', 'rate', 'turn', 'state', 'less', 'restrictive', 'gun', 'law', 'Moreover', 'counting', 'gun', 'law', 'certainly', 'open', 'interpretation', 'also', 'affect', 'outcome', 'enough', 'count', 'law', 'figure', 'reason', 'gun', 'death', 'low', 'one', 'state', 'another', 'One', 'would', 'need', 'specifically', 'determine', 'whether', 'certain', 'law', 'effect', 'time', 'gun', 'death', 'rate', 'state', 'instance', 'Murphy', 'claim', 'worthy', 'Three', 'Pinocchios', 'specifically', 'refer', 'homicide', 'rather', 'gun', 'death', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'checker', 'rating', 'murphy', '1', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'AR', '15', 'style', 'weapon', 'legal', 'United', 'States', '2004', 'ban', '10', 'year', 'coincidental', 'massive', 'increase', 'mass', 'shooting', 'country', '2004', 'Murphy', 'June', '15', 'another', 'problematic', 'claim', 'Murphy', 'staff', 'could', 'point', 'specific', 'data', 'back', 'significant', 'contrary', 'data', 'show', '10', 'year', 'assault', 'weapon', 'ban', 'little', 'effect', '2004', 'study', 'Justice', 'Department', 'found', 'ban', 'impact', 'gun', 'violence', 'mixed', 'best', 'ban', 'renew', 'likely', 'effect', 'gun', 'violence', 'likely', 'small', 'best', 'perhaps', 'small', 'reliable', 'measurement', 'report', 'say', 'assault', 'weapon', 'rarely', 'use', 'gun', 'crime', 'James', 'Alan', 'Fox', 'Northeastern', 'University', 'professor', 'collect', 'data', 'back', '1982', 'show', 'assault', 'weapon', 'account', '24', '6', 'percent', 'public', 'mass', 'shooting', 'Assault', 'weapon', 'commonplace', 'mass', 'shooting', 'gun', 'control', 'advocate', 'believe', 'Fox', 'write', '2012', 'article', 'journal', 'Homicide', 'Studies', 'Instead', 'semiautomatic', 'handgun', '47', '9', 'percent', 'far', 'prevalent', 'random', 'massacre', 'firearm', 'would', 'typically', 'classify', 'assault', 'weapon', 'assault', 'weapon', 'ban', 'make', 'difference', 'mass', 'shooting', 'significantly', 'accord', 'Fox', 'data', '1976', '1994', '18', 'mass', 'shooting', 'per', 'year', 'ban', '1995', '2004', '19', 'incident', 'per', 'year', 'ban', '2011', 'average', 'go', 'nearly', '21', '2016', 'study', 'publish', 'Applied', 'Economics', 'Benjamin', 'Blau', 'Utah', 'State', 'University', 'colleague', 'also', 'look', 'whether', 'state', 'federal', 'law', 'assault', 'rifle', 'affected', 'whether', 'weapon', 'use', 'public', 'shooting', '1982', '2014', 'study', 'seem', 'indicate', 'Federal', 'assault', 'rifle', 'ban', 'individual', 'State', 'assault', 'rifle', 'ban', 'affect', 'likelihood', 'assault', 'rifle', 'use', 'mass', 'shoot', 'Blau', 'say', 'Said', 'differently', 'type', 'ban', 'appear', 'deter', 'use', 'assault', 'rifle', 'mass', 'shoot', 'data', 'use', 'determine', 'whether', 'assault', 'rifle', 'ban', 'negatively', 'influence', 'likelihood', 'occurrence', 'mass', 'shoot', 'conclusive', 'colleague', 'Christopher', 'Ingraham', 'point', 'assault', 'style', 'rifle', 'use', 'seven', 'eight', 'high', 'profile', 'public', 'mass', 'shooting', 'since', 'July', 'last', 'year', 'certainly', 'raise', 'profile', 'weapon', 'data', 'far', 'show', 'link', 'use', 'weapon', 'lift', 'ban', 'Murphy', 'asserts', 'bottom', 'line', 'statistic', 'cite', 'story', 'told', 'Wednesday', 'show', 'undeniable', 'gun', 'kill', 'thousand', 'Americans', 'every', 'year', 'say', 'Murphy', 'spokesman', 'Chris', 'Harris', 'step', 'back', 'look', 'totality', 'data', 'easy', 'access', 'firearm', 'way', 'regulation', 'lead', 'increase', 'gun', 'death', 'homicide', 'suicide', 'true', 'U', 'compare', 'state', 'state', 'compare', 'U', 'nation', 'Pinocchio', 'Test', 'Murphy', 'say', 'coincidental', 'mass', 'shooting', 'increase', 'since', 'ban', 'lift', 'data', 'show', 'ban', 'particularly', 'effective', 'first', 'place', 'mass', 'shooting', 'increase', 'significantly', 'since', 'data', 'set', 'relatively', 'small', 'maybe', 'something', 'change', 'past', 'year', 'claim', 'worthy', 'Three', 'Pinocchios', 'Three', 'Pinocchios', 'rating', 'scale', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'checker', 'rating', 'trump', '2', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter', 'America', 'absolutely', 'awash', 'easily', 'obtainable', 'firearm', 'go', 'gun', 'show', 'local', 'convention', 'center', 'come', 'away', 'fully', 'automatic', 'assault', 'rifle', 'without', 'background', 'check', 'likely', 'without', 'show', 'identification', 'card', 'wait', 'Sen', 'Minority', 'Leader', 'Harry', 'Reid', 'Nev', 'quote', 'al', 'Qaeda', 'spokesman', 'statement', 'Senate', 'floor', 'June', '15', '2016', 'indeed', 'al', 'Qaeda', 'spokesman', 'Adam', 'Gadahn', 'use', 'exact', 'language', '2011', 'American', 'born', 'Gadahn', 'later', 'kill', 'drone', 'strike', '2015', 'clip', 'even', 'terrorist', 'get', 'fact', 'wrong', 'Senate', 'leader', 'uncritically', 'quote', 'Reid', 'put', 'terrorist', 'talk', 'gun', 'show', 'loophole', 'specifically', 'point', 'flaw', 'nation', 'gun', 'law', 'allows', 'convict', 'terrorist', 'slip', 'big', 'wide', 'hole', 'slip', 'Actually', 'buy', 'fully', 'automatic', 'assault', 'rifle', 'gun', 'show', 'al', 'Qaeda', 'spokesman', 'extension', 'Reid', 'mix', 'semiautomatic', 'weapon', 'automatic', 'weapon', 'Semiautomatic', 'define', '1968', 'Gun', 'Control', 'Act', 'mean', 'one', 'pull', 'trigger', 'equates', 'one', 'bullet', 'leave', 'barrel', 'Fully', 'automatic', 'rifle', 'available', 'United', 'States', 'require', 'six', 'month', 'paperwork', 'Bureau', 'Alcohol', 'Tobacco', 'Firearms', 'Explosives', 'also', 'significantly', 'expensive', 'single', 'fire', 'counterpart', 'ban', 'many', 'state', 'Moreover', '1986', 'law', 'ban', 'new', 'one', 'automatic', 'weapon', 'purchase', 'would', 'old', 'one', 'Still', 'one', 'could', 'purchase', 'semiautomatic', 'weapon', 'retrofit', 'something', 'call', 'auto', 'sear', 'mimic', 'automatic', 'weapon', 'though', 'weapon', 'still', 'fit', 'definition', 'semiautomatic', 'modification', 'require', 'permission', 'ATF', 'video', 'describe', 'one', 'product', 'call', 'gun', 'show', 'loophole', 'refers', 'private', 'sale', 'within', 'state', 'line', 'license', 'gun', 'dealer', 'gun', 'show', 'must', 'run', 'background', 'check', 'Anyone', 'gun', 'show', 'sell', 'someone', 'state', 'must', 'run', 'background', 'check', 'number', 'state', 'include', 'California', 'New', 'York', 'require', 'background', 'check', 'gun', 'transaction', 'state', 'require', 'handgun', 'purchase', 'gun', 'show', 'require', 'background', 'check', 'matter', 'policy', 'matter', 'state', 'law', 'say', 'Gadahn', 'extensive', 'Reid', 'speak', 'sloppily', 'Reid', 'spokesman', 'decline', 'provide', 'record', 'comment', 'Pinocchio', 'Test', 'may', 'make', 'sense', 'award', 'Pinocchios', 'dead', 'al', 'Qaeda', 'operative', 'case', 'Reid', 'clearly', 'state', 'Gadahn', 'correct', 'even', 'believe', 'quote', 'make', 'noteworthy', 'point', 'Reid', 'quote', 'fully', 'automatic', 'line', 'twice', 'without', 'inform', 'listener', 'actually', 'possible', 'Moreover', 'Reid', 'make', 'comment', 'prepared', 'statement', 'publicly', 'release', 'news', 'release', 'could', 'still', 'update', 'correction', 'Two', 'Pinocchios', 'rating', 'scale', 'Send', 'u', 'fact', 'check', 'fill', 'form', 'Check', '2016', 'candidate', 'fact', 'check', 'page', 'Sign', 'Fact', 'Checker', 'weekly', 'newsletter', 'var', 'x3e', 'id', 'x3e', 'poll', 'content', 'x3e', 'Simple', 'Id', 'Content', 'shareURL', 'encodeURIComponent', 'checker', 'rating', 'reid', 'byline', 'glenn', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', '0', 'var', 'var', 'viewType', 'embed', 'User', 'Poll', 'Results', 'Voting', 'close', 'poll', 'would', 'rate', 'claim', 'check', 'mark', 'mean', 'think', 'statement', 'true', 'agree', 'rating', '20', '20', '20', '20', '20', 'Pardon', 'interruption', 'need', 'verify', 'actual', 'person', 'View', 'Results', 'non', 'scientific', 'user', 'poll', 'Results', 'statistically', 'valid', 'cannot', 'assume', 'reflect', 'view', 'Washington', 'Post', 'user', 'group', 'general', 'population', 'Share', 'poll', 'Share', 'Facebook', 'Share', 'Twitter']\n"
     ]
    }
   ],
   "source": [
    "# After cleaning (removing JSON, HTML, etc)\n",
    "\n",
    "cleaned_doc=clean_document(data[get_index[212853]][4])\n",
    "print(len(lemma_stop(cleaned_doc)))\n",
    "print(lemma_stop(cleaned_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsbw8tNbrJfL"
   },
   "source": [
    "*Run the following cell once after all pre-processing (removing JSON etc), and store final lemmatized contents of all docs:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibq9gAauMI5Q"
   },
   "outputs": [],
   "source": [
    "# Run once after all pre-processing \n",
    "\n",
    "\n",
    "# # Tester code\n",
    "# import time\n",
    "\n",
    "# s = time.time()\n",
    "\n",
    "# for i in range(0, len(data)) :\n",
    "#     f_content = clean_document(data[i][4])\n",
    "#     contents = f_content\n",
    "#     # print(contents)\n",
    "#     final = lemma_stop (contents)\n",
    "# #     print(type(final))\n",
    "#     # print (final)\n",
    "    \n",
    "# print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary smaller dataset\n",
    "\n",
    "subset = []\n",
    "counter = 0\n",
    "for document in data:\n",
    "    subset.append(document)\n",
    "    counter += 1\n",
    "    if counter == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:42<00:00, 11.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.19727683067322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "for document in tqdm(subset):\n",
    "    modifiedContent = replace_dates(document[4])\n",
    "    modifiedContent = lemma_stop(modifiedContent)\n",
    "    modifiedTitle = lemma_stop((document[1]))\n",
    "    # modifiedContent = lemma_stop(clean_document(document[4]))\n",
    "    # modifiedTitle = lemma_stop(clean_document(document[1]))\n",
    "    titles.append(modifiedTitle)\n",
    "    contents.append(modifiedContent)\n",
    "    \n",
    "print(time.time() - start)  # 110.26236414909363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "contents_temp = contents\n",
    "titles_temp = titles\n",
    "for i in range(1000):\n",
    "    for j in range(len(contents[i])):\n",
    "        contents[i][j] = unidecode.unidecode(contents[i][j])\n",
    "    for j in range(len(titles[i])):\n",
    "        titles[i][j] = unidecode.unidecode(titles[i][j])\n",
    "# for document in contents_temp:\n",
    "#     for word in document:\n",
    "#         word = unidecode.unidecode(word)\n",
    "# for title in titles_temp:\n",
    "#     for word in title:\n",
    "#         word = unidecode.unidecode(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [],
   "source": [
    "import trie\n",
    "\n",
    "# Create map from docID of the document to an object of class Node \n",
    "# (i.e, the corresponding document trie structure)\n",
    "# ex. if the docID of the document is 1, \n",
    "# getReference[1] gives the object which is the trie structure of docID 1\n",
    "\n",
    "getReference = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentRoot = []\n",
    "collection = trie.CollectionNode()\n",
    "\n",
    "# initializing the root for 1000 documents\n",
    "for i in range(1000):\n",
    "    newDocument = trie.Node()\n",
    "    documentRoot.append(newDocument)\n",
    "    getReference[get_docID[i]] = newDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:23<00:00, 42.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.73133397102356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the documents\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(1000)):\n",
    "    for w in contents_temp[i]:\n",
    "        collection.add_(w, 0, get_docID[i])\n",
    "        documentRoot[i].add(w, 0)\n",
    "\n",
    "print(time.time() - start)  #39.19705152511597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import queue\n",
    "\n",
    "documentLength = {}\n",
    "N = len(documentRoot)\n",
    "\n",
    "for i in tqdm(range(len(documentRoot))):\n",
    "    \n",
    "    docID = get_docID[i]\n",
    "    length = 0\n",
    "    document = documentRoot[i]\n",
    "    q = queue.Queue()\n",
    "    q.put([document, ''])\n",
    "\n",
    "    while q.qsize() > 0:\n",
    "\n",
    "        current = q.get()\n",
    "        reference = current[0]\n",
    "        word = current[1]\n",
    "\n",
    "        if reference.words > 0:\n",
    "            df = len(collection.get_doc_list(word, 0))\n",
    "            idf = math.log10(N/df)\n",
    "            # print(word, reference.words, df)\n",
    "            length += (reference.words * idf) ** 2\n",
    "\n",
    "        for i in range(256):\n",
    "            if reference.children[i] is not None:\n",
    "                new_word = word + chr(i)\n",
    "                q.put([reference.children[i], new_word])\n",
    "\n",
    "    # print(length**0.5)\n",
    "    documentLength[docID] = length**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.getsizeof(documentRoot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9w6bB9rJfg",
    "outputId": "2e5b7bc8-2e68-4de8-f81d-a142a989c902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11Sep']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "query = '9/11'\n",
    "final_query = replace_dates(query)\n",
    "final_query = lemma_stop(final_query)\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "print(final_query)\n",
    "print(len(final_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4Kqgk9hrJfj",
    "outputId": "04bfaa8a-8053-4ab7-e55a-cee2d47edb6f"
   },
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1\n",
    "        \n",
    "    # Test code just to see distribution of query terms in the documents\n",
    "    \n",
    "    # print(w)\n",
    "    # df = len(collection.get_doc_list(w,0))\n",
    "    # print(collection.get_doc_list(w,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(documentRoot[1]), get_docID[1])\n",
    "# document = documentRoot[1]\n",
    "# N = len(documentRoot)\n",
    "\n",
    "# import queue\n",
    "# import math\n",
    "\n",
    "# length = 0\n",
    "# q = queue.Queue()\n",
    "# q.put([document, ''])\n",
    "\n",
    "# while q.qsize() > 0:\n",
    "    \n",
    "#     current = q.get()\n",
    "#     reference = current[0]\n",
    "#     word = current[1]\n",
    "    \n",
    "#     if reference.words > 0:\n",
    "#         df = len(collection.get_doc_list(word, 0))\n",
    "#         idf = math.log10(N/df)\n",
    "#         # print(word, reference.words, df)\n",
    "#         length += (reference.words * idf) ** 2\n",
    "    \n",
    "#     for i in range(256):\n",
    "#         if reference.children[i] is not None:\n",
    "#             new_word = word + chr(i)\n",
    "#             q.put([reference.children[i], new_word])\n",
    "\n",
    "# print(length**0.5)\n",
    "# print(replace_dates(subset[get_index[104]][4]))\n",
    "# replace_dates('12/12')\n",
    "# lemma_stop('12Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfT-kdUqrJfp",
    "outputId": "3c95efd0-76c9-4dfe-e425-e79c089931ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  11Sep\n",
      "\n",
      "df =  8\n",
      "idf =  2.0969100130080562\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  104\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "FOUND\n",
      "9/11\n",
      "... join U.S. military. New York City high school student, Malik watched twin towers implode top story school, wanted show behind 9/11 attacks “actions consequences.” five years army including stretches Baghdad Fallujah height Iraqi Civil War Malik returned home 2008, went St. John’s College G.I. Bill, tried... Continue reading&amphellip  ... \n",
      "tf-idf score= 1.713248839263599\n",
      "\n",
      "doc ID =  6492\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "FOUND\n",
      "9/11\n",
      "... behind new DMV systems new wave federal requirements newly available federal money meet new standards. RealID Act passed 2005 response 9/11 Commission’s identification requirements including requirement driver’s licenses stored digital form. States still charge licenses, licenses meet new federal requirements, stop valid use airports early 2018.The act say anything facial recognition, strongly encourages states find ways reduce fraud. Many seen facial scans natural way particularly since act come wave federal  ... \n",
      "tf-idf score= 0.7569711629704844\n",
      "\n",
      "doc ID =  6802\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "FOUND\n",
      "9/11.\n",
      "... dollar goes further science dollar.\" bigger part reason lies economic downturns occurred early 2000s, well enhanced interest international security following 9/11. US federal government runs deficits, biomedical research de-emphasized,\" Moses says. although may seem intuitively correct, would hope see trend reversed investment science tech good economic investment.\"\"Asia knows biomedical research vehicle enhance international stature.\"The good news, Moses says, possible huge impact changing research priorities. long time, Ebola viewed threat, therefore  ... \n",
      "tf-idf score= 0.6466434118230429\n",
      "\n",
      "doc ID =  7078\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "FOUND\n",
      "9/11\n",
      "... thwarted pilot unions fearful excessive oversight.Another compelling answer may simply cost. Gary Kelly, CEO budget American carrier Southwest Airlines, argues 9/11 low-fare carriers.\" opinion piece published week, Kelly paints picture razor-thin profit margins start last decade forced airlines shave every expenditure minimum. goes way explaining plane operators may reluctant make fresh investments upgrading CVR equipment. It&amprsquos easier sell people benefits sophisticated in-flight entertainment system convince plane they&amprsquore flying would make  ... \n",
      "tf-idf score= 0.6193462278582513\n",
      "\n",
      "doc ID =  6432\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "FOUND\n",
      "9/11,\n",
      "... system could fuel race weapons could overpower Clinton administration started making moves toward National Missile Defense 1990s. real push came 9/11, George Bush pulled anti-ballistic missile treaty fast-tracked became GMD. Grego colleagues Union Concerned Scientists argue, set system failure rushing missiles essentially still prototypes silos. Karako counters speed rollout necessary: unwilling, nation, accept vulnerability, complete vulnerability, blackmailed North Korea. going defenses, going improve time.” Photo: Missile Defense Agency Launch target  ... \n",
      "tf-idf score= 0.5780852780305834\n",
      "\n",
      "doc ID =  6440\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "FOUND\n",
      "9/11,”\n",
      "... primarily refusal accept US intelligence agencies’ conclusion Russia sought meddle presidential election. Morell called alleged Russian hacking campaign political equivalent 9/11,” Bash Beacon staffers warned journalists writing stories based leaks.Still, least one Beacon associate likely Trump’s cabinet: Trump selected Retired Marine General John Kelly, Beacon advisor, lead Department Homeland Security $255,000 contract access Dataminr’s software. Another Beacon member might wind Trump administration. Beacon vice president Bryan Smith recently named Trump’s  ... \n",
      "tf-idf score= 0.46868740929678454\n",
      "\n",
      "doc ID =  151\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "FOUND\n",
      "9/11\n",
      "... USDA Food Availability (Per Capita) Data System, USDA Livestock &amp Meat Domestic Data\"Here truth: earth round Saddam Hussein attack us 9/11 Elvis dead Obama born United States climate crisis real.\" &ampndash Al GoreTo produce 1 pound beef, friendly neighborhood farmer need 13 pounds grain estimated 2,500 gallons water. 1,000-pound cow yields 600 pounds beef, cow used 1.5 million gallons water 7,800 pounds grain. basic level, farming scale pretty inefficient, could  ... \n",
      "tf-idf score= 0.43446107660741323\n",
      "\n",
      "doc ID =  6977\n",
      "Keywords:\n",
      "11Sep \n",
      "\n",
      "FOUND\n",
      "9/11-sized\n",
      "... cars sirens. narrator scaly voice comes established al-Qaeda poker network could extract enough untraceable money United States days fund several 9/11-sized attacks,\" says. \"Say internet gambling.\"The message clear: internet gambling fund terrorism destroy family.Players reacted strongly, calling \"ridiculous hysterics\" \"distortion best.\" \"Get life, Sheldon!\" one commenter wrote.Sheldon course Sheldon Adelson, 80-year-old billionaire casino owner declared war internet gambling, breaking rest industry. pledged spend \"whatever takes\" fight spread online casinos, one  ... \n",
      "tf-idf score= 0.40245227170214826\n"
     ]
    }
   ],
   "source": [
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    # print('List of documents with this term=', docs_having_query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query_term = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        # print(docID)\n",
    "        tf_doc = getReference[docID].count_words(query_term, 0)\n",
    "        # print('tf for doc',docID,'is',tf_doc)\n",
    "        tfidf_doc_query = tf_doc * idf\n",
    "        \n",
    "        # print('tfidf for doc',doc,'is',tfidf_doc)\n",
    "        # print()\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query_term * tfidf_doc_query)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [tfidf_query_term * tfidf_doc_query, query_term])\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query_term * tfidf_doc_query)\n",
    "            bisect.insort(wordsInDoc[docID], [tfidf_query_term * tfidf_doc_query, query_term])\n",
    "        \n",
    "for doc in scores:\n",
    "    if documentLength[doc] != 0:\n",
    "        scores[doc] = scores[doc]/ math.sqrt(documentLength[doc])\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    # print(i)\n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], end = ' ')\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    for word in subset[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word)\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print()\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    \n",
    "# print('\\n\\n')\n",
    "# print('============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11Sep']\n"
     ]
    }
   ],
   "source": [
    "check_with=replace_dates('9/11')\n",
    "check_with=str(lemma_stop(check_with))\n",
    "print(check_with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqsqWNnFNOhO"
   },
   "source": [
    "***Original Text :***\n",
    "\n",
    "      We’re back in Dale Cooper’s position, wandering through a freshly revived world, and trying to catch up with the ways it’s moved on in his absence.\n",
    "\n",
    "***Processed Text :***\n",
    "\n",
    "      We back Dale Cooper position wander freshly revive world try catch way move absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
