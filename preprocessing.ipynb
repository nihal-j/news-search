{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__1Nxb4gRIeN"
   },
   "source": [
    "Done so far :\n",
    "\n",
    "\n",
    "*   Lemmatization\n",
    "*   Stop Words Removal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # sentence=arr[0][4]\n",
    "    tokenizer=RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized=[lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filtered_sentence=[w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop=' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence\n",
    "#     return after_lemma_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PvIwQ6ySVHv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data.npy\n",
    "# data.npy is a 2D array containing the dataset information as\n",
    "# data[i][0] : docID of ith document\n",
    "# data[i][1] : title of ith document\n",
    "# data[i][4] : content of ith document\n",
    "\n",
    "data = np.load('data.npy',allow_pickle=True)\n",
    "# sentence = data[0][4]\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6aQ7ZcPCH4d"
   },
   "outputs": [],
   "source": [
    "# creating a map {docID, index_in_data_npy}\n",
    "\n",
    "# ex. if ith element in data has docID j,\n",
    "# get_index[i] will return j\n",
    "\n",
    "get_index = {}\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_index[i] = int(data[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibq9gAauMI5Q"
   },
   "outputs": [],
   "source": [
    "# Tester code\n",
    "import time\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    f_content = data[get_index[i]][4]\n",
    "    contents = f_content\n",
    "    # print(contents)\n",
    "    final = lemma_stop (contents)\n",
    "#     print(type(final))\n",
    "    # print (final)\n",
    "    \n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = trie.CollectionNode()\n",
    "doc1 = trie.Node()\n",
    "for w in final:\n",
    "    print (w)\n",
    "    collection.add_(w, 0, 1)\n",
    "    doc1.add(w, 0)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = trie.Node()\n",
    "for w in final:\n",
    "    collection.add_(w, 0, 2)\n",
    "    doc2.add(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collection.get_doc_list(\"never\",0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"never Cooper\"\n",
    "final_query = lemma_stop(query)\n",
    "print(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1\n",
    "    print(w)\n",
    "    df = len(collection.get_doc_list(w,0))\n",
    "    print(collection.get_doc_list(w,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqsqWNnFNOhO"
   },
   "source": [
    "***Original Text :***\n",
    "\n",
    "      We’re back in Dale Cooper’s position, wandering through a freshly revived world, and trying to catch up with the ways it’s moved on in his absence.\n",
    "\n",
    "***Processed Text :***\n",
    "\n",
    "      We back Dale Cooper position wander freshly revive world try catch way move absence"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
